# Temp
Web Scraping Tutor

ğŸ•¸ï¸ Apache Jira Scraper

ğŸš€ A scalable, fault-tolerant web scraping pipeline to extract and transform Apache Jira issue data into structured JSONL format suitable for LLM training.

ğŸ§© Overview

This project automates the extraction of public issue data from Apacheâ€™s Jira instance and transforms it into a clean dataset for Large Language Model (LLM) training or downstream NLP tasks.

It was developed as part of the Scaler Web Scraping Assignment to demonstrate:

Resilient API scraping with rate-limit handling

Data checkpointing and recovery

Structured data transformation into JSONL corpus

Logging and error handling for large-scale scraping

ğŸ¯ Objective

The system is designed to:

Scrape issues (titles, descriptions, comments, metadata) from multiple Apache Jira projects.

Handle real-world issues like network failures, rate limits, and timeouts.

Resume automatically from the last successful checkpoint if interrupted.

Transform data into structured JSONL format ready for machine learning pipelines.

ğŸ§  Features
Feature	Description
ğŸ§¾ Multi-Project Scraping	Fetches issues from multiple Apache projects (HADOOP, SPARK, KAFKA)
ğŸ” Pagination & Checkpoints	Automatically handles paginated API calls and resumes from checkpoints
âš¡ Exponential Backoff Retries	Retries failed requests intelligently to avoid hammering the API
ğŸ§± Fault-Tolerant Design	Handles network drops, malformed data, and HTTP 429 / 5xx gracefully
ğŸ§¹ JSONL Transformation	Converts unstructured Jira data into clean JSONL lines
ğŸªµ Centralized Logging	All activities logged under the /logs folder
ğŸ’¾ Modular Architecture	Configurable settings via config/settings.py
ğŸ—ï¸ Architecture Overview
apache-jira-scraper/
â”‚
â”œâ”€â”€ main.py                  # Entry point for the scraper pipeline
â”œâ”€â”€ config/
â”‚   â””â”€â”€ settings.py          # Configurations (URLs, limits, paths)
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ scraper.py           # JiraScraper class (fetching issues)
â”‚   â”œâ”€â”€ transform.py         # DataTransformer class (JSONL processing)
â”‚   â”œâ”€â”€ utils.py             # Helper functions (checkpointing, file ops)
â”‚   â””â”€â”€ logger.py            # Custom logging setup
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                 # Raw API JSON responses
â”‚   â”œâ”€â”€ processed/           # Final processed JSONL outputs
â”‚   â””â”€â”€ checkpoints/         # Resume states for each project
â”‚
â””â”€â”€ logs/                    # Log files

âš™ï¸ Setup Instructions
1ï¸âƒ£ Clone the Repository
git clone https://github.com/YOUR_GITHUB_USERNAME/apache-jira-scraper.git
cd apache-jira-scraper

2ï¸âƒ£ Create a Virtual Environment
python -m venv venv
venv\Scripts\activate     # On Windows
# or
source venv/bin/activate  # On macOS/Linux

3ï¸âƒ£ Install Dependencies
pip install -r requirements.txt

4ï¸âƒ£ Configure Settings (optional)

Edit config/settings.py to adjust:

Projects list (DEFAULT_PROJECTS)

Rate limit delays

Max issues per project

Save paths

5ï¸âƒ£ Run the Scraper
python main.py

ğŸ“Š Output
ğŸ—‚ï¸ Raw Data

Raw JSON data for each project is saved in:

/data/raw/{PROJECT_NAME}_{offset}.json

ğŸ§® Processed JSONL

Final LLM-ready data is stored in:

/data/processed/{PROJECT_NAME}_processed.jsonl


Each line in .jsonl contains one JSON object like:

{
  "project": "HADOOP",
  "key": "HADOOP-12345",
  "title": "Improve shuffle performance in MapReduce",
  "status": "Closed",
  "description": "Detailed explanation of the shuffle bottleneck...",
  "comments": ["This patch improves...", "Merged in r1234"],
  "created": "2024-07-01T10:12:00.000+0000",
  "updated": "2024-07-05T11:30:00.000+0000",
  "labels": ["performance", "optimization"]
}

ğŸ§° Key Concepts Implemented

âœ… REST API integration with Apache Jira

âœ… Rate limit handling (429) and backoff strategies

âœ… Checkpoint-based recovery

âœ… Structured data transformation

âœ… Clean modular OOP design

âœ… Extensive logging system

âœ… Configurable constants for scalability

ğŸ§± Error Handling Strategies
Case	Solution
Network failure	Exponential backoff retry mechanism
API rate limit (429)	Graceful wait with retry
5xx server errors	Automatic retry with delay
Incomplete run	Checkpoint reload resumes last position
Empty/malformed JSON	Skipped and logged without crashing
