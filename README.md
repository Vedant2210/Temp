# Apache JIRA Scraper

> A robust, modular Python-based scraper to collect, process, and transform issue data from **Apache JIRA** projects (like Hadoop, Spark, and Kafka) into structured **JSONL format** â€” ideal for analytics or LLM training.

---

## Overview

The **Apache JIRA Scraper** automates the extraction of public issue data from Apacheâ€™s JIRA system using REST APIs.  
It helps researchers, developers, and data engineers create high-quality datasets by scraping issue descriptions, comments, metadata, and transforming them into a **machine-readable** format.

This tool is designed for:
-  Data collection for ML/NLP research  
-  Issue tracking & project analytics  
-  Training datasets for LLM fine-tuning  

---

## âš™ï¸ Tech Stack

| Category | Tools / Libraries |
|-----------|-------------------|
| **Language** | Python 3.10+ |
| **HTTP Requests** | `requests` |
| **Data Handling** | `json`, `pandas` |
| **Logging** | `logging`, custom loggers |
| **Configuration** | `argparse`, `settings.py` |
| **Environment** | `venv` |
| **Output Format** | `.jsonl` |
| **Version Control** | Git & GitHub |

---

## ğŸ§© Features

âœ… Fetches issue data from multiple Apache projects (Hadoop, Spark, Kafka, etc.)  
âœ… Transforms raw JSON into cleaned `.jsonl` format  
âœ… Auto-handles rate limits and retries  
âœ… Logs every step for debugging & monitoring  
âœ… Checkpoint system for fault tolerance  
âœ… Configurable via `config/settings.py`  
âœ… Modular and extensible for new data pipelines  

---

## ğŸ“ Project Structure

```bash
apache-jira-scraper/
â”‚
â”œâ”€â”€ config/
â”‚   â””â”€â”€ settings.py           # Configuration variables (project list, limits)
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                  # Raw JSON data from JIRA API
â”‚   â”œâ”€â”€ processed/            # Cleaned JSONL files
â”‚   â””â”€â”€ checkpoints/          # Track last processed issue ID
â”‚
â”œâ”€â”€ logs/                     # Logging directory
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ scraper.py            # Core scraper logic
â”‚   â”œâ”€â”€ transform.py          # Data transformation and saving
â”‚   â””â”€â”€ logger.py             # Logging setup
â”‚
â”œâ”€â”€ main.py                   # Entry point for the scraper
â”œâ”€â”€ requirements.txt          # Dependencies list
â””â”€â”€ README.md                 # Project documentation
```
## âš™ï¸ Configuration

All configuration variables are stored in **`config/settings.py`**.  
Below is a table describing each key configuration parameter:

| Variable Name       | Description                                              | Example Value                                                |
|----------------------|----------------------------------------------------------|--------------------------------------------------------------|
| `PROJECTS`           | List of Apache project names to scrape                   | `["HADOOP", "SPARK", "KAFKA"]`                              |
| `ISSUE_FETCH_LIMIT`  | Maximum number of issues to fetch per project            | `500`                                                        |
| `OUTPUT_DIR`         | Directory to store processed JSONL files                 | `"data/processed/"`                                          |
| `LOG_FILE`           | Path to the log file for tracking scraping progress      | `"logs/scraper.log"`                                         |
| `BASE_URL`           | Base URL of the Apache JIRA server                       | `"https://issues.apache.org/jira/rest/api/2/search"`         |

## âš™ï¸ Installation & Setup

Follow these steps to set up and run the scraper locally ğŸ‘‡

---

### 1ï¸âƒ£ Clone the Repository

```
git clone https://github.com/Vedant2210/Apache_Jira_Scraper.git
cd Apache_Jira_Scraper
```
2ï¸âƒ£ Create a Virtual Environment
```
python -m venv venv
```
3ï¸âƒ£ Activate the Virtual Environment
Windows:
```
venv\Scripts\activate
```
macOS/Linux:
```
source venv/bin/activate
```
4ï¸âƒ£ Install Dependencies
```
pip install -r requirements.txt
```
5ï¸âƒ£ Verify Installation
To ensure Python and pip are correctly installed, run:
```
python --version
pip --version
```
Expected output example:
```
nginx

Python 3.10.x
pip 23.x.x
```
â–¶ï¸ Usage Guide
Once setup is complete, run the scraper:
```
python main.py
```
This will start fetching issue data for all projects listed inside your config/settings.py file.

By default, the projects are:
```
DEFAULT_PROJECTS = ["HADOOP", "SPARK", "KAFKA"]
```
Each projectâ€™s issue data will be scraped, transformed, and saved in .jsonl format under the data/processed directory.





Output Format

All processed issues are stored in:
```
data/processed/{project_name}_issues.jsonl
```
Example:
```
{"id": "HADOOP-1001", "summary": "Fix namenode error", "status": "Open", "reporter": "user123"}
{"id": "SPARK-2020", "summary": "Improve shuffle performance", "status": "Closed", "reporter": "dev456"}
```
ğŸªµ Logging

Logs are automatically generated to help you monitor scraping progress and errors.
All logs are stored in the path specified in settings.py (default: logs/scraper.log).

Example log entry:
```
[2025-11-01 14:25:37] INFO: Fetched 100 issues from project HADOOP
[2025-11-01 14:26:12] WARNING: Failed to fetch issue SPARK-998 (Timeout)
```
### ğŸ§© Project Pipeline Overview

| Step | Description | File / Module | Output / Notes |
|------|--------------|----------------|----------------|
| 1ï¸âƒ£ | **Start the main script** | `main.py` | Initializes the pipeline |
| 2ï¸âƒ£ | **Load configuration settings** | `config/settings.py` | Loads parameters and credentials |
| 3ï¸âƒ£ | **Fetch issues from Jira** | `scraper.py` (class: `JiraScraper`) | Retrieves issues via API |
| 4ï¸âƒ£ | **Handle pagination, rate limits, and retries** | `scraper.py` | Ensures all pages are fetched reliably |
| 5ï¸âƒ£ | **Save raw JSON data** | `data/raw/` | Stores unprocessed API responses |
| 6ï¸âƒ£ | **Clean and transform data** | `transform.py` (class: `DataTransformer`) | Formats and preprocesses data |
| 7ï¸âƒ£ | **Generate JSONL formatted output** | `transform.py` | Produces structured JSONL |
| 8ï¸âƒ£ | **Save processed data** | `data/processed/` | Stores final cleaned dataset |
| 9ï¸âƒ£ | **Log progress and status** | `logs/*.log` | Keeps track of run details |
| ğŸ”Ÿ | **End of pipeline** | â€” | Process completed successfully |

---

### ğŸ§© End-to-End Workflow

| Step | Description | File / Module | Output / Notes | Checkpoint |
|------|--------------|----------------|----------------|-------------|
| 1ï¸âƒ£ | **Start the pipeline** | `main.py` | Initializes all modules and loggers | â€” |
| 2ï¸âƒ£ | **Load configuration settings** | `config/settings.py` | Loads API tokens, Jira URLs, and output paths | `.checkpoint_config` |
| 3ï¸âƒ£ | **Fetch issues from Jira** | `scraper.py` (class: `JiraScraper`) | Collects raw issues using API requests | `.checkpoint_scraper` |
| 4ï¸âƒ£ | **Handle pagination, rate limits, and retries** | `scraper.py` | Ensures reliable, complete data fetching | Integrated |
| 5ï¸âƒ£ | **Save raw JSON data** | `data/raw/` | Saves raw unprocessed data | `.checkpoint_raw` |
| 6ï¸âƒ£ | **Clean and transform data** | `transform.py` (class: `DataTransformer`) | Applies cleaning, formatting, deduplication | `.checkpoint_transformer` |
| 7ï¸âƒ£ | **Generate JSONL output** | `transform.py` | Creates structured JSONL file for downstream tasks | `.checkpoint_jsonl` |
| 8ï¸âƒ£ | **Save processed data** | `data/processed/` | Final cleaned and validated dataset | `.checkpoint_processed` |
| 9ï¸âƒ£ | **Log progress** | `logs/*.log` | Tracks each step and any exceptions | Auto |
| ğŸ”Ÿ | **Resume from last checkpoint (if rerun)** | `main.py` | Skips completed stages automatically | â€” |
| âœ… | **End of pipeline** | â€” | Successfully completed workflow | â€” |
