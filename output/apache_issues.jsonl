{"project": "HADOOP", "issue_key": "HADOOP-19736", "title": "ABFS: Support for new auth type: User-bound SAS", "status": "Open", "reporter": "Manika Joshi", "assignee": "Manika Joshi", "priority": "Major", "labels": ["pull-request-available"], "created": "2025-10-24T09:31:18.000+0000", "updated": "2025-10-31T13:05:18.000+0000", "description": "Adding support for new authentication type: user bound SAS", "comments": [], "derived_tasks": {"summarization": "ABFS: Support for new auth type: User-bound SAS", "classification": "feature", "qna": {"question": "What is the issue 'ABFS: Support for new auth type: User-bound SAS' about?", "answer": "Adding support for new authentication type: user bound SAS"}}}
{"project": "HADOOP", "issue_key": "HADOOP-19735", "title": "ABFS: Adding request priority for prefetches", "status": "Open", "reporter": "Manika Joshi", "assignee": "Manika Joshi", "priority": "Major", "labels": [], "created": "2025-10-24T04:46:13.000+0000", "updated": "2025-10-24T05:00:42.000+0000", "description": "Adding low traffic request priority (behind a config flag) for prefetches to reduce load on server during throttling", "comments": [], "derived_tasks": {"summarization": "ABFS: Adding request priority for prefetches", "classification": "feature", "qna": {"question": "What is the issue 'ABFS: Adding request priority for prefetches' about?", "answer": "Adding low traffic request priority (behind a config flag) for prefetches to reduce load on server during throttling"}}}
{"project": "HADOOP", "issue_key": "HADOOP-19734", "title": "S3A: retry on MPU completion failure \"One or more of the specified parts could not be found\"", "status": "Resolved", "reporter": "Steve Loughran", "assignee": null, "priority": "Minor", "labels": [], "created": "2025-10-23T15:42:38.000+0000", "updated": "2025-10-24T13:48:33.000+0000", "description": "\r\nExperienced transient failure in test run of https://github.com/apache/hadoop/pull/7882 : all MPU complete posts failed because the request or parts were not found...the tests started succeeding 60-90s later *and* a \"hadoop s3guards uploads\" call listed the outstanding uploads of the failing tests.\r\n\r\nHypothesis: a transient failure meant the server receiving the POST calls to complete the uploads was mistakenly reporting no upload IDs.\r\n\r\nOutcome: all active write operations failed, without any retry attempts. This can lose data and fail jobs, even though the store may recover.\r\n\r\nProposed. The multipart uploads, especially block output stream, retry on this error; treat it as a connectivity issue. ", "comments": [], "derived_tasks": {"summarization": "S3A: retry on MPU completion failure \"One or more of the specified parts could not be found\"", "classification": "feature", "qna": {"question": "What is the issue 'S3A: retry on MPU completion failure \"One or more of the specified parts could not be found\"' about?", "answer": "\r\nExperienced transient failure in test run of https://github.com/apache/hadoop/pull/7882 : all MPU complete posts failed because the request or parts were not found...the tests started succeeding 60-90s later *and* a \"hadoop s3guards uploads\" call listed the outstanding uploads of the failing tests.\r\n\r\nHypothesis: a transient failure meant the server receiving the POST calls to complete the uploads was mistakenly reporting no upload IDs.\r\n\r\nOutcome: all active write operations failed, without any retry attempts. This can lose data and fail jobs, even though the store may recover.\r\n\r\nProposed. The multipart uploads, especially block output stream, retry on this error; treat it as a connectivity issue. "}}}
{"project": "HADOOP", "issue_key": "HADOOP-19733", "title": "S3A: Credentials provider classes not found despite setting `fs.s3a.classloader.isolation` to `false`", "status": "Open", "reporter": "Brandon", "assignee": "Brandon", "priority": "Minor", "labels": ["pull-request-available"], "created": "2025-10-22T19:56:55.000+0000", "updated": "2025-10-23T19:44:03.000+0000", "description": "HADOOP-18993 added the option `fs.s3a.classloader.isolation` to support, for example, a Spark job using an AWS credentials provider class that is bundled into the Spark job JAR. In testing this, the AWS credentials provider classes are still not found.\r\n\r\nI think the cause is:\r\n * `fs.s3a.classloader.isolation` is implemented by setting (or not setting) a classloader on the `Configuration`\r\n * However, code paths to load AWS credential provider call `S3AUtils.getInstanceFromReflection`, which uses the classloader that loaded the S3AUtils class. That's likely to be the built-in application classloader, which won't be able to load classes in a Spark job JAR.\r\n\r\nAnd the fix seems small:\r\n * Change `S3AUtils.getInstanceFromReflection` to load classes using the `Configuration`'s classloader. Luckily we already have the Configuration in this method.", "comments": [], "derived_tasks": {"summarization": "S3A: Credentials provider classes not found despite setting `fs.s3a.classloader.isolation` to `false`", "classification": "feature", "qna": {"question": "What is the issue 'S3A: Credentials provider classes not found despite setting `fs.s3a.classloader.isolation` to `false`' about?", "answer": "HADOOP-18993 added the option `fs.s3a.classloader.isolation` to support, for example, a Spark job using an AWS credentials provider class that is bundled into the Spark job JAR. In testing this, the AWS credentials provider classes are still not found.\r\n\r\nI think the cause is:\r\n * `fs.s3a.classloader.isolation` is implemented by setting (or not setting) a classloader on the `Configuration`\r\n * However, code paths to load AWS credential provider call `S3AUtils.getInstanceFromReflection`, which uses the classloader that loaded the S3AUtils class. That's likely to be the built-in application classloader, which won't be able to load classes in a Spark job JAR.\r\n\r\nAnd the fix seems small:\r\n * Change `S3AUtils.getInstanceFromReflection` to load classes using the `Configuration`'s classloader. Luckily we already have the Configuration in this method."}}}
{"project": "HADOOP", "issue_key": "HADOOP-19732", "title": "Namenode Crash Due to Delegation Renewer Runtime Exit (NoMatchingRule)", "status": "Resolved", "reporter": "Karthik Palanisamy", "assignee": null, "priority": "Major", "labels": [], "created": "2025-10-21T18:43:38.000+0000", "updated": "2025-10-23T17:47:42.000+0000", "description": "The delegation token renewer enters runtime exit when a _NoMatchingRule_ error, which caused the entire namenode to crash. I think returning an error to the client should be fine but bringing down the namenode is not acceptable to anyone.\r\n\r\nAfter the AD change, the new realm was updated, but some jobs are still using the old realm as users are updating them gradually. This migration process will take time, and during this period, other jobs are still catching up with the new realm configuration. However, the namenode down disrupts all of them.\r\n\r\n\u00a0\r\n{code:java}\r\nERROR org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: ExpiredTokenRemover thread received unexpected exception\r\njava.lang.IllegalArgumentException: Illegal principal name hive/xxxxx@yyyy.COM\r\norg.apache.hadoop.security.authentication.util.KerberosName$NoMatchingRule: No rules applied to hive/xxxxx@yyyy.COM \u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.security.User.<init>(User.java:51)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.security.User.<init>(User.java:43)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.security.UserGroupInformation.createRemoteUser(UserGroupInformation.java:1417)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.security.UserGroupInformation.createRemoteUser(UserGroupInformation.java:1401)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier.getUser(AbstractDelegationTokenIdentifier.java:80)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenIdentifier.getUser(DelegationTokenIdentifier.java:81)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenIdentifier.toString(DelegationTokenIdentifier.java:91)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at java.lang.String.valueOf(String.java:2994)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at java.lang.StringBuilder.append(StringBuilder.java:137)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager.formatTokenId(AbstractDelegationTokenSecretManager.java:58)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager.logExpireTokens(AbstractDelegationTokenSecretManager.java:642)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager.removeExpiredToken(AbstractDelegationTokenSecretManager.java:635)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager.access$400(AbstractDelegationTokenSecretManager.java:51)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$ExpiredTokenRemover.run(AbstractDelegationTokenSecretManager.java:694)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at java.lang.Thread.run(Thread.java:750)\r\nCaused by: org.apache.hadoop.security.authentication.util.KerberosName$NoMatchingRule: No rules applied to hive/xxxxx@yyyy.COM\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.security.authentication.util.KerberosName.getShortName(KerberosName.java:429)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.security.User.<init>(User.java:48)\r\n\u00a0 \u00a0 \u00a0 \u00a0 ... 14 more {code}\r\n\u00a0", "comments": [], "derived_tasks": {"summarization": "Namenode Crash Due to Delegation Renewer Runtime Exit (NoMatchingRule)", "classification": "feature", "qna": {"question": "What is the issue 'Namenode Crash Due to Delegation Renewer Runtime Exit (NoMatchingRule)' about?", "answer": "The delegation token renewer enters runtime exit when a _NoMatchingRule_ error, which caused the entire namenode to crash. I think returning an error to the client should be fine but bringing down the namenode is not acceptable to anyone.\r\n\r\nAfter the AD change, the new realm was updated, but some jobs are still using the old realm as users are updating them gradually. This migration process will take time, and during this period, other jobs are still catching up with the new realm configuration. However, the namenode down disrupts all of them.\r\n\r\n\u00a0\r\n{code:java}\r\nERROR org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: ExpiredTokenRemover thread received unexpected exception\r\njava.lang.IllegalArgumentException: Illegal principal name hive/xxxxx@yyyy.COM\r\norg.apache.hadoop.security.authentication.util.KerberosName$NoMatchingRule: No rules applied to hive/xxxxx@yyyy.COM \u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.security.User.<init>(User.java:51)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.security.User.<init>(User.java:43)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.security.UserGroupInformation.createRemoteUser(UserGroupInformation.java:1417)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.security.UserGroupInformation.createRemoteUser(UserGroupInformation.java:1401)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier.getUser(AbstractDelegationTokenIdentifier.java:80)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenIdentifier.getUser(DelegationTokenIdentifier.java:81)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenIdentifier.toString(DelegationTokenIdentifier.java:91)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at java.lang.String.valueOf(String.java:2994)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at java.lang.StringBuilder.append(StringBuilder.java:137)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager.formatTokenId(AbstractDelegationTokenSecretManager.java:58)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager.logExpireTokens(AbstractDelegationTokenSecretManager.java:642)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager.removeExpiredToken(AbstractDelegationTokenSecretManager.java:635)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager.access$400(AbstractDelegationTokenSecretManager.java:51)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$ExpiredTokenRemover.run(AbstractDelegationTokenSecretManager.java:694)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at java.lang.Thread.run(Thread.java:750)\r\nCaused by: org.apache.hadoop.security.authentication.util.KerberosName$NoMatchingRule: No rules applied to hive/xxxxx@yyyy.COM\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.security.authentication.util.KerberosName.getShortName(KerberosName.java:429)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.security.User.<init>(User.java:48)\r\n\u00a0 \u00a0 \u00a0 \u00a0 ... 14 more {code}\r\n\u00a0"}}}
{"project": "HADOOP", "issue_key": "HADOOP-19731", "title": "Fix SpotBugs warnings introduced after SpotBugs version upgrade.", "status": "In Progress", "reporter": "Shilun Fan", "assignee": "Shilun Fan", "priority": "Major", "labels": ["pull-request-available"], "created": "2025-10-19T09:57:26.000+0000", "updated": "2025-10-31T02:29:09.000+0000", "description": "Following the upgrade to SpotBugs {*}4.9.7{*}, several new warnings have emerged.\r\nWe plan to address these warnings to improve code safety and maintain compatibility with the updated analysis rules.", "comments": [], "derived_tasks": {"summarization": "Fix SpotBugs warnings introduced after SpotBugs version upgrade.", "classification": "feature", "qna": {"question": "What is the issue 'Fix SpotBugs warnings introduced after SpotBugs version upgrade.' about?", "answer": "Following the upgrade to SpotBugs {*}4.9.7{*}, several new warnings have emerged.\r\nWe plan to address these warnings to improve code safety and maintain compatibility with the updated analysis rules."}}}
{"project": "HADOOP", "issue_key": "HADOOP-19730", "title": "upgrade bouncycastle to 1.82 due to CVE-2025-8916", "status": "Resolved", "reporter": "PJ Fanning", "assignee": "PJ Fanning", "priority": "Major", "labels": ["pull-request-available"], "created": "2025-10-19T09:09:36.000+0000", "updated": "2025-10-27T05:48:06.000+0000", "description": "https://github.com/advisories/GHSA-4cx2-fc23-5wg6\r\n\r\nThought it was tidier to upgrade to latest version even if the fix was a while ago.", "comments": [], "derived_tasks": {"summarization": "upgrade bouncycastle to 1.82 due to CVE-2025-8916", "classification": "feature", "qna": {"question": "What is the issue 'upgrade bouncycastle to 1.82 due to CVE-2025-8916' about?", "answer": "https://github.com/advisories/GHSA-4cx2-fc23-5wg6\r\n\r\nThought it was tidier to upgrade to latest version even if the fix was a while ago."}}}
{"project": "HADOOP", "issue_key": "HADOOP-19729", "title": "ABFS: [Perf] Network Profiling of Tailing Requests and Killing Bad Connections Proactively", "status": "Open", "reporter": "Anuj Modi", "assignee": "Anuj Modi", "priority": "Major", "labels": ["pull-request-available"], "created": "2025-10-17T05:41:02.000+0000", "updated": "2025-10-31T15:17:41.000+0000", "description": "It has been observed that certain requests taking more time than expected to complete hinders the performance of whole workload. Such requests are known as tailing requests. They can be taking more time due to a number of reasons and the prominent among them is a bad network connection. In Abfs driver we cache network connections and keeping such bad connections in cache and reusing them can be bad for perf.\r\n\r\nIn this effort we try to identify such connections and close them so that new good connetions can be established and perf can be improved. There are two parts of this effort.\r\n # Identifying Tailing Requests: This involves profiling all the network calls and getting percentiles value optimally. By default we consider p99 as the tail latency and all the future requests taking more than tail latency will be considere as Tailing requests.\r\n\r\n # Proactively Killing Socket Connections: With Apache client, we can now kill the socket connection and fail the tailing request. Such failures will not be thrown back to user and retried immediately without any sleep but from another socket connection.", "comments": [], "derived_tasks": {"summarization": "ABFS: [Perf] Network Profiling of Tailing Requests and Killing Bad Connections Proactively", "classification": "feature", "qna": {"question": "What is the issue 'ABFS: [Perf] Network Profiling of Tailing Requests and Killing Bad Connections Proactively' about?", "answer": "It has been observed that certain requests taking more time than expected to complete hinders the performance of whole workload. Such requests are known as tailing requests. They can be taking more time due to a number of reasons and the prominent among them is a bad network connection. In Abfs driver we cache network connections and keeping such bad connections in cache and reusing them can be bad for perf.\r\n\r\nIn this effort we try to identify such connections and close them so that new good connetions can be established and perf can be improved. There are two parts of this effort.\r\n # Identifying Tailing Requests: This involves profiling all the network calls and getting percentiles value optimally. By default we consider p99 as the tail latency and all the future requests taking more than tail latency will be considere as Tailing requests.\r\n\r\n # Proactively Killing Socket Connections: With Apache client, we can now kill the socket connection and fail the tailing request. Such failures will not be thrown back to user and retried immediately without any sleep but from another socket connection."}}}
{"project": "HADOOP", "issue_key": "HADOOP-19728", "title": "S3A: add ipv6 support", "status": "Open", "reporter": "Steve Loughran", "assignee": null, "priority": "Major", "labels": [], "created": "2025-10-15T13:16:53.000+0000", "updated": "2025-10-15T13:16:53.000+0000", "description": "Support IPv6 with a flag to enable/disable dual stack endpoints\r\n\r\nhttps://docs.aws.amazon.com/AmazonS3/latest/API/ipv6-access.html", "comments": [], "derived_tasks": {"summarization": "S3A: add ipv6 support", "classification": "feature", "qna": {"question": "What is the issue 'S3A: add ipv6 support' about?", "answer": "Support IPv6 with a flag to enable/disable dual stack endpoints\r\n\r\nhttps://docs.aws.amazon.com/AmazonS3/latest/API/ipv6-access.html"}}}
{"project": "HADOOP", "issue_key": "HADOOP-19727", "title": "Release hadoop-thirdparty 1.5.0", "status": "In Progress", "reporter": "Steve Loughran", "assignee": "Steve Loughran", "priority": "Major", "labels": [], "created": "2025-10-14T14:34:32.000+0000", "updated": "2025-10-20T16:40:40.000+0000", "description": "\r\nRelease hadoop-thirdparty 1.5.0", "comments": [], "derived_tasks": {"summarization": "Release hadoop-thirdparty 1.5.0", "classification": "feature", "qna": {"question": "What is the issue 'Release hadoop-thirdparty 1.5.0' about?", "answer": "\r\nRelease hadoop-thirdparty 1.5.0"}}}
{"project": "HADOOP", "issue_key": "HADOOP-19726", "title": "Add JDK 17 compile options for maven-surefire-plugin in hadoop-tos module", "status": "Resolved", "reporter": "Shilun Fan", "assignee": "Shilun Fan", "priority": "Major", "labels": ["pull-request-available"], "created": "2025-10-12T23:47:15.000+0000", "updated": "2025-10-19T03:00:16.000+0000", "description": "Currently, the {{hadoop-tos}} module does not have the JDK 17 compile options configured for the {{{}maven-surefire-plugin{}}}, which causes the following error during unit test execution:\r\n{code:java}\r\njava.lang.IllegalStateException: Failed to set environment variable\tat org.apache.hadoop.fs.tosfs.util.TestUtility.setSystemEnv(TestUtility.java:120)\tat org.apache.hadoop.fs.tosfs.object.ObjectStorageTestBase.setUp(ObjectStorageTestBase.java:59)\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\tat java.base/java.util.ArrayList.forEach(ArrayList.java:1511)\tat java.base/java.util.ArrayList.forEach(ArrayList.java:1511)Caused by: java.lang.reflect.InaccessibleObjectException: Unable to make field private final java.util.Map java.util.Collections$UnmodifiableMap.m accessible: module java.base does not \"opens java.util\" to unnamed module @69eee410\tat java.base/java.lang.reflect.AccessibleObject.checkCanSetAccessible(AccessibleObject.java:354)\tat java.base/java.lang.reflect.AccessibleObject.checkCanSetAccessible(AccessibleObject.java:297)\tat java.base/java.lang.reflect.Field.checkCanSetAccessible(Field.java:178)\tat java.base/java.lang.reflect.Field.setAccessible(Field.java:172)\tat org.apache.hadoop.fs.tosfs.util.TestUtility.setSystemEnv(TestUtility.java:116)\t... 4 more\r\n {code}\r\nThis error occurs due to the module system restrictions in JDK 17, where reflection cannot access private fields in the java.util.Collections$UnmodifiableMap class.\r\n\r\n\u00a0\r\n\r\nTo resolve this issue, JDK 17 compile options have been added to ensure the maven-surefire-plugin works correctly in a JDK 17 environment. This PR adds the necessary compile options for maven-surefire-plugin to support JDK 17, fixing the error and ensuring that unit tests can run smoothly.", "comments": [], "derived_tasks": {"summarization": "Add JDK 17 compile options for maven-surefire-plugin in hadoop-tos module", "classification": "feature", "qna": {"question": "What is the issue 'Add JDK 17 compile options for maven-surefire-plugin in hadoop-tos module' about?", "answer": "Currently, the {{hadoop-tos}} module does not have the JDK 17 compile options configured for the {{{}maven-surefire-plugin{}}}, which causes the following error during unit test execution:\r\n{code:java}\r\njava.lang.IllegalStateException: Failed to set environment variable\tat org.apache.hadoop.fs.tosfs.util.TestUtility.setSystemEnv(TestUtility.java:120)\tat org.apache.hadoop.fs.tosfs.object.ObjectStorageTestBase.setUp(ObjectStorageTestBase.java:59)\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\tat java.base/java.util.ArrayList.forEach(ArrayList.java:1511)\tat java.base/java.util.ArrayList.forEach(ArrayList.java:1511)Caused by: java.lang.reflect.InaccessibleObjectException: Unable to make field private final java.util.Map java.util.Collections$UnmodifiableMap.m accessible: module java.base does not \"opens java.util\" to unnamed module @69eee410\tat java.base/java.lang.reflect.AccessibleObject.checkCanSetAccessible(AccessibleObject.java:354)\tat java.base/java.lang.reflect.AccessibleObject.checkCanSetAccessible(AccessibleObject.java:297)\tat java.base/java.lang.reflect.Field.checkCanSetAccessible(Field.java:178)\tat java.base/java.lang.reflect.Field.setAccessible(Field.java:172)\tat org.apache.hadoop.fs.tosfs.util.TestUtility.setSystemEnv(TestUtility.java:116)\t... 4 more\r\n {code}\r\nThis error occurs due to the module system restrictions in JDK 17, where reflection cannot access private fields in the java.util.Collections$UnmodifiableMap class.\r\n\r\n\u00a0\r\n\r\nTo resolve this issue, JDK 17 compile options have been added to ensure the maven-surefire-plugin works correctly in a JDK 17 environment. This PR adds the necessary compile options for maven-surefire-plugin to support JDK 17, fixing the error and ensuring that unit tests can run smoothly."}}}
{"project": "HADOOP", "issue_key": "HADOOP-19725", "title": "Upgrade SpotBugs Version to Support JDK 17 Compilation", "status": "Resolved", "reporter": "Shilun Fan", "assignee": "Shilun Fan", "priority": "Major", "labels": ["pull-request-available"], "created": "2025-10-12T23:27:49.000+0000", "updated": "2025-10-16T14:06:32.000+0000", "description": "The current SpotBugs version used in the project is 4.2.2 (SpotBugs) and 4.2.0 (SpotBugs Maven Plugin), which is not fully compatible with JDK 17 compilation. To ensure proper functionality of the code quality check tool in a JDK 17 environment, SpotBugs needs to be upgraded to the latest version that supports JDK 17.", "comments": [], "derived_tasks": {"summarization": "Upgrade SpotBugs Version to Support JDK 17 Compilation", "classification": "feature", "qna": {"question": "What is the issue 'Upgrade SpotBugs Version to Support JDK 17 Compilation' about?", "answer": "The current SpotBugs version used in the project is 4.2.2 (SpotBugs) and 4.2.0 (SpotBugs Maven Plugin), which is not fully compatible with JDK 17 compilation. To ensure proper functionality of the code quality check tool in a JDK 17 environment, SpotBugs needs to be upgraded to the latest version that supports JDK 17."}}}
{"project": "HADOOP", "issue_key": "HADOOP-19724", "title": "[RISC-V]  Add rv bulk CRC32 (non-CRC32C) optimized path", "status": "Open", "reporter": "Ptroc", "assignee": null, "priority": "Major", "labels": ["native", "pull-request-available", "risc-v"], "created": "2025-10-12T12:49:32.000+0000", "updated": "2025-10-14T12:08:33.000+0000", "description": null, "comments": [], "derived_tasks": {"summarization": "[RISC-V]  Add rv bulk CRC32 (non-CRC32C) optimized path", "classification": "feature", "qna": {"question": "What is the issue '[RISC-V]  Add rv bulk CRC32 (non-CRC32C) optimized path' about?", "answer": null}}}
{"project": "HADOOP", "issue_key": "HADOOP-19723", "title": "Build multi-arch hadoop image", "status": "Resolved", "reporter": "Attila Doroszlai", "assignee": "Attila Doroszlai", "priority": "Major", "labels": ["pull-request-available"], "created": "2025-10-09T09:22:57.000+0000", "updated": "2025-10-10T06:07:00.000+0000", "description": "Build {{apache/hadoop}} Docker image for both amd64 and arm64.", "comments": [], "derived_tasks": {"summarization": "Build multi-arch hadoop image", "classification": "feature", "qna": {"question": "What is the issue 'Build multi-arch hadoop image' about?", "answer": "Build {{apache/hadoop}} Docker image for both amd64 and arm64."}}}
{"project": "HADOOP", "issue_key": "HADOOP-19722", "title": "Pin robotframework version", "status": "Resolved", "reporter": "Attila Doroszlai", "assignee": "Attila Doroszlai", "priority": "Minor", "labels": ["pull-request-available"], "created": "2025-10-09T07:00:41.000+0000", "updated": "2025-10-12T17:54:24.000+0000", "description": "{{hadoop-runner}} installs {{robotframework}} without version definition.  Re-building the image for unrelated changes may unexpectedly upgrade {{robotframework}}, too.", "comments": [], "derived_tasks": {"summarization": "Pin robotframework version", "classification": "feature", "qna": {"question": "What is the issue 'Pin robotframework version' about?", "answer": "{{hadoop-runner}} installs {{robotframework}} without version definition.  Re-building the image for unrelated changes may unexpectedly upgrade {{robotframework}}, too."}}}
{"project": "HADOOP", "issue_key": "HADOOP-19721", "title": "Upgrade hadoop-runner to Ubuntu 24.04", "status": "Open", "reporter": "Attila Doroszlai", "assignee": "Attila Doroszlai", "priority": "Minor", "labels": [], "created": "2025-10-09T06:23:41.000+0000", "updated": "2025-10-09T06:23:41.000+0000", "description": "Latest {{hadoop-runner}} images are based on Ubuntu 22.04.  Upgrade to 24.04.", "comments": [], "derived_tasks": {"summarization": "Upgrade hadoop-runner to Ubuntu 24.04", "classification": "feature", "qna": {"question": "What is the issue 'Upgrade hadoop-runner to Ubuntu 24.04' about?", "answer": "Latest {{hadoop-runner}} images are based on Ubuntu 22.04.  Upgrade to 24.04."}}}
{"project": "HADOOP", "issue_key": "HADOOP-19720", "title": "Publish multi-arch hadoop-runner image to GitHub", "status": "Resolved", "reporter": "Attila Doroszlai", "assignee": "Attila Doroszlai", "priority": "Major", "labels": ["pull-request-available"], "created": "2025-10-08T17:21:10.000+0000", "updated": "2025-10-09T07:01:33.000+0000", "description": "Create GitHub Actions workflow to publish the apache/hadoop-runner Docker image to GitHub Container Registry.  Build for both amd64 and arm64.", "comments": [], "derived_tasks": {"summarization": "Publish multi-arch hadoop-runner image to GitHub", "classification": "feature", "qna": {"question": "What is the issue 'Publish multi-arch hadoop-runner image to GitHub' about?", "answer": "Create GitHub Actions workflow to publish the apache/hadoop-runner Docker image to GitHub Container Registry.  Build for both amd64 and arm64."}}}
{"project": "HADOOP", "issue_key": "HADOOP-19719", "title": "Upgrade to wildfly version with support for openssl 3", "status": "Resolved", "reporter": "Steve Loughran", "assignee": "Steve Loughran", "priority": "Major", "labels": ["pull-request-available"], "created": "2025-10-08T10:52:33.000+0000", "updated": "2025-10-20T12:47:50.000+0000", "description": "Wildfly 2.1.4\r\n* doesn't work with openssl 3 (that symbol change...why did they do that?)\r\n\r\n\r\nwe need a version with \r\nhttps://github.com/wildfly-security/wildfly-openssl-natives/commit/6cecd42a254cd78585fefd9a0e41ad7954ece80d\r\n\r\n2.2.5.Final does the openssl 3 support. ", "comments": [], "derived_tasks": {"summarization": "Upgrade to wildfly version with support for openssl 3", "classification": "feature", "qna": {"question": "What is the issue 'Upgrade to wildfly version with support for openssl 3' about?", "answer": "Wildfly 2.1.4\r\n* doesn't work with openssl 3 (that symbol change...why did they do that?)\r\n\r\n\r\nwe need a version with \r\nhttps://github.com/wildfly-security/wildfly-openssl-natives/commit/6cecd42a254cd78585fefd9a0e41ad7954ece80d\r\n\r\n2.2.5.Final does the openssl 3 support. "}}}
{"project": "HADOOP", "issue_key": "HADOOP-19718", "title": "[ABFS]: Throw HTTPException when AAD token fetch fails ", "status": "Open", "reporter": "Sneha Vijayarajan", "assignee": "Sneha Vijayarajan", "priority": "Minor", "labels": [], "created": "2025-10-07T12:56:26.000+0000", "updated": "2025-10-07T12:56:26.000+0000", "description": "Reported by [~enigma25] :\r\nIn [this code snippet](https://github.com/Azure/azure-data-lake-store-java/blob/26eca936da60286aa70b0dd7823ff5e627987bc4/src/main/java/com/microsoft/azure/datalake/store/oauth2/AzureADAuthenticator.java#L252-L293) from AzureADAuthenticator, the `conn` is being returned as null due to a (possibly) corrupted connection between Azure Data Lake Store Client and AAD while fetching AAD token. The code snippet linked, runs into an NPE. I wanted to know from the maintainers and community if this bug/symptom has been seen by them earlier and what might be the best way to handle this? Secondly, I wanted to know the right place for the fix too - whether it should be the application code or the SDK code itself should handle such NPEs and fail more gracefully?\r\nOpen to thoughts and comments.\r\nCheers,\r\nNikhil\r\n\u00a0\r\n\u00a0\r\n```\r\njava.util.concurrent.ExecutionException: java.lang.NullPointerException: Cannot invoke \"java.io.InputStream.read(byte[], int, int)\" because \"inStream\" is null\r\nat org.apache.kafka.connect.util.ConvertingFutureCallback.result(ConvertingFutureCallback.java:135)\r\nat org.apache.kafka.connect.util.ConvertingFutureCallback.get(ConvertingFutureCallback.java:122)\r\nat org.apache.kafka.connect.runtime.rest.resources.ConnectorPluginsResource.validateConfigs(ConnectorPluginsResource.java:129)\r\nat java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103)\r\nat java.base/java.lang.reflect.Method.invoke(Method.java:580)\r\nat org.glassfish.jersey.server.model.internal.ResourceMethodInvocationHandlerFactory.lambda$static$0(ResourceMethodInvocationHandlerFactory.java:52)\r\nat org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher$1.run(AbstractJavaResourceMethodDispatcher.java:134)\r\nat org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.invoke(AbstractJavaResourceMethodDispatcher.java:177)\r\nat org.glassfish.jersey.server.model.internal.JavaResourceMethodDispatcherProvider$TypeOutInvoker.doDispatch(JavaResourceMethodDispatcherProvider.java:219)\r\nat org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.dispatch(AbstractJavaResourceMethodDispatcher.java:81)\r\nat org.glassfish.jersey.server.model.ResourceMethodInvoker.invoke(ResourceMethodInvoker.java:478)\r\nat org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:400)\r\nat org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:81)\r\nat org.glassfish.jersey.server.ServerRuntime$1.run(ServerRuntime.java:256)\r\nat org.glassfish.jersey.internal.Errors$1.call(Errors.java:248)\r\nat org.glassfish.jersey.internal.Errors$1.call(Errors.java:244)\r\nat org.glassfish.jersey.internal.Errors.process(Errors.java:292)\r\nat org.glassfish.jersey.internal.Errors.process(Errors.java:274)\r\nat org.glassfish.jersey.internal.Errors.process(Errors.java:244)\r\nat org.glassfish.jersey.process.internal.RequestScope.runInScope(RequestScope.java:265)\r\nat org.glassfish.jersey.server.ServerRuntime.process(ServerRuntime.java:235)\r\nat org.glassfish.jersey.server.ApplicationHandler.handle(ApplicationHandler.java:684)\r\nat org.glassfish.jersey.servlet.WebComponent.serviceImpl(WebComponent.java:394)\r\nat org.glassfish.jersey.servlet.WebComponent.service(WebComponent.java:346)\r\nat org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:358)\r\nat org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:311)\r\nat org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:205)\r\nat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)\r\nat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:554)\r\nat org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)\r\nat org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:1624)\r\nat org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)\r\nat org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1440)\r\nat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)\r\nat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:505)\r\nat org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:1594)\r\nat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)\r\nat org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1355)\r\nat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\r\nat org.eclipse.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:234)\r\nat org.eclipse.jetty.server.handler.StatisticsHandler.handle(StatisticsHandler.java:181)\r\nat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\r\nat org.eclipse.jetty.server.Server.handle(Server.java:516)\r\nat org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:487)\r\nat org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:732)\r\nat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:479)\r\nat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\r\nat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\r\nat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)\r\nat org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\r\nat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)\r\nat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)\r\nat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)\r\nat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)\r\nat org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409)\r\nat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)\r\nat org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)\r\nat java.base/java.lang.Thread.run(Thread.java:1583)\r\nCaused by: java.lang.NullPointerException: Cannot invoke \"java.io.InputStream.read(byte[], int, int)\" because \"inStream\" is null\r\nat org.apache.hadoop.fs.azurebfs.oauth2.AzureADAuthenticator.consumeInputStream(AzureADAuthenticator.java:345)\r\nat org.apache.hadoop.fs.azurebfs.oauth2.AzureADAuthenticator.getTokenSingleCall(AzureADAuthenticator.java:275)\r\nat org.apache.hadoop.fs.azurebfs.oauth2.AzureADAuthenticator.getTokenCall(AzureADAuthenticator.java:216)\r\nat org.apache.hadoop.fs.azurebfs.oauth2.AzureADAuthenticator.getTokenUsingClientCreds(AzureADAuthenticator.java:95)\r\nat org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider.refreshToken(ClientCredsTokenProvider.java:58)\r\nat org.apache.hadoop.fs.azurebfs.oauth2.AccessTokenProvider.getToken(AccessTokenProvider.java:50)\r\nat org.apache.hadoop.fs.azurebfs.services.AbfsClient.getAccessToken(AbfsClient.java:583)\r\nat org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.executeHttpOperation(AbfsRestOperation.java:162)\r\nat org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.execute(AbfsRestOperation.java:134)\r\nat org.apache.hadoop.fs.azurebfs.services.AbfsClient.getFilesystemProperties(AbfsClient.java:205)\r\nat io.confluent.connect.azure.datalake.gen2.validation.Validations.verifyClient(Validations.java:175)\r\nat io.confluent.connect.azure.datalake.gen2.validation.Validations.createAndValidateClient(Validations.java:395)\r\nat io.confluent.connect.azure.datalake.gen2.validation.Validations.validateAll(Validations.java:131)\r\nat io.confluent.connect.utils.validators.all.ConfigValidation.lambda$callValidators$0(ConfigValidation.java:222)\r\nat java.base/java.util.Spliterators$ArraySpliterator.forEachRemaining(Spliterators.java:1024)\r\nat java.base/java.util.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:762)\r\nat io.confluent.connect.utils.validators.all.ConfigValidation.callValidators(ConfigValidation.java:222)\r\nat io.confluent.connect.utils.validators.all.ConfigValidation.validate(ConfigValidation.java:182)\r\nat io.confluent.connect.azure.datalake.gen2.AzureDataLakeGen2SinkConnector.validate(AzureDataLakeGen2SinkConnector.java:97)\r\nat org.apache.kafka.connect.runtime.AbstractHerder.validateConnectorConfig(AbstractHerder.java:641)\r\nat org.apache.kafka.connect.runtime.AbstractHerder.lambda$validateConnectorConfig$7(AbstractHerder.java:493)\r\nat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\r\nat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)\r\nat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\nat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n... 1 more\r\n```", "comments": [], "derived_tasks": {"summarization": "[ABFS]: Throw HTTPException when AAD token fetch fails ", "classification": "feature", "qna": {"question": "What is the issue '[ABFS]: Throw HTTPException when AAD token fetch fails ' about?", "answer": "Reported by [~enigma25] :\r\nIn [this code snippet](https://github.com/Azure/azure-data-lake-store-java/blob/26eca936da60286aa70b0dd7823ff5e627987bc4/src/main/java/com/microsoft/azure/datalake/store/oauth2/AzureADAuthenticator.java#L252-L293) from AzureADAuthenticator, the `conn` is being returned as null due to a (possibly) corrupted connection between Azure Data Lake Store Client and AAD while fetching AAD token. The code snippet linked, runs into an NPE. I wanted to know from the maintainers and community if this bug/symptom has been seen by them earlier and what might be the best way to handle this? Secondly, I wanted to know the right place for the fix too - whether it should be the application code or the SDK code itself should handle such NPEs and fail more gracefully?\r\nOpen to thoughts and comments.\r\nCheers,\r\nNikhil\r\n\u00a0\r\n\u00a0\r\n```\r\njava.util.concurrent.ExecutionException: java.lang.NullPointerException: Cannot invoke \"java.io.InputStream.read(byte[], int, int)\" because \"inStream\" is null\r\nat org.apache.kafka.connect.util.ConvertingFutureCallback.result(ConvertingFutureCallback.java:135)\r\nat org.apache.kafka.connect.util.ConvertingFutureCallback.get(ConvertingFutureCallback.java:122)\r\nat org.apache.kafka.connect.runtime.rest.resources.ConnectorPluginsResource.validateConfigs(ConnectorPluginsResource.java:129)\r\nat java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103)\r\nat java.base/java.lang.reflect.Method.invoke(Method.java:580)\r\nat org.glassfish.jersey.server.model.internal.ResourceMethodInvocationHandlerFactory.lambda$static$0(ResourceMethodInvocationHandlerFactory.java:52)\r\nat org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher$1.run(AbstractJavaResourceMethodDispatcher.java:134)\r\nat org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.invoke(AbstractJavaResourceMethodDispatcher.java:177)\r\nat org.glassfish.jersey.server.model.internal.JavaResourceMethodDispatcherProvider$TypeOutInvoker.doDispatch(JavaResourceMethodDispatcherProvider.java:219)\r\nat org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.dispatch(AbstractJavaResourceMethodDispatcher.java:81)\r\nat org.glassfish.jersey.server.model.ResourceMethodInvoker.invoke(ResourceMethodInvoker.java:478)\r\nat org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:400)\r\nat org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:81)\r\nat org.glassfish.jersey.server.ServerRuntime$1.run(ServerRuntime.java:256)\r\nat org.glassfish.jersey.internal.Errors$1.call(Errors.java:248)\r\nat org.glassfish.jersey.internal.Errors$1.call(Errors.java:244)\r\nat org.glassfish.jersey.internal.Errors.process(Errors.java:292)\r\nat org.glassfish.jersey.internal.Errors.process(Errors.java:274)\r\nat org.glassfish.jersey.internal.Errors.process(Errors.java:244)\r\nat org.glassfish.jersey.process.internal.RequestScope.runInScope(RequestScope.java:265)\r\nat org.glassfish.jersey.server.ServerRuntime.process(ServerRuntime.java:235)\r\nat org.glassfish.jersey.server.ApplicationHandler.handle(ApplicationHandler.java:684)\r\nat org.glassfish.jersey.servlet.WebComponent.serviceImpl(WebComponent.java:394)\r\nat org.glassfish.jersey.servlet.WebComponent.service(WebComponent.java:346)\r\nat org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:358)\r\nat org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:311)\r\nat org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:205)\r\nat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)\r\nat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:554)\r\nat org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)\r\nat org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:1624)\r\nat org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)\r\nat org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1440)\r\nat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)\r\nat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:505)\r\nat org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:1594)\r\nat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)\r\nat org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1355)\r\nat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\r\nat org.eclipse.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:234)\r\nat org.eclipse.jetty.server.handler.StatisticsHandler.handle(StatisticsHandler.java:181)\r\nat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\r\nat org.eclipse.jetty.server.Server.handle(Server.java:516)\r\nat org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:487)\r\nat org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:732)\r\nat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:479)\r\nat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\r\nat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\r\nat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)\r\nat org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\r\nat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)\r\nat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)\r\nat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)\r\nat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)\r\nat org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409)\r\nat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)\r\nat org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)\r\nat java.base/java.lang.Thread.run(Thread.java:1583)\r\nCaused by: java.lang.NullPointerException: Cannot invoke \"java.io.InputStream.read(byte[], int, int)\" because \"inStream\" is null\r\nat org.apache.hadoop.fs.azurebfs.oauth2.AzureADAuthenticator.consumeInputStream(AzureADAuthenticator.java:345)\r\nat org.apache.hadoop.fs.azurebfs.oauth2.AzureADAuthenticator.getTokenSingleCall(AzureADAuthenticator.java:275)\r\nat org.apache.hadoop.fs.azurebfs.oauth2.AzureADAuthenticator.getTokenCall(AzureADAuthenticator.java:216)\r\nat org.apache.hadoop.fs.azurebfs.oauth2.AzureADAuthenticator.getTokenUsingClientCreds(AzureADAuthenticator.java:95)\r\nat org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider.refreshToken(ClientCredsTokenProvider.java:58)\r\nat org.apache.hadoop.fs.azurebfs.oauth2.AccessTokenProvider.getToken(AccessTokenProvider.java:50)\r\nat org.apache.hadoop.fs.azurebfs.services.AbfsClient.getAccessToken(AbfsClient.java:583)\r\nat org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.executeHttpOperation(AbfsRestOperation.java:162)\r\nat org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.execute(AbfsRestOperation.java:134)\r\nat org.apache.hadoop.fs.azurebfs.services.AbfsClient.getFilesystemProperties(AbfsClient.java:205)\r\nat io.confluent.connect.azure.datalake.gen2.validation.Validations.verifyClient(Validations.java:175)\r\nat io.confluent.connect.azure.datalake.gen2.validation.Validations.createAndValidateClient(Validations.java:395)\r\nat io.confluent.connect.azure.datalake.gen2.validation.Validations.validateAll(Validations.java:131)\r\nat io.confluent.connect.utils.validators.all.ConfigValidation.lambda$callValidators$0(ConfigValidation.java:222)\r\nat java.base/java.util.Spliterators$ArraySpliterator.forEachRemaining(Spliterators.java:1024)\r\nat java.base/java.util.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:762)\r\nat io.confluent.connect.utils.validators.all.ConfigValidation.callValidators(ConfigValidation.java:222)\r\nat io.confluent.connect.utils.validators.all.ConfigValidation.validate(ConfigValidation.java:182)\r\nat io.confluent.connect.azure.datalake.gen2.AzureDataLakeGen2SinkConnector.validate(AzureDataLakeGen2SinkConnector.java:97)\r\nat org.apache.kafka.connect.runtime.AbstractHerder.validateConnectorConfig(AbstractHerder.java:641)\r\nat org.apache.kafka.connect.runtime.AbstractHerder.lambda$validateConnectorConfig$7(AbstractHerder.java:493)\r\nat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\r\nat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)\r\nat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\nat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n... 1 more\r\n```"}}}
{"project": "HADOOP", "issue_key": "HADOOP-19717", "title": "Resolve build error caused by missing Checker Framework (NonNull not recognized)", "status": "Resolved", "reporter": "Shilun Fan", "assignee": "Shilun Fan", "priority": "Major", "labels": ["pull-request-available"], "created": "2025-10-07T03:49:55.000+0000", "updated": "2025-10-20T16:32:20.000+0000", "description": "In the recent build, we encountered the following issue: *org.checkerframework.checker.nullness.qual.NonNull* could not be recognized, and the following error was observed.\r\n{code:java}\r\n[ERROR] /home/jenkins/jenkins-agent/workspace/hadoop-multibranch_PR-8011/ubuntu-focal/src/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterRpcServer.java:[216,50] package org.checkerframework.checker.nullness.qual does not exist\r\n[ERROR] /home/jenkins/jenkins-agent/workspace/hadoop-multibranch_PR-8011/ubuntu-focal/src/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterRpcServer.java:[2509,30] cannot find symbol\r\n[ERROR]   symbol:   class NonNull\r\n[ERROR]   location: class org.apache.hadoop.hdfs.server.federation.router.RouterRpcServer.AsyncThreadFactory\r\n {code}\r\nI checked the usage in the related modules, and we should use *org.apache.hadoop.thirdparty.org.checkerframework.checker.nullness.qual.NonNull* instead of directly using {*}org.checkerframework.checker.nullness.qual.NonNull{*}.\r\n\r\n\u00a0", "comments": [], "derived_tasks": {"summarization": "Resolve build error caused by missing Checker Framework (NonNull not recognized)", "classification": "feature", "qna": {"question": "What is the issue 'Resolve build error caused by missing Checker Framework (NonNull not recognized)' about?", "answer": "In the recent build, we encountered the following issue: *org.checkerframework.checker.nullness.qual.NonNull* could not be recognized, and the following error was observed.\r\n{code:java}\r\n[ERROR] /home/jenkins/jenkins-agent/workspace/hadoop-multibranch_PR-8011/ubuntu-focal/src/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterRpcServer.java:[216,50] package org.checkerframework.checker.nullness.qual does not exist\r\n[ERROR] /home/jenkins/jenkins-agent/workspace/hadoop-multibranch_PR-8011/ubuntu-focal/src/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterRpcServer.java:[2509,30] cannot find symbol\r\n[ERROR]   symbol:   class NonNull\r\n[ERROR]   location: class org.apache.hadoop.hdfs.server.federation.router.RouterRpcServer.AsyncThreadFactory\r\n {code}\r\nI checked the usage in the related modules, and we should use *org.apache.hadoop.thirdparty.org.checkerframework.checker.nullness.qual.NonNull* instead of directly using {*}org.checkerframework.checker.nullness.qual.NonNull{*}.\r\n\r\n\u00a0"}}}
{"project": "HADOOP", "issue_key": "HADOOP-19716", "title": "Create lean docker image", "status": "Resolved", "reporter": "Attila Doroszlai", "assignee": "Attila Doroszlai", "priority": "Minor", "labels": ["pull-request-available"], "created": "2025-10-06T09:52:36.000+0000", "updated": "2025-10-09T10:04:45.000+0000", "description": "Create a new docker image based on the lean tarball.\r\n\r\nhadoop-3.4.2-lean.tar.gz", "comments": [], "derived_tasks": {"summarization": "Create lean docker image", "classification": "feature", "qna": {"question": "What is the issue 'Create lean docker image' about?", "answer": "Create a new docker image based on the lean tarball.\r\n\r\nhadoop-3.4.2-lean.tar.gz"}}}
{"project": "HADOOP", "issue_key": "HADOOP-19715", "title": "Update restrict-imports-enforcer-rule from 2.0.0 to 2.6.1", "status": "In Progress", "reporter": "Shilun Fan", "assignee": "Shilun Fan", "priority": "Major", "labels": ["pull-request-available"], "created": "2025-10-06T01:54:13.000+0000", "updated": "2025-10-09T02:36:22.000+0000", "description": "The project is currently using {{restrict-imports-enforcer-rule}} version {*}2.0.0{*}, which was released in *2021* and is now outdated. Since then, several new versions have been released with important bug fixes, performance improvements, and enhanced compatibility. To maintain a modern and stable build environment, it is necessary to upgrade to version {*}2.6.1{*}.", "comments": [], "derived_tasks": {"summarization": "Update restrict-imports-enforcer-rule from 2.0.0 to 2.6.1", "classification": "feature", "qna": {"question": "What is the issue 'Update restrict-imports-enforcer-rule from 2.0.0 to 2.6.1' about?", "answer": "The project is currently using {{restrict-imports-enforcer-rule}} version {*}2.0.0{*}, which was released in *2021* and is now outdated. Since then, several new versions have been released with important bug fixes, performance improvements, and enhanced compatibility. To maintain a modern and stable build environment, it is necessary to upgrade to version {*}2.6.1{*}."}}}
{"project": "HADOOP", "issue_key": "HADOOP-19713", "title": "make container build work on macOS Tahoe", "status": "Open", "reporter": "Sangjin Lee", "assignee": null, "priority": "Minor", "labels": [], "created": "2025-10-02T23:15:57.000+0000", "updated": "2025-10-07T19:12:45.000+0000", "description": "macOS Tahoe includes a native container daemon and runtime and it is supposed to be near feature-compatible with docker. In principle, it should be possible to run the container build using the container command line on macOS Tahoe.\r\n\r\nIt would be great if we can add this functionality to the build script so folks can build on macOS natively without creating another VM.", "comments": [], "derived_tasks": {"summarization": "make container build work on macOS Tahoe", "classification": "feature", "qna": {"question": "What is the issue 'make container build work on macOS Tahoe' about?", "answer": "macOS Tahoe includes a native container daemon and runtime and it is supposed to be near feature-compatible with docker. In principle, it should be possible to run the container build using the container command line on macOS Tahoe.\r\n\r\nIt would be great if we can add this functionality to the build script so folks can build on macOS natively without creating another VM."}}}
{"project": "HADOOP", "issue_key": "HADOOP-19712", "title": "S3A: Deadlock observed in IOStatistics EvaluatingStatisticsMap.entryset()", "status": "Resolved", "reporter": "Steve Loughran", "assignee": "Steve Loughran", "priority": "Major", "labels": ["pull-request-available"], "created": "2025-10-01T15:29:06.000+0000", "updated": "2025-10-16T19:22:52.000+0000", "description": "\r\nWe have evidence that `IOStatisticsSupport.snapshotIOStatistics()` can hang, specifically on the statistics collected by an S3AInputStream, whose statistics are merged in to the FS stats in close();\r\n\r\n{code}\r\njdk.internal.misc.Unsafe.park(Native Method)\r\njava.util.concurrent.locks.LockSupport.park(LockSupport.java:341)\r\njava.util.concurrent.ForkJoinTask.awaitDone(ForkJoinTask.java:468)\r\njava.util.concurrent.ForkJoinTask.invoke(ForkJoinTask.java:687)\r\njava.util.stream.ReduceOps$ReduceOp.evaluateParallel(ReduceOps.java:927)\r\njava.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:233)\r\njava.util.stream.ReferencePipeline.collect(ReferencePipeline.java:682)\r\norg.apache.hadoop.fs.statistics.impl.EvaluatingStatisticsMap.entrySet(EvaluatingStatisticsMap.java:166)\r\njava.util.Collections$UnmodifiableMap.entrySet(Collections.java:1529)\r\norg.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.copyMap(IOStatisticsBinding.java:172)\r\norg.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.snapshotMap(IOStatisticsBinding.java:216)\r\norg.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.snapshotMap(IOStatisticsBinding.java:199)\r\norg.apache.hadoop.fs.statistics.IOStatisticsSnapshot.snapshot(IOStatisticsSnapshot.java:165)\r\norg.apache.hadoop.fs.statistics.IOStatisticsSnapshot.<init>(IOStatisticsSnapshot.java:125)\r\norg.apache.hadoop.fs.statistics.IOStatisticsSupport.snapshotIOStatistics(IOStatisticsSupport.java:49)\r\n{code}\r\n\r\nthe code in question is calling `parallelStream()`, which uses a fixed pool of threads shared by all uses of the API\r\n{code}\r\n    Set<Entry<String, E>> r = evalEntries.parallelStream().map((e) ->\r\n        new EntryImpl<>(e.getKey(), e.getValue().apply(e.getKey())))\r\n        .collect(Collectors.toSet());\r\n{code}\r\n\r\nProposed: \r\n* move off parallelStream() to stream()\r\n* review code to if there is any other way this iteration can lead to a deadlock, e.g. the apply() calls.\r\n* could we do the merge more efficiently?\r\n\r\n", "comments": [], "derived_tasks": {"summarization": "S3A: Deadlock observed in IOStatistics EvaluatingStatisticsMap.entryset()", "classification": "feature", "qna": {"question": "What is the issue 'S3A: Deadlock observed in IOStatistics EvaluatingStatisticsMap.entryset()' about?", "answer": "\r\nWe have evidence that `IOStatisticsSupport.snapshotIOStatistics()` can hang, specifically on the statistics collected by an S3AInputStream, whose statistics are merged in to the FS stats in close();\r\n\r\n{code}\r\njdk.internal.misc.Unsafe.park(Native Method)\r\njava.util.concurrent.locks.LockSupport.park(LockSupport.java:341)\r\njava.util.concurrent.ForkJoinTask.awaitDone(ForkJoinTask.java:468)\r\njava.util.concurrent.ForkJoinTask.invoke(ForkJoinTask.java:687)\r\njava.util.stream.ReduceOps$ReduceOp.evaluateParallel(ReduceOps.java:927)\r\njava.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:233)\r\njava.util.stream.ReferencePipeline.collect(ReferencePipeline.java:682)\r\norg.apache.hadoop.fs.statistics.impl.EvaluatingStatisticsMap.entrySet(EvaluatingStatisticsMap.java:166)\r\njava.util.Collections$UnmodifiableMap.entrySet(Collections.java:1529)\r\norg.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.copyMap(IOStatisticsBinding.java:172)\r\norg.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.snapshotMap(IOStatisticsBinding.java:216)\r\norg.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.snapshotMap(IOStatisticsBinding.java:199)\r\norg.apache.hadoop.fs.statistics.IOStatisticsSnapshot.snapshot(IOStatisticsSnapshot.java:165)\r\norg.apache.hadoop.fs.statistics.IOStatisticsSnapshot.<init>(IOStatisticsSnapshot.java:125)\r\norg.apache.hadoop.fs.statistics.IOStatisticsSupport.snapshotIOStatistics(IOStatisticsSupport.java:49)\r\n{code}\r\n\r\nthe code in question is calling `parallelStream()`, which uses a fixed pool of threads shared by all uses of the API\r\n{code}\r\n    Set<Entry<String, E>> r = evalEntries.parallelStream().map((e) ->\r\n        new EntryImpl<>(e.getKey(), e.getValue().apply(e.getKey())))\r\n        .collect(Collectors.toSet());\r\n{code}\r\n\r\nProposed: \r\n* move off parallelStream() to stream()\r\n* review code to if there is any other way this iteration can lead to a deadlock, e.g. the apply() calls.\r\n* could we do the merge more efficiently?\r\n\r\n"}}}
{"project": "HADOOP", "issue_key": "HADOOP-19711", "title": "Upgrade hadoop3 docker scripts to 3.4.2", "status": "Resolved", "reporter": "Shilun Fan", "assignee": "Shilun Fan", "priority": "Major", "labels": ["pull-request-available"], "created": "2025-10-01T08:56:49.000+0000", "updated": "2025-10-09T10:05:02.000+0000", "description": "The Hadoop 3.4.2 version has been released, and we need to update the Hadoop Docker image to version 3.4.2.", "comments": [], "derived_tasks": {"summarization": "Upgrade hadoop3 docker scripts to 3.4.2", "classification": "feature", "qna": {"question": "What is the issue 'Upgrade hadoop3 docker scripts to 3.4.2' about?", "answer": "The Hadoop 3.4.2 version has been released, and we need to update the Hadoop Docker image to version 3.4.2."}}}
{"project": "HADOOP", "issue_key": "HADOOP-19710", "title": "ABFS: Read Buffer Manager V2 should not be allowed untill implemented", "status": "Open", "reporter": "Anuj Modi", "assignee": "Anuj Modi", "priority": "Major", "labels": ["pull-request-available"], "created": "2025-09-29T08:59:21.000+0000", "updated": "2025-10-07T09:54:40.000+0000", "description": "Read Buffer Manager V2 is a Work in progress and not yet fully ready to be used.\r\nThis is to stop any user explicitly enabling the config to enable RBMV2.", "comments": [], "derived_tasks": {"summarization": "ABFS: Read Buffer Manager V2 should not be allowed untill implemented", "classification": "feature", "qna": {"question": "What is the issue 'ABFS: Read Buffer Manager V2 should not be allowed untill implemented' about?", "answer": "Read Buffer Manager V2 is a Work in progress and not yet fully ready to be used.\r\nThis is to stop any user explicitly enabling the config to enable RBMV2."}}}
{"project": "HADOOP", "issue_key": "HADOOP-19709", "title": "[JDK17] Add debian:12 and debian:13 as a build platform with JDK-17 as default", "status": "Open", "reporter": "Vinayakumar B", "assignee": "Vinayakumar B", "priority": "Major", "labels": ["pull-request-available"], "created": "2025-09-27T04:38:55.000+0000", "updated": "2025-10-21T16:28:27.000+0000", "description": "Add a new Dockerfiles to compile Hadoop on latest Debian:12 and Debian:13 with JDK17 as the default compiler.", "comments": [], "derived_tasks": {"summarization": "[JDK17] Add debian:12 and debian:13 as a build platform with JDK-17 as default", "classification": "feature", "qna": {"question": "What is the issue '[JDK17] Add debian:12 and debian:13 as a build platform with JDK-17 as default' about?", "answer": "Add a new Dockerfiles to compile Hadoop on latest Debian:12 and Debian:13 with JDK17 as the default compiler."}}}
{"project": "HADOOP", "issue_key": "HADOOP-19708", "title": "volcano tos: disable shading when -DskipShade is set on a build", "status": "In Progress", "reporter": "Steve Loughran", "assignee": "Steve Loughran", "priority": "Major", "labels": [], "created": "2025-09-26T09:30:31.000+0000", "updated": "2025-10-09T19:59:47.000+0000", "description": "hadoop-tos is shaded, includes things like httpclient5, which leads to a big 4M artifact, with unknown content inside\r\n{code}\r\n 92K    share/hadoop/common/lib/hadoop-aliyun-3.5.0-20250916.124028-685.jar\r\n912K    share/hadoop/common/lib/hadoop-aws-3.5.0-20250916.124028-686.jar\r\n808K    share/hadoop/common/lib/hadoop-azure-3.5.0-20250916.124028-685.jar\r\n 36K    share/hadoop/common/lib/hadoop-azure-datalake-3.5.0-20250916.124028-685.jar\r\n 72K    share/hadoop/common/lib/hadoop-cos-3.5.0-20250916.124028-683.jar\r\n136K    share/hadoop/common/lib/hadoop-gcp-3.5.0-SNAPSHOT.jar\r\n140K    share/hadoop/common/lib/hadoop-huaweicloud-3.5.0-SNAPSHOT.jar\r\n3.8M    share/hadoop/common/lib/hadoop-tos-3.5.0-20250916.124028-202.jar\r\n{code}\r\n\r\nOne thing it includes is yet-another mozilla/public-suffix-list.txt. These are a recurrent PITA and I don't want\r\nto find them surfacing again.\r\n{code}\r\n\r\n15. Required Resources\r\n======================\r\n\r\nresource: mozilla/public-suffix-list.txt\r\n       jar:file:/Users/stevel/Projects/Releases/hadoop-3.5.0-SNAPSHOT/share/hadoop/common/lib/hadoop-tos-3.5.0-20250916.124028-202.jar!/mozilla/public-suffix-list.txt\r\n{code}\r\n\r\nPlan\r\n* Move the shade stage behind a profile; off for ASF releases. Exclude mozilla/public-suffix-list.txt  \r\n* Explicitly declare and manage httpclient5 dependency\r\n* hadoop-cloud-storage pom to include hadoop-tos but not dependencies in build, unless asked.\r\n* LICENSE-binary to declare optional redist of ve-tos-java-sdk-hadoop and its license.\r\n\r\n", "comments": [], "derived_tasks": {"summarization": "volcano tos: disable shading when -DskipShade is set on a build", "classification": "feature", "qna": {"question": "What is the issue 'volcano tos: disable shading when -DskipShade is set on a build' about?", "answer": "hadoop-tos is shaded, includes things like httpclient5, which leads to a big 4M artifact, with unknown content inside\r\n{code}\r\n 92K    share/hadoop/common/lib/hadoop-aliyun-3.5.0-20250916.124028-685.jar\r\n912K    share/hadoop/common/lib/hadoop-aws-3.5.0-20250916.124028-686.jar\r\n808K    share/hadoop/common/lib/hadoop-azure-3.5.0-20250916.124028-685.jar\r\n 36K    share/hadoop/common/lib/hadoop-azure-datalake-3.5.0-20250916.124028-685.jar\r\n 72K    share/hadoop/common/lib/hadoop-cos-3.5.0-20250916.124028-683.jar\r\n136K    share/hadoop/common/lib/hadoop-gcp-3.5.0-SNAPSHOT.jar\r\n140K    share/hadoop/common/lib/hadoop-huaweicloud-3.5.0-SNAPSHOT.jar\r\n3.8M    share/hadoop/common/lib/hadoop-tos-3.5.0-20250916.124028-202.jar\r\n{code}\r\n\r\nOne thing it includes is yet-another mozilla/public-suffix-list.txt. These are a recurrent PITA and I don't want\r\nto find them surfacing again.\r\n{code}\r\n\r\n15. Required Resources\r\n======================\r\n\r\nresource: mozilla/public-suffix-list.txt\r\n       jar:file:/Users/stevel/Projects/Releases/hadoop-3.5.0-SNAPSHOT/share/hadoop/common/lib/hadoop-tos-3.5.0-20250916.124028-202.jar!/mozilla/public-suffix-list.txt\r\n{code}\r\n\r\nPlan\r\n* Move the shade stage behind a profile; off for ASF releases. Exclude mozilla/public-suffix-list.txt  \r\n* Explicitly declare and manage httpclient5 dependency\r\n* hadoop-cloud-storage pom to include hadoop-tos but not dependencies in build, unless asked.\r\n* LICENSE-binary to declare optional redist of ve-tos-java-sdk-hadoop and its license.\r\n\r\n"}}}
{"project": "HADOOP", "issue_key": "HADOOP-19707", "title": "Surefire upgrade leads to increased report output, can cause Jenkins OOM", "status": "Resolved", "reporter": "Michael Smith", "assignee": "Michael Smith", "priority": "Major", "labels": ["pull-request-available"], "created": "2025-09-25T23:37:18.000+0000", "updated": "2025-10-02T19:02:11.000+0000", "description": "The Surefire upgrade to 3.3+ added enableOutErrElements which defaults to true and includes system-out and system-err in the xml artifacts. This significantly increases their size: ~45KB to ~1MB in many cases. In my test environment, that leads to\r\n{code}\r\nRecording test results\r\nERROR: Step \u2018Publish JUnit test result report\u2019 aborted due to exception: \r\njava.lang.OutOfMemoryError: Java heap space\r\n{code}\r\n\r\nCapturing stdout seems useful when doing ad-hoc testing or smaller test runs. I'd propose adding a profile to set enableOutErrElements=false for CI environments where that extra output can cause problems.", "comments": [], "derived_tasks": {"summarization": "Surefire upgrade leads to increased report output, can cause Jenkins OOM", "classification": "feature", "qna": {"question": "What is the issue 'Surefire upgrade leads to increased report output, can cause Jenkins OOM' about?", "answer": "The Surefire upgrade to 3.3+ added enableOutErrElements which defaults to true and includes system-out and system-err in the xml artifacts. This significantly increases their size: ~45KB to ~1MB in many cases. In my test environment, that leads to\r\n{code}\r\nRecording test results\r\nERROR: Step \u2018Publish JUnit test result report\u2019 aborted due to exception: \r\njava.lang.OutOfMemoryError: Java heap space\r\n{code}\r\n\r\nCapturing stdout seems useful when doing ad-hoc testing or smaller test runs. I'd propose adding a profile to set enableOutErrElements=false for CI environments where that extra output can cause problems."}}}
{"project": "HADOOP", "issue_key": "HADOOP-19706", "title": "Support Java Modularity", "status": "Open", "reporter": "Tsz-wo Sze", "assignee": null, "priority": "Major", "labels": [], "created": "2025-09-24T20:03:42.000+0000", "updated": "2025-09-24T20:04:21.000+0000", "description": "This is an umbrella JIRA for supporting Java 9 Modularity.", "comments": [], "derived_tasks": {"summarization": "Support Java Modularity", "classification": "feature", "qna": {"question": "What is the issue 'Support Java Modularity' about?", "answer": "This is an umbrella JIRA for supporting Java 9 Modularity."}}}
{"project": "HADOOP", "issue_key": "HADOOP-19705", "title": "[JDK17] Do not use Long(long) and similar constructors", "status": "Open", "reporter": "Tsz-wo Sze", "assignee": null, "priority": "Major", "labels": [], "created": "2025-09-24T19:00:19.000+0000", "updated": "2025-09-24T19:00:19.000+0000", "description": "'Long(long)' is deprecated since version 9 and marked for removal.", "comments": [], "derived_tasks": {"summarization": "[JDK17] Do not use Long(long) and similar constructors", "classification": "feature", "qna": {"question": "What is the issue '[JDK17] Do not use Long(long) and similar constructors' about?", "answer": "'Long(long)' is deprecated since version 9 and marked for removal."}}}
{"project": "HADOOP", "issue_key": "HADOOP-19703", "title": "UserGroupInformation.java is using a non-support operation in JDK25", "status": "Open", "reporter": "Hugo Costa", "assignee": null, "priority": "Major", "labels": [], "created": "2025-09-24T13:42:31.000+0000", "updated": "2025-09-24T15:26:11.000+0000", "description": "Hello,\r\n\r\nI'm trying to upgrade my version of ParquetJava and I'm seeing the following error locally\r\n{code:java}\r\n\u00a0\u00a0\u00a0 java.lang.UnsupportedOperationException: getSubject is not supported\r\n\r\n\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 at java.base/javax.security.auth.Subject.getSubject(Subject.java:277)\r\n\r\n\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:577)\r\n\r\n\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3852)\r\n\r\n\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3842)\r\n\r\n\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3630)\r\n\r\n\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:557)\r\n\r\n\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:290)\r\n\r\n\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:541)\r\n\r\n\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\r\n\r\n\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 at org.apache.parquet.hadoop.util.HadoopOutputFile.fromPath(HadoopOutputFile.java:58)\r\n\r\n\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 at com.amazon.networkvalidator.parquet.ParquetWriter.write(ParquetWriter.kt:75)\r\n\r\n\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 at com.amazon.networkvalidator.parquet.ParquetWriterTest$test \r\nwriting to parquet$1.invokeSuspend(ParquetWriterTest.kt:88)\r\n\r\n\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 at com.amazon.networkvalidator.parquet.ParquetWriterTest$test writing to parquet$1.invoke(ParquetWriterTest.kt)\r\n\r\n\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 at com.amazon.networkvalidator.parquet.ParquetWriterTest$test writing to parquet$1.invoke(ParquetWriterTest.kt)\r\n\r\n\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 at kotlinx.coroutines.test.TestBuildersKt__TestBuildersKt$runTest$2$1$1.invokeSuspend(TestBuilders.kt:318)\r\n\r\n\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 at kotlin.coroutines.jvm.internal.BaseContinuationImpl.resumeWith(ContinuationImpl.kt:33)\r\n\r\n\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 at kotlinx.coroutines.DispatchedTask.run(DispatchedTask.kt:101)\r\n\r\n\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 at kotlinx.coroutines.test.TestDispatcher.processEvent$kotlinx_coroutines_test(TestDispatcher.kt:24)\r\n\r\n\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 at \r\nkotlinx.coroutines.test.TestCoroutineScheduler.tryRunNextTaskUnless$kotlinx_coroutines_test(TestCoroutineScheduler.kt:99)\r\n\r\n\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 at kotlinx.coroutines.test.TestBuildersKt__TestBuildersKt$runTest$2$1$workRunner$1.invokeSuspend(TestBuilders.kt:327)\r\n\r\n\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 at kotlin.coroutines.jvm.internal.BaseContinuationImpl.resumeWith(ContinuationImpl.kt:33)\r\n\r\n\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 at kotlinx.coroutines.DispatchedTask.run(DispatchedTask.kt:101)\r\n\r\n\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 at kotlinx.coroutines.EventLoopImplBase.processNextEvent(EventLoop.common.kt:263)\r\n\r\n\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 at kotlinx.coroutines.BlockingCoroutine.joinBlocking(Builders.kt:95)\r\n\r\n\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 at kotlinx.coroutines.BuildersKt__BuildersKt.runBlocking(Builders.kt:69)\r\n\r\n\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 at kotlinx.coroutines.BuildersKt.runBlocking(Unknown Source)\r\n\r\n\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 at kotlinx.coroutines.BuildersKt__BuildersKt.runBlocking$default(Builders.kt:47)\r\n\r\n\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 at kotlinx.coroutines.BuildersKt.runBlocking$default(Unknown Source)\r\n\r\n\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 at kotlinx.coroutines.test.TestBuildersJvmKt.createTestResult(TestBuildersJvm.kt:10)\r\n\r\n\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 at kotlinx.coroutines.test.TestBuildersKt__TestBuildersKt.runTest-8Mi8wO0(TestBuilders.kt:310)\r\n\r\n\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 at kotlinx.coroutines.test.TestBuildersKt.runTest-8Mi8wO0(Unknown Source)\r\n\r\n\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 at kotlinx.coroutines.test.TestBuildersKt__TestBuildersKt.runTest-8Mi8wO0(TestBuilders.kt:168)\r\n\r\n\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 at kotlinx.coroutines.test.TestBuildersKt.runTest-8Mi8wO0(Unknown Source)\r\n\r\n\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 at kotlinx.coroutines.test.TestBuildersKt__TestBuildersKt.runTest-8Mi8wO0$default(TestBuilders.kt:160)\r\n\r\n\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 at kotlinx.coroutines.test.TestBuildersKt.runTest-8Mi8wO0$default(Unknown Source)\r\n\r\n\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 at com.amazon.networkvalidator.parquet.ParquetWriterTest.test writing to parquet(ParquetWriterTest.kt:76)\r\n {code}\r\nThe class making this unsupported call is UserGroupInformation, which is part of the common hadoop pkg - https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-common/3.4.2", "comments": [], "derived_tasks": {"summarization": "UserGroupInformation.java is using a non-support operation in JDK25", "classification": "feature", "qna": {"question": "What is the issue 'UserGroupInformation.java is using a non-support operation in JDK25' about?", "answer": "Hello,\r\n\r\nI'm trying to upgrade my version of ParquetJava and I'm seeing the following error locally\r\n{code:java}\r\n\u00a0\u00a0\u00a0 java.lang.UnsupportedOperationException: getSubject is not supported\r\n\r\n\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 at java.base/javax.security.auth.Subject.getSubject(Subject.java:277)\r\n\r\n\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:577)\r\n\r\n\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3852)\r\n\r\n\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3842)\r\n\r\n\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3630)\r\n\r\n\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:557)\r\n\r\n\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:290)\r\n\r\n\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:541)\r\n\r\n\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\r\n\r\n\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 at org.apache.parquet.hadoop.util.HadoopOutputFile.fromPath(HadoopOutputFile.java:58)\r\n\r\n\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 at com.amazon.networkvalidator.parquet.ParquetWriter.write(ParquetWriter.kt:75)\r\n\r\n\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 at com.amazon.networkvalidator.parquet.ParquetWriterTest$test \r\nwriting to parquet$1.invokeSuspend(ParquetWriterTest.kt:88)\r\n\r\n\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 at com.amazon.networkvalidator.parquet.ParquetWriterTest$test writing to parquet$1.invoke(ParquetWriterTest.kt)\r\n\r\n\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 at com.amazon.networkvalidator.parquet.ParquetWriterTest$test writing to parquet$1.invoke(ParquetWriterTest.kt)\r\n\r\n\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 at kotlinx.coroutines.test.TestBuildersKt__TestBuildersKt$runTest$2$1$1.invokeSuspend(TestBuilders.kt:318)\r\n\r\n\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 at kotlin.coroutines.jvm.internal.BaseContinuationImpl.resumeWith(ContinuationImpl.kt:33)\r\n\r\n\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 at kotlinx.coroutines.DispatchedTask.run(DispatchedTask.kt:101)\r\n\r\n\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 at kotlinx.coroutines.test.TestDispatcher.processEvent$kotlinx_coroutines_test(TestDispatcher.kt:24)\r\n\r\n\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 at \r\nkotlinx.coroutines.test.TestCoroutineScheduler.tryRunNextTaskUnless$kotlinx_coroutines_test(TestCoroutineScheduler.kt:99)\r\n\r\n\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 at kotlinx.coroutines.test.TestBuildersKt__TestBuildersKt$runTest$2$1$workRunner$1.invokeSuspend(TestBuilders.kt:327)\r\n\r\n\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 at kotlin.coroutines.jvm.internal.BaseContinuationImpl.resumeWith(ContinuationImpl.kt:33)\r\n\r\n\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 at kotlinx.coroutines.DispatchedTask.run(DispatchedTask.kt:101)\r\n\r\n\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 at kotlinx.coroutines.EventLoopImplBase.processNextEvent(EventLoop.common.kt:263)\r\n\r\n\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 at kotlinx.coroutines.BlockingCoroutine.joinBlocking(Builders.kt:95)\r\n\r\n\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 at kotlinx.coroutines.BuildersKt__BuildersKt.runBlocking(Builders.kt:69)\r\n\r\n\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 at kotlinx.coroutines.BuildersKt.runBlocking(Unknown Source)\r\n\r\n\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 at kotlinx.coroutines.BuildersKt__BuildersKt.runBlocking$default(Builders.kt:47)\r\n\r\n\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 at kotlinx.coroutines.BuildersKt.runBlocking$default(Unknown Source)\r\n\r\n\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 at kotlinx.coroutines.test.TestBuildersJvmKt.createTestResult(TestBuildersJvm.kt:10)\r\n\r\n\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 at kotlinx.coroutines.test.TestBuildersKt__TestBuildersKt.runTest-8Mi8wO0(TestBuilders.kt:310)\r\n\r\n\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 at kotlinx.coroutines.test.TestBuildersKt.runTest-8Mi8wO0(Unknown Source)\r\n\r\n\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 at kotlinx.coroutines.test.TestBuildersKt__TestBuildersKt.runTest-8Mi8wO0(TestBuilders.kt:168)\r\n\r\n\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 at kotlinx.coroutines.test.TestBuildersKt.runTest-8Mi8wO0(Unknown Source)\r\n\r\n\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 at kotlinx.coroutines.test.TestBuildersKt__TestBuildersKt.runTest-8Mi8wO0$default(TestBuilders.kt:160)\r\n\r\n\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 at kotlinx.coroutines.test.TestBuildersKt.runTest-8Mi8wO0$default(Unknown Source)\r\n\r\n\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 at com.amazon.networkvalidator.parquet.ParquetWriterTest.test writing to parquet(ParquetWriterTest.kt:76)\r\n {code}\r\nThe class making this unsupported call is UserGroupInformation, which is part of the common hadoop pkg - https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-common/3.4.2"}}}
{"project": "HADOOP", "issue_key": "HADOOP-19702", "title": "Update non-thirdparty Guava version to  33.4.8-jre", "status": "Resolved", "reporter": "Istvan Toth", "assignee": "Istvan Toth", "priority": "Major", "labels": ["pull-request-available"], "created": "2025-09-23T17:28:42.000+0000", "updated": "2025-10-03T06:56:03.000+0000", "description": "Keep in sync with recently upgraded thirdparty Guava", "comments": [], "derived_tasks": {"summarization": "Update non-thirdparty Guava version to  33.4.8-jre", "classification": "feature", "qna": {"question": "What is the issue 'Update non-thirdparty Guava version to  33.4.8-jre' about?", "answer": "Keep in sync with recently upgraded thirdparty Guava"}}}
{"project": "HADOOP", "issue_key": "HADOOP-19701", "title": "Remove invalid `licenses` field of `hadoop-aliyun` SBOM by upgradding `cyclonedx` to 2.9.1", "status": "Resolved", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "priority": "Major", "labels": ["pull-request-available"], "created": "2025-09-23T04:55:31.000+0000", "updated": "2025-09-24T04:18:44.000+0000", "description": null, "comments": [], "derived_tasks": {"summarization": "Remove invalid `licenses` field of `hadoop-aliyun` SBOM by upgradding `cyclonedx` to 2.9.1", "classification": "feature", "qna": {"question": "What is the issue 'Remove invalid `licenses` field of `hadoop-aliyun` SBOM by upgradding `cyclonedx` to 2.9.1' about?", "answer": null}}}
{"project": "HADOOP", "issue_key": "HADOOP-19700", "title": "hadoop-thirdparty build to update maven plugin dependencies", "status": "Resolved", "reporter": "Steve Loughran", "assignee": "Steve Loughran", "priority": "Major", "labels": ["pull-request-available"], "created": "2025-09-22T13:05:13.000+0000", "updated": "2025-09-30T10:02:32.000+0000", "description": "github action builds of PRs for hadoopHthirdparty fail because of throttling NVE throttling of requests; needs an update to a later version with either retries or use of a github source cve list.\r\n\r\ndependency checker 11+ \r\n\r\n{code}\r\nMandatory Upgrade Notice\r\nUpgrading to 10.0.2 or later is mandatory\r\n\r\nOlder versions of dependency-check are causing numerous, duplicative requests that end in processing failures are causing unnecassary load on the NVD API. Dependency-check 10.0.2 uses an updated User-Agent header that will allow the NVD to block calls from the older client.\r\n{code}\r\n\r\n----\r\n\r\nThe upgraded dependency checker now *requires* java11+, and *prefers* the provision of an API key for the national vulnerabilities database. It also skips the sonatype check as that no longer supports anonymous checks at all\r\n", "comments": [], "derived_tasks": {"summarization": "hadoop-thirdparty build to update maven plugin dependencies", "classification": "feature", "qna": {"question": "What is the issue 'hadoop-thirdparty build to update maven plugin dependencies' about?", "answer": "github action builds of PRs for hadoopHthirdparty fail because of throttling NVE throttling of requests; needs an update to a later version with either retries or use of a github source cve list.\r\n\r\ndependency checker 11+ \r\n\r\n{code}\r\nMandatory Upgrade Notice\r\nUpgrading to 10.0.2 or later is mandatory\r\n\r\nOlder versions of dependency-check are causing numerous, duplicative requests that end in processing failures are causing unnecassary load on the NVD API. Dependency-check 10.0.2 uses an updated User-Agent header that will allow the NVD to block calls from the older client.\r\n{code}\r\n\r\n----\r\n\r\nThe upgraded dependency checker now *requires* java11+, and *prefers* the provision of an API key for the national vulnerabilities database. It also skips the sonatype check as that no longer supports anonymous checks at all\r\n"}}}
{"project": "HADOOP", "issue_key": "HADOOP-19698", "title": "S3A Analytics-Accelerator: Update LICENSE-binary", "status": "Resolved", "reporter": "Ahmar Suhail", "assignee": "Ahmar Suhail", "priority": "Major", "labels": ["pull-request-available"], "created": "2025-09-18T10:46:22.000+0000", "updated": "2025-09-18T13:23:56.000+0000", "description": "update LICENSE-binary to include AAL dependency\u00a0", "comments": [], "derived_tasks": {"summarization": "S3A Analytics-Accelerator: Update LICENSE-binary", "classification": "feature", "qna": {"question": "What is the issue 'S3A Analytics-Accelerator: Update LICENSE-binary' about?", "answer": "update LICENSE-binary to include AAL dependency\u00a0"}}}
{"project": "HADOOP", "issue_key": "HADOOP-19697", "title": "google gs connector registration failing", "status": "Open", "reporter": "Steve Loughran", "assignee": null, "priority": "Blocker", "labels": [], "created": "2025-09-17T14:04:27.000+0000", "updated": "2025-10-08T04:44:25.000+0000", "description": "Surfaced during HADOOP-19696 and work with all the cloud connectors on the classpath.\r\n\r\nThere's a missing dependency causing the gcs connector to fail to register *when the first filesystem is instantiated*, because the service registration process loads the gcs connector class, instantiates one and asks for its schema.\r\n\r\nAs well as a sign of a problem, it's better to just add an entry in core-default.xml as this saves all classloading overhead. It's what the other asf bundled ones do.", "comments": [], "derived_tasks": {"summarization": "google gs connector registration failing", "classification": "feature", "qna": {"question": "What is the issue 'google gs connector registration failing' about?", "answer": "Surfaced during HADOOP-19696 and work with all the cloud connectors on the classpath.\r\n\r\nThere's a missing dependency causing the gcs connector to fail to register *when the first filesystem is instantiated*, because the service registration process loads the gcs connector class, instantiates one and asks for its schema.\r\n\r\nAs well as a sign of a problem, it's better to just add an entry in core-default.xml as this saves all classloading overhead. It's what the other asf bundled ones do."}}}
{"project": "HADOOP", "issue_key": "HADOOP-19696", "title": "hadoop binary distribution to move cloud connectors to hadoop common/lib", "status": "Open", "reporter": "Steve Loughran", "assignee": "Steve Loughran", "priority": "Major", "labels": ["pull-request-available"], "created": "2025-09-17T13:36:12.000+0000", "updated": "2025-10-21T22:22:37.000+0000", "description": "Place all the cloud connector hadoop-* artifacts and dependencies into hadoop/common/lib so that the stores can be directly accessed.\r\n\r\n* filesystem operations against abfs, s3a, gcs, etc don't need any effort setting things up. \r\n* Releases without the aws bundle.jar can be trivially updated by adding any version of the sdk libraries to the common/lib dir. \r\n\r\nThis adds a lot more stuff into the distribution, so I'm doing the following design\r\n* all hadoop-* modules in common/lib\r\n* minimal dependencies for hadoop-azure and hadoop-gcs (once we get those right!)\r\n* hadoop-aws: everything except bundle.jar\r\n* other connectors: only included with explicit profiles.\r\n\r\nASF releases will support azure out the box, the others once you add the dependencies. And anyone can build their own release with everything\r\n\r\nOne concern here, we make hadoop-cloud-storage artifact incomplete at pulling in things when depended on. We may need a separate module for the distro setup.\r\n\r\nNoticed during this that the hadoop-tos component is shaded and includes stuff (httpclient5) that we need under control. Filed HADOOP-19708 and incorporating here. \r\n\r\n\r\n", "comments": [], "derived_tasks": {"summarization": "hadoop binary distribution to move cloud connectors to hadoop common/lib", "classification": "feature", "qna": {"question": "What is the issue 'hadoop binary distribution to move cloud connectors to hadoop common/lib' about?", "answer": "Place all the cloud connector hadoop-* artifacts and dependencies into hadoop/common/lib so that the stores can be directly accessed.\r\n\r\n* filesystem operations against abfs, s3a, gcs, etc don't need any effort setting things up. \r\n* Releases without the aws bundle.jar can be trivially updated by adding any version of the sdk libraries to the common/lib dir. \r\n\r\nThis adds a lot more stuff into the distribution, so I'm doing the following design\r\n* all hadoop-* modules in common/lib\r\n* minimal dependencies for hadoop-azure and hadoop-gcs (once we get those right!)\r\n* hadoop-aws: everything except bundle.jar\r\n* other connectors: only included with explicit profiles.\r\n\r\nASF releases will support azure out the box, the others once you add the dependencies. And anyone can build their own release with everything\r\n\r\nOne concern here, we make hadoop-cloud-storage artifact incomplete at pulling in things when depended on. We may need a separate module for the distro setup.\r\n\r\nNoticed during this that the hadoop-tos component is shaded and includes stuff (httpclient5) that we need under control. Filed HADOOP-19708 and incorporating here. \r\n\r\n\r\n"}}}
{"project": "HADOOP", "issue_key": "HADOOP-19695", "title": "Add dual-stack/IPv6 Support to HttpServer2", "status": "Resolved", "reporter": "Ferenc Erdelyi", "assignee": "Ferenc Erdelyi", "priority": "Minor", "labels": ["pull-request-available"], "created": "2025-09-17T11:21:23.000+0000", "updated": "2025-10-13T08:48:57.000+0000", "description": "To support clients connecting to JHS via IPv6, we need to equip the YARN WebApp class to bind to an IPv6 address. WebApp uses the HttpServer2, and adding the IPv6 connector to this class makes the solution more elegant.\r\n\r\nTo enable dual-stack or IPv6 support, use InetAddress.getAllByName(hostname) to resolve the IP addresses of a host.\r\nWhen the system property java.net.preferIPv4Stack is set to true, only IPv4 addresses are returned, and any IPv6 addresses are ignored, so no extra check is needed to exclude IPv6.\r\nWhen java.net.preferIPv4Stack is false, both IPv4 and IPv6 addresses may be returned, and any IPv6 addresses will also be added as connectors.\r\nTo disable IPv4, you need to configure the OS at the system level.\r\n ", "comments": [], "derived_tasks": {"summarization": "Add dual-stack/IPv6 Support to HttpServer2", "classification": "feature", "qna": {"question": "What is the issue 'Add dual-stack/IPv6 Support to HttpServer2' about?", "answer": "To support clients connecting to JHS via IPv6, we need to equip the YARN WebApp class to bind to an IPv6 address. WebApp uses the HttpServer2, and adding the IPv6 connector to this class makes the solution more elegant.\r\n\r\nTo enable dual-stack or IPv6 support, use InetAddress.getAllByName(hostname) to resolve the IP addresses of a host.\r\nWhen the system property java.net.preferIPv4Stack is set to true, only IPv4 addresses are returned, and any IPv6 addresses are ignored, so no extra check is needed to exclude IPv6.\r\nWhen java.net.preferIPv4Stack is false, both IPv4 and IPv6 addresses may be returned, and any IPv6 addresses will also be added as connectors.\r\nTo disable IPv4, you need to configure the OS at the system level.\r\n "}}}
{"project": "HADOOP", "issue_key": "HADOOP-19694", "title": "Bump guava to  33.4.8-jre due to EOL", "status": "Resolved", "reporter": "Rohit Kumar", "assignee": "Rohit Kumar", "priority": "Major", "labels": ["pull-request-available"], "created": "2025-09-17T11:01:22.000+0000", "updated": "2025-10-20T16:41:38.000+0000", "description": "We can use the latest 33.4.8-jre version as the current one is quite old.", "comments": [], "derived_tasks": {"summarization": "Bump guava to  33.4.8-jre due to EOL", "classification": "feature", "qna": {"question": "What is the issue 'Bump guava to  33.4.8-jre due to EOL' about?", "answer": "We can use the latest 33.4.8-jre version as the current one is quite old."}}}
{"project": "HADOOP", "issue_key": "HADOOP-19693", "title": "Update Java 24 to 25 in docker images", "status": "Resolved", "reporter": "Istvan Toth", "assignee": "Istvan Toth", "priority": "Major", "labels": ["Java25", "pull-request-available"], "created": "2025-09-17T07:57:36.000+0000", "updated": "2025-10-06T00:33:24.000+0000", "description": "Temurin JDK25 packages are expected to be available shortly, update JDK 24 to 25 in the docker images.", "comments": [], "derived_tasks": {"summarization": "Update Java 24 to 25 in docker images", "classification": "feature", "qna": {"question": "What is the issue 'Update Java 24 to 25 in docker images' about?", "answer": "Temurin JDK25 packages are expected to be available shortly, update JDK 24 to 25 in the docker images."}}}
{"project": "HADOOP", "issue_key": "HADOOP-19692", "title": "Exclude junit 4 transitive dependency", "status": "Resolved", "reporter": "Tsz-wo Sze", "assignee": "Tsz-wo Sze", "priority": "Major", "labels": ["pull-request-available"], "created": "2025-09-16T18:18:40.000+0000", "updated": "2025-09-19T17:06:35.000+0000", "description": "HADOOP-19617 removed direct junit 4 dependency.  However, junit 4 is still pulled transitively by other dependencies.", "comments": [], "derived_tasks": {"summarization": "Exclude junit 4 transitive dependency", "classification": "feature", "qna": {"question": "What is the issue 'Exclude junit 4 transitive dependency' about?", "answer": "HADOOP-19617 removed direct junit 4 dependency.  However, junit 4 is still pulled transitively by other dependencies."}}}
{"project": "HADOOP", "issue_key": "HADOOP-19691", "title": "[JDK17] Disallow JUnit4 Imports After JUnit5 Migration", "status": "Resolved", "reporter": "Shilun Fan", "assignee": "Shilun Fan", "priority": "Major", "labels": ["pull-request-available"], "created": "2025-09-16T03:05:49.000+0000", "updated": "2025-09-24T22:13:59.000+0000", "description": "As our project has fully migrated to JUnit5, we should now enforce a rule that prevents the import and usage of JUnit4 classes (such as org.junit.Test, org.junit.Assert, etc.) to ensure consistency, avoid regressions, and allow safe removal of legacy dependencies.\r\n\r\nThis task involves identifying and eliminating any remaining JUnit4 imports, and introducing static code analysis or linting rules to ban future usage.", "comments": [], "derived_tasks": {"summarization": "[JDK17] Disallow JUnit4 Imports After JUnit5 Migration", "classification": "feature", "qna": {"question": "What is the issue '[JDK17] Disallow JUnit4 Imports After JUnit5 Migration' about?", "answer": "As our project has fully migrated to JUnit5, we should now enforce a rule that prevents the import and usage of JUnit4 classes (such as org.junit.Test, org.junit.Assert, etc.) to ensure consistency, avoid regressions, and allow safe removal of legacy dependencies.\r\n\r\nThis task involves identifying and eliminating any remaining JUnit4 imports, and introducing static code analysis or linting rules to ban future usage."}}}
{"project": "HADOOP", "issue_key": "HADOOP-19690", "title": "Bump commons-lang3 to 3.18.0 due to CVE-2025-48924", "status": "Resolved", "reporter": "PJ Fanning", "assignee": "PJ Fanning", "priority": "Major", "labels": ["pull-request-available"], "created": "2025-09-15T17:53:25.000+0000", "updated": "2025-09-23T05:30:25.000+0000", "description": "https://www.cve.org/CVERecord?id=CVE-2025-48924\r\n\r\nWill update commons-text to 1.14.0 which was released with commons-lang3 3.18.0. Due to https://issues.apache.org/jira/browse/HADOOP-19532 - seems best to upgrade them together.  ", "comments": [], "derived_tasks": {"summarization": "Bump commons-lang3 to 3.18.0 due to CVE-2025-48924", "classification": "feature", "qna": {"question": "What is the issue 'Bump commons-lang3 to 3.18.0 due to CVE-2025-48924' about?", "answer": "https://www.cve.org/CVERecord?id=CVE-2025-48924\r\n\r\nWill update commons-text to 1.14.0 which was released with commons-lang3 3.18.0. Due to https://issues.apache.org/jira/browse/HADOOP-19532 - seems best to upgrade them together.  "}}}
{"project": "HADOOP", "issue_key": "HADOOP-19689", "title": "Bump netty to 4.1.127 due to CVE-2025-58057", "status": "Resolved", "reporter": "PJ Fanning", "assignee": "PJ Fanning", "priority": "Major", "labels": ["pull-request-available"], "created": "2025-09-15T17:45:00.000+0000", "updated": "2025-09-23T05:31:15.000+0000", "description": "https://www.cve.org/CVERecord?id=CVE-2025-58057\r\n\r\nfixed in 4.1.125 but no harm upgrading to latest", "comments": [], "derived_tasks": {"summarization": "Bump netty to 4.1.127 due to CVE-2025-58057", "classification": "feature", "qna": {"question": "What is the issue 'Bump netty to 4.1.127 due to CVE-2025-58057' about?", "answer": "https://www.cve.org/CVERecord?id=CVE-2025-58057\r\n\r\nfixed in 4.1.125 but no harm upgrading to latest"}}}
{"project": "HADOOP", "issue_key": "HADOOP-19688", "title": "S3A: ITestS3ACommitterMRJob failing on Junit5", "status": "Resolved", "reporter": "Steve Loughran", "assignee": "Steve Loughran", "priority": "Major", "labels": ["pull-request-available"], "created": "2025-09-15T17:38:39.000+0000", "updated": "2025-09-16T12:05:03.000+0000", "description": "NPE in test200 of ITestS3ACommitterMRJob.\r\n\r\nCause is\r\n* test dir setup and propagation calls Path.getRoot().toUri() which returns the /home dir\r\n* somehow the locatedFileStatus of that path being 1 so a codepath in FileInputFormat NPEs.\r\n\r\nFix is in test code, though FileInputFormat has its NPE messages improved to help understand what is going wrong. ", "comments": [], "derived_tasks": {"summarization": "S3A: ITestS3ACommitterMRJob failing on Junit5", "classification": "feature", "qna": {"question": "What is the issue 'S3A: ITestS3ACommitterMRJob failing on Junit5' about?", "answer": "NPE in test200 of ITestS3ACommitterMRJob.\r\n\r\nCause is\r\n* test dir setup and propagation calls Path.getRoot().toUri() which returns the /home dir\r\n* somehow the locatedFileStatus of that path being 1 so a codepath in FileInputFormat NPEs.\r\n\r\nFix is in test code, though FileInputFormat has its NPE messages improved to help understand what is going wrong. "}}}
{"project": "HADOOP", "issue_key": "HADOOP-19687", "title": "Upgrade nimbus-jose-jwt to 10.0.2+ due to CVE-2025-53864", "status": "Resolved", "reporter": "Rohit Kumar", "assignee": null, "priority": "Major", "labels": ["pull-request-available"], "created": "2025-09-15T11:29:00.000+0000", "updated": "2025-09-17T10:25:28.000+0000", "description": "*CVE-2025-53864:*\r\n\r\nConnect2id Nimbus JOSE + JWT before 10.0.2 allows a remote attacker to cause a denial of service via a deeply nested JSON object supplied in a JWT claim set, because of uncontrolled recursion. NOTE: this is independent of the Gson 2.11.0 issue because the Connect2id product could have checked the JSON object nesting depth, regardless of what limits (if any) were imposed by Gson.\r\n\r\nSeverity: 6.9 (medium)", "comments": [], "derived_tasks": {"summarization": "Upgrade nimbus-jose-jwt to 10.0.2+ due to CVE-2025-53864", "classification": "feature", "qna": {"question": "What is the issue 'Upgrade nimbus-jose-jwt to 10.0.2+ due to CVE-2025-53864' about?", "answer": "*CVE-2025-53864:*\r\n\r\nConnect2id Nimbus JOSE + JWT before 10.0.2 allows a remote attacker to cause a denial of service via a deeply nested JSON object supplied in a JWT claim set, because of uncontrolled recursion. NOTE: this is independent of the Gson 2.11.0 issue because the Connect2id product could have checked the JSON object nesting depth, regardless of what limits (if any) were imposed by Gson.\r\n\r\nSeverity: 6.9 (medium)"}}}
{"project": "HADOOP", "issue_key": "HADOOP-19685", "title": "Clover breaks on double semicolon", "status": "Resolved", "reporter": "Michael Smith", "assignee": "Michael Smith", "priority": "Major", "labels": ["pull-request-available"], "created": "2025-09-12T20:38:19.000+0000", "updated": "2025-09-12T23:48:23.000+0000", "description": "Building with {{-Pclover}} fails with\r\n{code}\r\n[INFO] Instrumentation error\r\ncom.atlassian.clover.api.CloverException: /home/jenkins/hadoop/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/impl/ITestS3APutIfMatchAndIfNoneMatch.java:43:43:unexpected token: ;\r\n...\r\nFailed to execute goal org.openclover:clover-maven-plugin:4.4.1:setup (clover-setup) on project hadoop-aws: Clover has failed to instrument the source files in the [/home/jenkins/hadoop/hadoop-tools/hadoop-aws/target/clover/src-test-instrumented] directory -> [Help 1]\r\norg.apache.maven.lifecycle.LifecycleExecutionException: Failed to execute goal org.openclover:clover-maven-plugin:4.4.1:setup (clover-setup) on project hadoop-aws: Clover has failed to instrument the source files in the [/home/jenkins/hadoop/hadoop-tools/hadoop-aws/target/clover/src-test-instrumented] directory\r\n{code}\r\n\r\nIt doesn't seem to like a double semicolon in ITestS3APutIfMatchAndIfNoneMatch.java that was added in HADOOP-19256.", "comments": [], "derived_tasks": {"summarization": "Clover breaks on double semicolon", "classification": "feature", "qna": {"question": "What is the issue 'Clover breaks on double semicolon' about?", "answer": "Building with {{-Pclover}} fails with\r\n{code}\r\n[INFO] Instrumentation error\r\ncom.atlassian.clover.api.CloverException: /home/jenkins/hadoop/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/impl/ITestS3APutIfMatchAndIfNoneMatch.java:43:43:unexpected token: ;\r\n...\r\nFailed to execute goal org.openclover:clover-maven-plugin:4.4.1:setup (clover-setup) on project hadoop-aws: Clover has failed to instrument the source files in the [/home/jenkins/hadoop/hadoop-tools/hadoop-aws/target/clover/src-test-instrumented] directory -> [Help 1]\r\norg.apache.maven.lifecycle.LifecycleExecutionException: Failed to execute goal org.openclover:clover-maven-plugin:4.4.1:setup (clover-setup) on project hadoop-aws: Clover has failed to instrument the source files in the [/home/jenkins/hadoop/hadoop-tools/hadoop-aws/target/clover/src-test-instrumented] directory\r\n{code}\r\n\r\nIt doesn't seem to like a double semicolon in ITestS3APutIfMatchAndIfNoneMatch.java that was added in HADOOP-19256."}}}
{"project": "HADOOP", "issue_key": "HADOOP-19684", "title": "Add JDK 21 to Ubuntu 20.04 docker development images", "status": "Resolved", "reporter": "Istvan Toth", "assignee": "Istvan Toth", "priority": "Major", "labels": ["pull-request-available"], "created": "2025-09-10T14:00:14.000+0000", "updated": "2025-09-14T07:14:28.000+0000", "description": "We want to support JDK21, we better have it available in the development image for testing.\r\n", "comments": [], "derived_tasks": {"summarization": "Add JDK 21 to Ubuntu 20.04 docker development images", "classification": "feature", "qna": {"question": "What is the issue 'Add JDK 21 to Ubuntu 20.04 docker development images' about?", "answer": "We want to support JDK21, we better have it available in the development image for testing.\r\n"}}}
{"project": "HADOOP", "issue_key": "HADOOP-19682", "title": "Fix incorrect link from current3 of hadoop-site", "status": "Resolved", "reporter": "Xiaoqiao He", "assignee": "Xiaoqiao He", "priority": "Major", "labels": ["pull-request-available"], "created": "2025-09-09T03:15:33.000+0000", "updated": "2025-09-09T03:57:52.000+0000", "description": "Fix hadoop site incorrect link which reported by https://lists.apache.org/thread/ozvkh0x7qdr8d3vr0tbm5g7jx1jprbct.", "comments": [], "derived_tasks": {"summarization": "Fix incorrect link from current3 of hadoop-site", "classification": "feature", "qna": {"question": "What is the issue 'Fix incorrect link from current3 of hadoop-site' about?", "answer": "Fix hadoop site incorrect link which reported by https://lists.apache.org/thread/ozvkh0x7qdr8d3vr0tbm5g7jx1jprbct."}}}
{"project": "HADOOP", "issue_key": "HADOOP-19681", "title": "Fix S3A failing to initialize S3 buckets having namespace with dot followed by number", "status": "Open", "reporter": "Syed Shameerur Rahman", "assignee": "Syed Shameerur Rahman", "priority": "Major", "labels": ["pull-request-available"], "created": "2025-09-08T08:51:08.000+0000", "updated": "2025-09-24T15:46:51.000+0000", "description": "S3A fails to initialize when S3 bucket namespace is having dot followed by a number.\u00a0\r\n\r\n{*}Specific Problem{*}: URI parsing fails when S3 bucket names contain a dot followed by a number (like {{{}bucket-v1.1-us-east-1{}}}). Java's\r\nURI.getHost() method incorrectly interprets the dot-number pattern as a port specification, causing it to return null.\r\n\r\n\u00a0\r\n\r\n{{}}\r\n{code:java}\r\nhadoop dfs -ls s3a://bucket-v1.1-us-east-1/\r\n\r\nWARNING: Use of this script to execute dfs is deprecated.\r\nWARNING: Attempting to execute replacement \"hdfs dfs\" instead.\r\n\r\n2025-09-08 06:13:06,670 WARN fs.FileSystem: Failed to initialize filesystem s3://bucket-v1.1-us-east-1/: java.lang.IllegalArgumentException: bucket is null/empty\r\n-ls: bucket is null/empty{code}\r\n\u00a0\r\n\r\n{*}Please Note{*}: Although there has been discussion on not allowing S3 buckets with such a namespace ([https://aws.amazon.com/blogs/aws/amazon-s3-path-deprecation-plan-the-rest-of-the-story/]) , Amazon S3 still allows you to create a bucket with such a namespace.", "comments": [], "derived_tasks": {"summarization": "Fix S3A failing to initialize S3 buckets having namespace with dot followed by number", "classification": "feature", "qna": {"question": "What is the issue 'Fix S3A failing to initialize S3 buckets having namespace with dot followed by number' about?", "answer": "S3A fails to initialize when S3 bucket namespace is having dot followed by a number.\u00a0\r\n\r\n{*}Specific Problem{*}: URI parsing fails when S3 bucket names contain a dot followed by a number (like {{{}bucket-v1.1-us-east-1{}}}). Java's\r\nURI.getHost() method incorrectly interprets the dot-number pattern as a port specification, causing it to return null.\r\n\r\n\u00a0\r\n\r\n{{}}\r\n{code:java}\r\nhadoop dfs -ls s3a://bucket-v1.1-us-east-1/\r\n\r\nWARNING: Use of this script to execute dfs is deprecated.\r\nWARNING: Attempting to execute replacement \"hdfs dfs\" instead.\r\n\r\n2025-09-08 06:13:06,670 WARN fs.FileSystem: Failed to initialize filesystem s3://bucket-v1.1-us-east-1/: java.lang.IllegalArgumentException: bucket is null/empty\r\n-ls: bucket is null/empty{code}\r\n\u00a0\r\n\r\n{*}Please Note{*}: Although there has been discussion on not allowing S3 buckets with such a namespace ([https://aws.amazon.com/blogs/aws/amazon-s3-path-deprecation-plan-the-rest-of-the-story/]) , Amazon S3 still allows you to create a bucket with such a namespace."}}}
{"project": "HADOOP", "issue_key": "HADOOP-19680", "title": "Update non-thirdparty Guava version to 32.0.1", "status": "Resolved", "reporter": "Istvan Toth", "assignee": "Istvan Toth", "priority": "Critical", "labels": ["pull-request-available"], "created": "2025-09-08T04:59:06.000+0000", "updated": "2025-09-23T17:30:25.000+0000", "description": "Guava has been already updated to 32.0.1 in hadoop-thirdparty.\r\nHowever, Hadoop also dependency manages the non-thirdparty guava version (coming from dependencies), which is still at 27.0-jre , showing up on static scanners.\r\n\r\nSync the non-thirdparty Guava version to the thirdparty one.", "comments": [], "derived_tasks": {"summarization": "Update non-thirdparty Guava version to 32.0.1", "classification": "feature", "qna": {"question": "What is the issue 'Update non-thirdparty Guava version to 32.0.1' about?", "answer": "Guava has been already updated to 32.0.1 in hadoop-thirdparty.\r\nHowever, Hadoop also dependency manages the non-thirdparty guava version (coming from dependencies), which is still at 27.0-jre , showing up on static scanners.\r\n\r\nSync the non-thirdparty Guava version to the thirdparty one."}}}
{"project": "HADOOP", "issue_key": "HADOOP-19679", "title": "Maven site task fails with Java 17", "status": "Resolved", "reporter": "Michael Smith", "assignee": "Michael Smith", "priority": "Major", "labels": ["pull-request-available"], "created": "2025-09-05T22:47:46.000+0000", "updated": "2025-09-08T16:27:15.000+0000", "description": "If I try to build site with Java 17, I get an IllegalArgumentException from analyze-report\r\n{code}\r\n$ mvn -e -Dmaven.javadoc.skip=true -Dhbase.profile=2.0 -DskipTests -DskipITs clean install site:site\r\n...\r\n[ERROR] Failed to execute goal org.apache.maven.plugins:maven-site-plugin:3.9.1:site (default-cli) on project hadoop-annotations: Error generating maven-dependency-plugin:3.0.2:analyze-report report: IllegalArgumentException -> [Help 1]\r\norg.apache.maven.lifecycle.LifecycleExecutionException: Failed to execute goal org.apache.maven.plugins:maven-site-plugin:3.9.1:site (default-cli) on project hadoop-annotations: Error generating maven-dependency-plugin:3.0.2:analyze-report report\r\n    at org.apache.maven.lifecycle.internal.MojoExecutor.doExecute2 (MojoExecutor.java:347)\r\n    at org.apache.maven.lifecycle.internal.MojoExecutor.doExecute (MojoExecutor.java:330)\r\n    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:213)\r\n    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:175)\r\n    at org.apache.maven.lifecycle.internal.MojoExecutor.access$000 (MojoExecutor.java:76)\r\n    at org.apache.maven.lifecycle.internal.MojoExecutor$1.run (MojoExecutor.java:163)\r\n    at org.apache.maven.plugin.DefaultMojosExecutionStrategy.execute (DefaultMojosExecutionStrategy.java:39)\r\n    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:160)\r\n    at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:105)\r\n    at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:73)\r\n    at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build (SingleThreadedBuilder.java:53)\r\n    at org.apache.maven.lifecycle.internal.LifecycleStarter.execute (LifecycleStarter.java:118)\r\n    at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:261)\r\n    at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:173)\r\n    at org.apache.maven.DefaultMaven.execute (DefaultMaven.java:101)\r\n    at org.apache.maven.cli.MavenCli.execute (MavenCli.java:910)\r\n    at org.apache.maven.cli.MavenCli.doMain (MavenCli.java:283)\r\n    at org.apache.maven.cli.MavenCli.main (MavenCli.java:206)\r\n    at jdk.internal.reflect.NativeMethodAccessorImpl.invoke0 (Native Method)\r\n    at jdk.internal.reflect.NativeMethodAccessorImpl.invoke (NativeMethodAccessorImpl.java:77)\r\n    at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke (DelegatingMethodAccessorImpl.java:43)\r\n    at java.lang.reflect.Method.invoke (Method.java:569)\r\n    at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced (Launcher.java:283)\r\n    at org.codehaus.plexus.classworlds.launcher.Launcher.launch (Launcher.java:226)\r\n    at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode (Launcher.java:407)\r\n    at org.codehaus.plexus.classworlds.launcher.Launcher.main (Launcher.java:348)\r\nCaused by: org.apache.maven.plugin.MojoExecutionException: Error generating maven-dependency-plugin:3.0.2:analyze-report report\r\n    at org.apache.maven.plugins.site.render.SiteMojo.execute (SiteMojo.java:153)\r\n    at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo (DefaultBuildPluginManager.java:126)\r\n    at org.apache.maven.lifecycle.internal.MojoExecutor.doExecute2 (MojoExecutor.java:342)\r\n    at org.apache.maven.lifecycle.internal.MojoExecutor.doExecute (MojoExecutor.java:330)\r\n    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:213)\r\n    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:175)\r\n    at org.apache.maven.lifecycle.internal.MojoExecutor.access$000 (MojoExecutor.java:76)\r\n    at org.apache.maven.lifecycle.internal.MojoExecutor$1.run (MojoExecutor.java:163)\r\n    at org.apache.maven.plugin.DefaultMojosExecutionStrategy.execute (DefaultMojosExecutionStrategy.java:39)\r\n    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:160)\r\n    at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:105)\r\n    at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:73)\r\n    at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build (SingleThreadedBuilder.java:53)\r\n    at org.apache.maven.lifecycle.internal.LifecycleStarter.execute (LifecycleStarter.java:118)\r\n    at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:261)\r\n    at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:173)\r\n    at org.apache.maven.DefaultMaven.execute (DefaultMaven.java:101)\r\n    at org.apache.maven.cli.MavenCli.execute (MavenCli.java:910)\r\n    at org.apache.maven.cli.MavenCli.doMain (MavenCli.java:283)\r\n    at org.apache.maven.cli.MavenCli.main (MavenCli.java:206)\r\n    at jdk.internal.reflect.NativeMethodAccessorImpl.invoke0 (Native Method)\r\n    at jdk.internal.reflect.NativeMethodAccessorImpl.invoke (NativeMethodAccessorImpl.java:77)\r\n    at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke (DelegatingMethodAccessorImpl.java:43)\r\n    at java.lang.reflect.Method.invoke (Method.java:569)\r\n    at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced (Launcher.java:283)\r\n    at org.codehaus.plexus.classworlds.launcher.Launcher.launch (Launcher.java:226)\r\n    at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode (Launcher.java:407)\r\n    at org.codehaus.plexus.classworlds.launcher.Launcher.main (Launcher.java:348)\r\nCaused by: org.apache.maven.doxia.siterenderer.RendererException: Error generating maven-dependency-plugin:3.0.2:analyze-report report\r\n    at org.apache.maven.plugins.site.render.ReportDocumentRenderer.renderDocument (ReportDocumentRenderer.java:247)\r\n    at org.apache.maven.doxia.siterenderer.DefaultSiteRenderer.render (DefaultSiteRenderer.java:349)\r\n    at org.apache.maven.plugins.site.render.SiteMojo.renderLocale (SiteMojo.java:194)\r\n    at org.apache.maven.plugins.site.render.SiteMojo.execute (SiteMojo.java:143)\r\n    at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo (DefaultBuildPluginManager.java:126)\r\n    at org.apache.maven.lifecycle.internal.MojoExecutor.doExecute2 (MojoExecutor.java:342)\r\n    at org.apache.maven.lifecycle.internal.MojoExecutor.doExecute (MojoExecutor.java:330)\r\n    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:213)\r\n    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:175)\r\n    at org.apache.maven.lifecycle.internal.MojoExecutor.access$000 (MojoExecutor.java:76)\r\n    at org.apache.maven.lifecycle.internal.MojoExecutor$1.run (MojoExecutor.java:163)\r\n    at org.apache.maven.plugin.DefaultMojosExecutionStrategy.execute (DefaultMojosExecutionStrategy.java:39)\r\n    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:160)\r\n    at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:105)\r\n    at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:73)\r\n    at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build (SingleThreadedBuilder.java:53)\r\n    at org.apache.maven.lifecycle.internal.LifecycleStarter.execute (LifecycleStarter.java:118)\r\n    at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:261)\r\n    at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:173)\r\n    at org.apache.maven.DefaultMaven.execute (DefaultMaven.java:101)\r\n    at org.apache.maven.cli.MavenCli.execute (MavenCli.java:910)\r\n    at org.apache.maven.cli.MavenCli.doMain (MavenCli.java:283)\r\n    at org.apache.maven.cli.MavenCli.main (MavenCli.java:206)\r\n    at jdk.internal.reflect.NativeMethodAccessorImpl.invoke0 (Native Method)\r\n    at jdk.internal.reflect.NativeMethodAccessorImpl.invoke (NativeMethodAccessorImpl.java:77)\r\n    at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke (DelegatingMethodAccessorImpl.java:43)\r\n    at java.lang.reflect.Method.invoke (Method.java:569)\r\n    at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced (Launcher.java:283)\r\n    at org.codehaus.plexus.classworlds.launcher.Launcher.launch (Launcher.java:226)\r\n    at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode (Launcher.java:407)\r\n    at org.codehaus.plexus.classworlds.launcher.Launcher.main (Launcher.java:348)\r\nCaused by: java.lang.IllegalArgumentException\r\n    at org.objectweb.asm.ClassReader.<init> (Unknown Source)\r\n    at org.objectweb.asm.ClassReader.<init> (Unknown Source)\r\n    at org.objectweb.asm.ClassReader.<init> (Unknown Source)\r\n    at org.apache.maven.shared.dependency.analyzer.asm.DependencyClassFileVisitor.visitClass (DependencyClassFileVisitor.java:65)\r\n    at org.apache.maven.shared.dependency.analyzer.ClassFileVisitorUtils.visitClass (ClassFileVisitorUtils.java:163)\r\n    at org.apache.maven.shared.dependency.analyzer.ClassFileVisitorUtils.acceptDirectory (ClassFileVisitorUtils.java:143)\r\n    at org.apache.maven.shared.dependency.analyzer.ClassFileVisitorUtils.accept (ClassFileVisitorUtils.java:71)\r\n    at org.apache.maven.shared.dependency.analyzer.asm.ASMDependencyAnalyzer.analyze (ASMDependencyAnalyzer.java:50)\r\n    at org.apache.maven.shared.dependency.analyzer.DefaultProjectDependencyAnalyzer.buildDependencyClasses (DefaultProjectDependencyAnalyzer.java:211)\r\n    at org.apache.maven.shared.dependency.analyzer.DefaultProjectDependencyAnalyzer.buildDependencyClasses (DefaultProjectDependencyAnalyzer.java:198)\r\n    at org.apache.maven.shared.dependency.analyzer.DefaultProjectDependencyAnalyzer.analyze (DefaultProjectDependencyAnalyzer.java:74)\r\n    at org.apache.maven.plugins.dependency.analyze.AnalyzeReportMojo.executeReport (AnalyzeReportMojo.java:138)\r\n    at org.apache.maven.reporting.AbstractMavenReport.generate (AbstractMavenReport.java:255)\r\n    at org.apache.maven.plugins.site.render.ReportDocumentRenderer.renderDocument (ReportDocumentRenderer.java:226)\r\n    at org.apache.maven.doxia.siterenderer.DefaultSiteRenderer.render (DefaultSiteRenderer.java:349)\r\n    at org.apache.maven.plugins.site.render.SiteMojo.renderLocale (SiteMojo.java:194)\r\n    at org.apache.maven.plugins.site.render.SiteMojo.execute (SiteMojo.java:143)\r\n    at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo (DefaultBuildPluginManager.java:126)\r\n    at org.apache.maven.lifecycle.internal.MojoExecutor.doExecute2 (MojoExecutor.java:342)\r\n    at org.apache.maven.lifecycle.internal.MojoExecutor.doExecute (MojoExecutor.java:330)\r\n    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:213)\r\n    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:175)\r\n    at org.apache.maven.lifecycle.internal.MojoExecutor.access$000 (MojoExecutor.java:76)\r\n    at org.apache.maven.lifecycle.internal.MojoExecutor$1.run (MojoExecutor.java:163)\r\n    at org.apache.maven.plugin.DefaultMojosExecutionStrategy.execute (DefaultMojosExecutionStrategy.java:39)\r\n    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:160)\r\n    at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:105)\r\n    at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:73)\r\n    at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build (SingleThreadedBuilder.java:53)\r\n    at org.apache.maven.lifecycle.internal.LifecycleStarter.execute (LifecycleStarter.java:118)\r\n    at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:261)\r\n    at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:173)\r\n    at org.apache.maven.DefaultMaven.execute (DefaultMaven.java:101)\r\n    at org.apache.maven.cli.MavenCli.execute (MavenCli.java:910)\r\n    at org.apache.maven.cli.MavenCli.doMain (MavenCli.java:283)\r\n    at org.apache.maven.cli.MavenCli.main (MavenCli.java:206)\r\n    at jdk.internal.reflect.NativeMethodAccessorImpl.invoke0 (Native Method)\r\n    at jdk.internal.reflect.NativeMethodAccessorImpl.invoke (NativeMethodAccessorImpl.java:77)\r\n    at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke (DelegatingMethodAccessorImpl.java:43)\r\n    at java.lang.reflect.Method.invoke (Method.java:569)\r\n    at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced (Launcher.java:283)\r\n    at org.codehaus.plexus.classworlds.launcher.Launcher.launch (Launcher.java:226)\r\n    at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode (Launcher.java:407)\r\n    at org.codehaus.plexus.classworlds.launcher.Launcher.main (Launcher.java:348)\r\n[ERROR] \r\n[ERROR] Re-run Maven using the -X switch to enable full debug logging.\r\n[ERROR] \r\n[ERROR] For more information about the errors and possible solutions, please read the following articles:\r\n[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException\r\n[ERROR] \r\n[ERROR] After correcting the problems, you can resume the build with the command\r\n[ERROR]   mvn <args> -rf :hadoop-annotations\r\n{code}\r\n\r\nUpdating to the latest maven-dependency-plugin version (3.8.1) fixes it for me.", "comments": [], "derived_tasks": {"summarization": "Maven site task fails with Java 17", "classification": "feature", "qna": {"question": "What is the issue 'Maven site task fails with Java 17' about?", "answer": "If I try to build site with Java 17, I get an IllegalArgumentException from analyze-report\r\n{code}\r\n$ mvn -e -Dmaven.javadoc.skip=true -Dhbase.profile=2.0 -DskipTests -DskipITs clean install site:site\r\n...\r\n[ERROR] Failed to execute goal org.apache.maven.plugins:maven-site-plugin:3.9.1:site (default-cli) on project hadoop-annotations: Error generating maven-dependency-plugin:3.0.2:analyze-report report: IllegalArgumentException -> [Help 1]\r\norg.apache.maven.lifecycle.LifecycleExecutionException: Failed to execute goal org.apache.maven.plugins:maven-site-plugin:3.9.1:site (default-cli) on project hadoop-annotations: Error generating maven-dependency-plugin:3.0.2:analyze-report report\r\n    at org.apache.maven.lifecycle.internal.MojoExecutor.doExecute2 (MojoExecutor.java:347)\r\n    at org.apache.maven.lifecycle.internal.MojoExecutor.doExecute (MojoExecutor.java:330)\r\n    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:213)\r\n    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:175)\r\n    at org.apache.maven.lifecycle.internal.MojoExecutor.access$000 (MojoExecutor.java:76)\r\n    at org.apache.maven.lifecycle.internal.MojoExecutor$1.run (MojoExecutor.java:163)\r\n    at org.apache.maven.plugin.DefaultMojosExecutionStrategy.execute (DefaultMojosExecutionStrategy.java:39)\r\n    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:160)\r\n    at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:105)\r\n    at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:73)\r\n    at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build (SingleThreadedBuilder.java:53)\r\n    at org.apache.maven.lifecycle.internal.LifecycleStarter.execute (LifecycleStarter.java:118)\r\n    at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:261)\r\n    at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:173)\r\n    at org.apache.maven.DefaultMaven.execute (DefaultMaven.java:101)\r\n    at org.apache.maven.cli.MavenCli.execute (MavenCli.java:910)\r\n    at org.apache.maven.cli.MavenCli.doMain (MavenCli.java:283)\r\n    at org.apache.maven.cli.MavenCli.main (MavenCli.java:206)\r\n    at jdk.internal.reflect.NativeMethodAccessorImpl.invoke0 (Native Method)\r\n    at jdk.internal.reflect.NativeMethodAccessorImpl.invoke (NativeMethodAccessorImpl.java:77)\r\n    at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke (DelegatingMethodAccessorImpl.java:43)\r\n    at java.lang.reflect.Method.invoke (Method.java:569)\r\n    at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced (Launcher.java:283)\r\n    at org.codehaus.plexus.classworlds.launcher.Launcher.launch (Launcher.java:226)\r\n    at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode (Launcher.java:407)\r\n    at org.codehaus.plexus.classworlds.launcher.Launcher.main (Launcher.java:348)\r\nCaused by: org.apache.maven.plugin.MojoExecutionException: Error generating maven-dependency-plugin:3.0.2:analyze-report report\r\n    at org.apache.maven.plugins.site.render.SiteMojo.execute (SiteMojo.java:153)\r\n    at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo (DefaultBuildPluginManager.java:126)\r\n    at org.apache.maven.lifecycle.internal.MojoExecutor.doExecute2 (MojoExecutor.java:342)\r\n    at org.apache.maven.lifecycle.internal.MojoExecutor.doExecute (MojoExecutor.java:330)\r\n    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:213)\r\n    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:175)\r\n    at org.apache.maven.lifecycle.internal.MojoExecutor.access$000 (MojoExecutor.java:76)\r\n    at org.apache.maven.lifecycle.internal.MojoExecutor$1.run (MojoExecutor.java:163)\r\n    at org.apache.maven.plugin.DefaultMojosExecutionStrategy.execute (DefaultMojosExecutionStrategy.java:39)\r\n    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:160)\r\n    at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:105)\r\n    at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:73)\r\n    at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build (SingleThreadedBuilder.java:53)\r\n    at org.apache.maven.lifecycle.internal.LifecycleStarter.execute (LifecycleStarter.java:118)\r\n    at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:261)\r\n    at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:173)\r\n    at org.apache.maven.DefaultMaven.execute (DefaultMaven.java:101)\r\n    at org.apache.maven.cli.MavenCli.execute (MavenCli.java:910)\r\n    at org.apache.maven.cli.MavenCli.doMain (MavenCli.java:283)\r\n    at org.apache.maven.cli.MavenCli.main (MavenCli.java:206)\r\n    at jdk.internal.reflect.NativeMethodAccessorImpl.invoke0 (Native Method)\r\n    at jdk.internal.reflect.NativeMethodAccessorImpl.invoke (NativeMethodAccessorImpl.java:77)\r\n    at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke (DelegatingMethodAccessorImpl.java:43)\r\n    at java.lang.reflect.Method.invoke (Method.java:569)\r\n    at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced (Launcher.java:283)\r\n    at org.codehaus.plexus.classworlds.launcher.Launcher.launch (Launcher.java:226)\r\n    at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode (Launcher.java:407)\r\n    at org.codehaus.plexus.classworlds.launcher.Launcher.main (Launcher.java:348)\r\nCaused by: org.apache.maven.doxia.siterenderer.RendererException: Error generating maven-dependency-plugin:3.0.2:analyze-report report\r\n    at org.apache.maven.plugins.site.render.ReportDocumentRenderer.renderDocument (ReportDocumentRenderer.java:247)\r\n    at org.apache.maven.doxia.siterenderer.DefaultSiteRenderer.render (DefaultSiteRenderer.java:349)\r\n    at org.apache.maven.plugins.site.render.SiteMojo.renderLocale (SiteMojo.java:194)\r\n    at org.apache.maven.plugins.site.render.SiteMojo.execute (SiteMojo.java:143)\r\n    at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo (DefaultBuildPluginManager.java:126)\r\n    at org.apache.maven.lifecycle.internal.MojoExecutor.doExecute2 (MojoExecutor.java:342)\r\n    at org.apache.maven.lifecycle.internal.MojoExecutor.doExecute (MojoExecutor.java:330)\r\n    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:213)\r\n    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:175)\r\n    at org.apache.maven.lifecycle.internal.MojoExecutor.access$000 (MojoExecutor.java:76)\r\n    at org.apache.maven.lifecycle.internal.MojoExecutor$1.run (MojoExecutor.java:163)\r\n    at org.apache.maven.plugin.DefaultMojosExecutionStrategy.execute (DefaultMojosExecutionStrategy.java:39)\r\n    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:160)\r\n    at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:105)\r\n    at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:73)\r\n    at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build (SingleThreadedBuilder.java:53)\r\n    at org.apache.maven.lifecycle.internal.LifecycleStarter.execute (LifecycleStarter.java:118)\r\n    at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:261)\r\n    at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:173)\r\n    at org.apache.maven.DefaultMaven.execute (DefaultMaven.java:101)\r\n    at org.apache.maven.cli.MavenCli.execute (MavenCli.java:910)\r\n    at org.apache.maven.cli.MavenCli.doMain (MavenCli.java:283)\r\n    at org.apache.maven.cli.MavenCli.main (MavenCli.java:206)\r\n    at jdk.internal.reflect.NativeMethodAccessorImpl.invoke0 (Native Method)\r\n    at jdk.internal.reflect.NativeMethodAccessorImpl.invoke (NativeMethodAccessorImpl.java:77)\r\n    at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke (DelegatingMethodAccessorImpl.java:43)\r\n    at java.lang.reflect.Method.invoke (Method.java:569)\r\n    at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced (Launcher.java:283)\r\n    at org.codehaus.plexus.classworlds.launcher.Launcher.launch (Launcher.java:226)\r\n    at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode (Launcher.java:407)\r\n    at org.codehaus.plexus.classworlds.launcher.Launcher.main (Launcher.java:348)\r\nCaused by: java.lang.IllegalArgumentException\r\n    at org.objectweb.asm.ClassReader.<init> (Unknown Source)\r\n    at org.objectweb.asm.ClassReader.<init> (Unknown Source)\r\n    at org.objectweb.asm.ClassReader.<init> (Unknown Source)\r\n    at org.apache.maven.shared.dependency.analyzer.asm.DependencyClassFileVisitor.visitClass (DependencyClassFileVisitor.java:65)\r\n    at org.apache.maven.shared.dependency.analyzer.ClassFileVisitorUtils.visitClass (ClassFileVisitorUtils.java:163)\r\n    at org.apache.maven.shared.dependency.analyzer.ClassFileVisitorUtils.acceptDirectory (ClassFileVisitorUtils.java:143)\r\n    at org.apache.maven.shared.dependency.analyzer.ClassFileVisitorUtils.accept (ClassFileVisitorUtils.java:71)\r\n    at org.apache.maven.shared.dependency.analyzer.asm.ASMDependencyAnalyzer.analyze (ASMDependencyAnalyzer.java:50)\r\n    at org.apache.maven.shared.dependency.analyzer.DefaultProjectDependencyAnalyzer.buildDependencyClasses (DefaultProjectDependencyAnalyzer.java:211)\r\n    at org.apache.maven.shared.dependency.analyzer.DefaultProjectDependencyAnalyzer.buildDependencyClasses (DefaultProjectDependencyAnalyzer.java:198)\r\n    at org.apache.maven.shared.dependency.analyzer.DefaultProjectDependencyAnalyzer.analyze (DefaultProjectDependencyAnalyzer.java:74)\r\n    at org.apache.maven.plugins.dependency.analyze.AnalyzeReportMojo.executeReport (AnalyzeReportMojo.java:138)\r\n    at org.apache.maven.reporting.AbstractMavenReport.generate (AbstractMavenReport.java:255)\r\n    at org.apache.maven.plugins.site.render.ReportDocumentRenderer.renderDocument (ReportDocumentRenderer.java:226)\r\n    at org.apache.maven.doxia.siterenderer.DefaultSiteRenderer.render (DefaultSiteRenderer.java:349)\r\n    at org.apache.maven.plugins.site.render.SiteMojo.renderLocale (SiteMojo.java:194)\r\n    at org.apache.maven.plugins.site.render.SiteMojo.execute (SiteMojo.java:143)\r\n    at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo (DefaultBuildPluginManager.java:126)\r\n    at org.apache.maven.lifecycle.internal.MojoExecutor.doExecute2 (MojoExecutor.java:342)\r\n    at org.apache.maven.lifecycle.internal.MojoExecutor.doExecute (MojoExecutor.java:330)\r\n    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:213)\r\n    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:175)\r\n    at org.apache.maven.lifecycle.internal.MojoExecutor.access$000 (MojoExecutor.java:76)\r\n    at org.apache.maven.lifecycle.internal.MojoExecutor$1.run (MojoExecutor.java:163)\r\n    at org.apache.maven.plugin.DefaultMojosExecutionStrategy.execute (DefaultMojosExecutionStrategy.java:39)\r\n    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:160)\r\n    at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:105)\r\n    at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:73)\r\n    at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build (SingleThreadedBuilder.java:53)\r\n    at org.apache.maven.lifecycle.internal.LifecycleStarter.execute (LifecycleStarter.java:118)\r\n    at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:261)\r\n    at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:173)\r\n    at org.apache.maven.DefaultMaven.execute (DefaultMaven.java:101)\r\n    at org.apache.maven.cli.MavenCli.execute (MavenCli.java:910)\r\n    at org.apache.maven.cli.MavenCli.doMain (MavenCli.java:283)\r\n    at org.apache.maven.cli.MavenCli.main (MavenCli.java:206)\r\n    at jdk.internal.reflect.NativeMethodAccessorImpl.invoke0 (Native Method)\r\n    at jdk.internal.reflect.NativeMethodAccessorImpl.invoke (NativeMethodAccessorImpl.java:77)\r\n    at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke (DelegatingMethodAccessorImpl.java:43)\r\n    at java.lang.reflect.Method.invoke (Method.java:569)\r\n    at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced (Launcher.java:283)\r\n    at org.codehaus.plexus.classworlds.launcher.Launcher.launch (Launcher.java:226)\r\n    at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode (Launcher.java:407)\r\n    at org.codehaus.plexus.classworlds.launcher.Launcher.main (Launcher.java:348)\r\n[ERROR] \r\n[ERROR] Re-run Maven using the -X switch to enable full debug logging.\r\n[ERROR] \r\n[ERROR] For more information about the errors and possible solutions, please read the following articles:\r\n[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException\r\n[ERROR] \r\n[ERROR] After correcting the problems, you can resume the build with the command\r\n[ERROR]   mvn <args> -rf :hadoop-annotations\r\n{code}\r\n\r\nUpdating to the latest maven-dependency-plugin version (3.8.1) fixes it for me."}}}
{"project": "HADOOP", "issue_key": "HADOOP-19678", "title": "[JDK17] Remove powermock dependency", "status": "Resolved", "reporter": "Tsz-wo Sze", "assignee": "Tsz-wo Sze", "priority": "Major", "labels": ["pull-request-available"], "created": "2025-09-05T18:22:53.000+0000", "updated": "2025-09-08T16:28:18.000+0000", "description": "The powermock dependency is specified in\r\n- hadoop-cloud-storage-project/hadoop-huaweicloud/pom.xml\r\n- hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-documentstore/pom.xml\r\n\r\nbut not used anywhere.  We should remove it.", "comments": [], "derived_tasks": {"summarization": "[JDK17] Remove powermock dependency", "classification": "feature", "qna": {"question": "What is the issue '[JDK17] Remove powermock dependency' about?", "answer": "The powermock dependency is specified in\r\n- hadoop-cloud-storage-project/hadoop-huaweicloud/pom.xml\r\n- hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice-documentstore/pom.xml\r\n\r\nbut not used anywhere.  We should remove it."}}}
{"project": "HADOOP", "issue_key": "HADOOP-19677", "title": "[JDK17] Remove mockito-all 1.10.19 and powermock", "status": "Resolved", "reporter": "Tsz-wo Sze", "assignee": "Tsz-wo Sze", "priority": "Major", "labels": ["pull-request-available"], "created": "2025-09-05T17:51:30.000+0000", "updated": "2025-09-08T16:27:38.000+0000", "description": "- The mockito-all 1.10.19 dependency should be replaced by mockito-core 4.11.\r\n\r\n - The powermock dependency is specified in the pom below. We should replace it with mockito since it is known to be incompatible with JDK17.\r\n -* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-catalog/hadoop-yarn-applications-catalog-webapp/pom.xml\r\n\r\n\u00a0", "comments": [], "derived_tasks": {"summarization": "[JDK17] Remove mockito-all 1.10.19 and powermock", "classification": "feature", "qna": {"question": "What is the issue '[JDK17] Remove mockito-all 1.10.19 and powermock' about?", "answer": "- The mockito-all 1.10.19 dependency should be replaced by mockito-core 4.11.\r\n\r\n - The powermock dependency is specified in the pom below. We should replace it with mockito since it is known to be incompatible with JDK17.\r\n -* hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-catalog/hadoop-yarn-applications-catalog-webapp/pom.xml\r\n\r\n\u00a0"}}}
{"project": "HADOOP", "issue_key": "HADOOP-19676", "title": "ABFS: Enhancing ABFS Driver Metrics for Analytical Usability", "status": "Open", "reporter": "Manish Bhatt", "assignee": "Manish Bhatt", "priority": "Major", "labels": [], "created": "2025-09-04T16:09:30.000+0000", "updated": "2025-09-04T16:09:46.000+0000", "description": null, "comments": [], "derived_tasks": {"summarization": "ABFS: Enhancing ABFS Driver Metrics for Analytical Usability", "classification": "feature", "qna": {"question": "What is the issue 'ABFS: Enhancing ABFS Driver Metrics for Analytical Usability' about?", "answer": null}}}
{"project": "HADOOP", "issue_key": "HADOOP-19675", "title": "Close stale PRs updated over 100 days ago.", "status": "Resolved", "reporter": "Xiaoqiao He", "assignee": "Xiaoqiao He", "priority": "Major", "labels": ["pull-request-available"], "created": "2025-09-04T07:59:48.000+0000", "updated": "2025-09-16T12:42:55.000+0000", "description": "Close stale PRs on GitHub which updated over 100 days ago, base on the voting thread: https://lists.apache.org/thread/7x6c029fp9k62bxt2995dz6q6j6sg0bh", "comments": [], "derived_tasks": {"summarization": "Close stale PRs updated over 100 days ago.", "classification": "feature", "qna": {"question": "What is the issue 'Close stale PRs updated over 100 days ago.' about?", "answer": "Close stale PRs on GitHub which updated over 100 days ago, base on the voting thread: https://lists.apache.org/thread/7x6c029fp9k62bxt2995dz6q6j6sg0bh"}}}
{"project": "HADOOP", "issue_key": "HADOOP-19674", "title": "[JDK 17] Implementation of JAXB-API has not been found on module path or classpath", "status": "Resolved", "reporter": "Bence Kosztolnik", "assignee": "Bence Kosztolnik", "priority": "Major", "labels": ["pull-request-available"], "created": "2025-09-03T11:30:33.000+0000", "updated": "2025-09-09T13:06:35.000+0000", "description": "When i try to run the *org.apache.hadoop.yarn.webapp.TestWebApp* tests over JDK17 i am facing the following errors:\r\n\r\n{noformat}\r\nCaused by: A MultiException has 2 exceptions.  They are:\r\n1. javax.xml.bind.JAXBException: Implementation of JAXB-API has not been found on module path or classpath.\r\n2. java.lang.IllegalStateException: Unable to perform operation: create on org.apache.hadoop.yarn.webapp.MyTestJAXBContextResolver\r\n{noformat}\r\n\r\nRepro steps:\r\n- run: ./start-build-env.sh ubuntu_24\r\n- in docker run: mvn clean install -Pjdk17+ -T 32 -DskipTests \r\n- in hadoop-yarn-common modul run: mvn test -Dtest=org.apache.hadoop.yarn.webapp.TestWebApp\r\n\r\nI found a similar error here:\r\nhttps://issues.apache.org/jira/browse/HDDS-5068\r\n\r\nBased on my understanding the problem is the JDK11+ environments does not have the JAXB runtime, so we have to explicit provide them. Maybe this is just a temporal solution, till the whole Jakarta upgrade can be done.\r\n\r\n{panel:title=Full error log}\r\n{code}\r\n[ERROR] testRobotsText  Time elapsed: 0.064 s  <<< ERROR!\r\norg.apache.hadoop.yarn.webapp.WebAppException: Error starting http server\r\n\tat org.apache.hadoop.yarn.webapp.WebApps$Builder.start(WebApps.java:506)\r\n\tat org.apache.hadoop.yarn.webapp.WebApps$Builder.start(WebApps.java:492)\r\n\tat org.apache.hadoop.yarn.webapp.TestWebApp.testRobotsText(TestWebApp.java:324)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\r\n\tat org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:725)\r\n\tat org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)\r\n\tat org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)\r\n\tat org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:149)\r\n\tat org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:140)\r\n\tat org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestMethod(TimeoutExtension.java:84)\r\n\tat org.junit.jupiter.engine.execution.ExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(ExecutableInvoker.java:115)\r\n\tat org.junit.jupiter.engine.execution.ExecutableInvoker.lambda$invoke$0(ExecutableInvoker.java:105)\r\n\tat org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)\r\n\tat org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)\r\n\tat org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)\r\n\tat org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)\r\n\tat org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:104)\r\n\tat org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:98)\r\n\tat org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$7(TestMethodTestDescriptor.java:214)\r\n\tat org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)\r\n\tat org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:210)\r\n\tat org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:135)\r\n\tat org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:66)\r\n\tat org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)\r\n\tat org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)\r\n\tat org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)\r\n\tat org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)\r\n\tat org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)\r\n\tat org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)\r\n\tat org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)\r\n\tat org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)\r\n\tat java.base/java.util.ArrayList.forEach(ArrayList.java:1511)\r\n\tat org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:41)\r\n\tat org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)\r\n\tat org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)\r\n\tat org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)\r\n\tat org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)\r\n\tat org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)\r\n\tat org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)\r\n\tat org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)\r\n\tat org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)\r\n\tat java.base/java.util.ArrayList.forEach(ArrayList.java:1511)\r\n\tat org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:41)\r\n\tat org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)\r\n\tat org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)\r\n\tat org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)\r\n\tat org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)\r\n\tat org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)\r\n\tat org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)\r\n\tat org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)\r\n\tat org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)\r\n\tat org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.submit(SameThreadHierarchicalTestExecutorService.java:35)\r\n\tat org.junit.platform.engine.support.hierarchical.HierarchicalTestExecutor.execute(HierarchicalTestExecutor.java:57)\r\n\tat org.junit.platform.engine.support.hierarchical.HierarchicalTestEngine.execute(HierarchicalTestEngine.java:54)\r\n\tat org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:107)\r\n\tat org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)\r\n\tat org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)\r\n\tat org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)\r\n\tat org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)\r\n\tat org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)\r\n\tat org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)\r\n\tat org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)\r\n\tat org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)\r\n\tat org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:142)\r\n\tat org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:113)\r\n\tat org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)\r\n\tat org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)\r\n\tat org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)\r\n\tat org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)\r\nCaused by: java.io.IOException: Unable to initialize WebAppContext\r\n\tat org.apache.hadoop.http.HttpServer2.start(HttpServer2.java:1453)\r\n\tat org.apache.hadoop.yarn.webapp.WebApps$Builder.start(WebApps.java:503)\r\n\t... 71 more\r\nCaused by: javax.servlet.ServletException: org.glassfish.jersey.servlet.ServletContainer-640d604==org.glassfish.jersey.servlet.ServletContainer@f679d7ba{jsp=null,order=-1,inst=true,async=true,src=EMBEDDED:null,STARTED}\r\n\tat org.eclipse.jetty.servlet.ServletHolder.initServlet(ServletHolder.java:650)\r\n\tat org.eclipse.jetty.servlet.ServletHolder.initialize(ServletHolder.java:415)\r\n\tat org.eclipse.jetty.servlet.ServletHandler.lambda$initialize$0(ServletHandler.java:750)\r\n\tat java.base/java.util.stream.SortedOps$SizedRefSortingSink.end(SortedOps.java:357)\r\n\tat java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:510)\r\n\tat java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:499)\r\n\tat java.base/java.util.stream.StreamSpliterators$WrappingSpliterator.forEachRemaining(StreamSpliterators.java:310)\r\n\tat java.base/java.util.stream.Streams$ConcatSpliterator.forEachRemaining(Streams.java:735)\r\n\tat java.base/java.util.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:762)\r\n\tat org.eclipse.jetty.servlet.ServletHandler.initialize(ServletHandler.java:774)\r\n\tat org.eclipse.jetty.servlet.ServletContextHandler.startContext(ServletContextHandler.java:379)\r\n\tat org.eclipse.jetty.webapp.WebAppContext.startWebapp(WebAppContext.java:1449)\r\n\tat org.eclipse.jetty.webapp.WebAppContext.startContext(WebAppContext.java:1414)\r\n\tat org.eclipse.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:916)\r\n\tat org.eclipse.jetty.servlet.ServletContextHandler.doStart(ServletContextHandler.java:288)\r\n\tat org.eclipse.jetty.webapp.WebAppContext.doStart(WebAppContext.java:524)\r\n\tat org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:73)\r\n\tat org.eclipse.jetty.util.component.ContainerLifeCycle.start(ContainerLifeCycle.java:169)\r\n\tat org.eclipse.jetty.util.component.ContainerLifeCycle.doStart(ContainerLifeCycle.java:117)\r\n\tat org.eclipse.jetty.server.handler.AbstractHandler.doStart(AbstractHandler.java:97)\r\n\tat org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:73)\r\n\tat org.eclipse.jetty.util.component.ContainerLifeCycle.start(ContainerLifeCycle.java:169)\r\n\tat org.eclipse.jetty.util.component.ContainerLifeCycle.doStart(ContainerLifeCycle.java:110)\r\n\tat org.eclipse.jetty.server.handler.AbstractHandler.doStart(AbstractHandler.java:97)\r\n\tat org.eclipse.jetty.server.handler.StatisticsHandler.doStart(StatisticsHandler.java:264)\r\n\tat org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:73)\r\n\tat org.eclipse.jetty.util.component.ContainerLifeCycle.start(ContainerLifeCycle.java:169)\r\n\tat org.eclipse.jetty.server.Server.start(Server.java:423)\r\n\tat org.eclipse.jetty.util.component.ContainerLifeCycle.doStart(ContainerLifeCycle.java:110)\r\n\tat org.eclipse.jetty.server.handler.AbstractHandler.doStart(AbstractHandler.java:97)\r\n\tat org.eclipse.jetty.server.Server.doStart(Server.java:387)\r\n\tat org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:73)\r\n\tat org.apache.hadoop.http.HttpServer2.start(HttpServer2.java:1416)\r\n\t... 72 more\r\nCaused by: A MultiException has 2 exceptions.  They are:\r\n1. javax.xml.bind.JAXBException: Implementation of JAXB-API has not been found on module path or classpath.\r\n2. java.lang.IllegalStateException: Unable to perform operation: create on org.apache.hadoop.yarn.webapp.MyTestJAXBContextResolver\r\n\r\n\tat org.jvnet.hk2.internal.ClazzCreator.create(ClazzCreator.java:368)\r\n\tat org.jvnet.hk2.internal.SystemDescriptor.create(SystemDescriptor.java:463)\r\n\tat org.jvnet.hk2.internal.SingletonContext$1.compute(SingletonContext.java:59)\r\n\tat org.jvnet.hk2.internal.SingletonContext$1.compute(SingletonContext.java:47)\r\n\tat org.glassfish.hk2.utilities.cache.Cache$OriginThreadAwareFuture$1.call(Cache.java:74)\r\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\r\n\tat org.glassfish.hk2.utilities.cache.Cache$OriginThreadAwareFuture.run(Cache.java:131)\r\n\tat org.glassfish.hk2.utilities.cache.Cache.compute(Cache.java:176)\r\n\tat org.jvnet.hk2.internal.SingletonContext.findOrCreate(SingletonContext.java:98)\r\n\tat org.jvnet.hk2.internal.Utilities.createService(Utilities.java:2102)\r\n\tat org.jvnet.hk2.internal.ServiceLocatorImpl.internalGetAllServiceHandles(ServiceLocatorImpl.java:1481)\r\n\tat org.jvnet.hk2.internal.ServiceLocatorImpl.getAllServices(ServiceLocatorImpl.java:799)\r\n\tat org.glassfish.jersey.inject.hk2.AbstractHk2InjectionManager.getAllInstances(AbstractHk2InjectionManager.java:171)\r\n\tat org.glassfish.jersey.inject.hk2.ImmediateHk2InjectionManager.getAllInstances(ImmediateHk2InjectionManager.java:30)\r\n\tat org.glassfish.jersey.internal.ContextResolverFactory$ContextResolversConfigurator.postInit(ContextResolverFactory.java:69)\r\n\tat org.glassfish.jersey.server.ApplicationHandler.lambda$initialize$2(ApplicationHandler.java:353)\r\n\tat java.base/java.util.Arrays$ArrayList.forEach(Arrays.java:4204)\r\n\tat org.glassfish.jersey.server.ApplicationHandler.initialize(ApplicationHandler.java:353)\r\n\tat org.glassfish.jersey.server.ApplicationHandler.lambda$initialize$1(ApplicationHandler.java:297)\r\n\tat org.glassfish.jersey.internal.Errors.process(Errors.java:292)\r\n\tat org.glassfish.jersey.internal.Errors.process(Errors.java:274)\r\n\tat org.glassfish.jersey.internal.Errors.processWithException(Errors.java:232)\r\n\tat org.glassfish.jersey.server.ApplicationHandler.initialize(ApplicationHandler.java:296)\r\n\tat org.glassfish.jersey.server.ApplicationHandler.<init>(ApplicationHandler.java:261)\r\n\tat org.glassfish.jersey.servlet.WebComponent.<init>(WebComponent.java:314)\r\n\tat org.glassfish.jersey.servlet.ServletContainer.init(ServletContainer.java:154)\r\n\tat org.glassfish.jersey.servlet.ServletContainer.init(ServletContainer.java:360)\r\n\tat javax.servlet.GenericServlet.init(GenericServlet.java:244)\r\n\tat org.eclipse.jetty.servlet.ServletHolder.initServlet(ServletHolder.java:632)\r\n\t... 104 more\r\nCaused by: javax.xml.bind.JAXBException: Implementation of JAXB-API has not been found on module path or classpath.\r\n - with linked exception:\r\n[java.lang.ClassNotFoundException: com.sun.xml.bind.v2.ContextFactory]\r\n\tat javax.xml.bind.ContextFinder.newInstance(ContextFinder.java:232)\r\n\tat javax.xml.bind.ContextFinder.find(ContextFinder.java:375)\r\n\tat javax.xml.bind.JAXBContext.newInstance(JAXBContext.java:691)\r\n\tat javax.xml.bind.JAXBContext.newInstance(JAXBContext.java:632)\r\n\tat org.glassfish.jersey.jettison.JettisonJaxbContext.<init>(JettisonJaxbContext.java:73)\r\n\tat org.glassfish.jersey.jettison.JettisonJaxbContext.<init>(JettisonJaxbContext.java:54)\r\n\tat org.apache.hadoop.yarn.webapp.MyTestJAXBContextResolver.<init>(MyTestJAXBContextResolver.java:45)\r\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)\r\n\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r\n\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)\r\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)\r\n\tat org.glassfish.hk2.utilities.reflection.ReflectionHelper.makeMe(ReflectionHelper.java:1356)\r\n\tat org.jvnet.hk2.internal.ClazzCreator.createMe(ClazzCreator.java:248)\r\n\tat org.jvnet.hk2.internal.ClazzCreator.create(ClazzCreator.java:342)\r\n\t... 132 more\r\nCaused by: java.lang.ClassNotFoundException: com.sun.xml.bind.v2.ContextFactory\r\n\tat java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:641)\r\n\tat java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:188)\r\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\r\n\tat org.eclipse.jetty.webapp.WebAppClassLoader.loadClass(WebAppClassLoader.java:538)\r\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\r\n\tat javax.xml.bind.ServiceLoaderUtil.nullSafeLoadClass(ServiceLoaderUtil.java:92)\r\n\tat javax.xml.bind.ServiceLoaderUtil.safeLoadClass(ServiceLoaderUtil.java:125)\r\n\tat javax.xml.bind.ContextFinder.newInstance(ContextFinder.java:230)\r\n\t... 146 more\r\n{code}\r\n{panel}\r\n\r\n", "comments": [], "derived_tasks": {"summarization": "[JDK 17] Implementation of JAXB-API has not been found on module path or classpath", "classification": "feature", "qna": {"question": "What is the issue '[JDK 17] Implementation of JAXB-API has not been found on module path or classpath' about?", "answer": "When i try to run the *org.apache.hadoop.yarn.webapp.TestWebApp* tests over JDK17 i am facing the following errors:\r\n\r\n{noformat}\r\nCaused by: A MultiException has 2 exceptions.  They are:\r\n1. javax.xml.bind.JAXBException: Implementation of JAXB-API has not been found on module path or classpath.\r\n2. java.lang.IllegalStateException: Unable to perform operation: create on org.apache.hadoop.yarn.webapp.MyTestJAXBContextResolver\r\n{noformat}\r\n\r\nRepro steps:\r\n- run: ./start-build-env.sh ubuntu_24\r\n- in docker run: mvn clean install -Pjdk17+ -T 32 -DskipTests \r\n- in hadoop-yarn-common modul run: mvn test -Dtest=org.apache.hadoop.yarn.webapp.TestWebApp\r\n\r\nI found a similar error here:\r\nhttps://issues.apache.org/jira/browse/HDDS-5068\r\n\r\nBased on my understanding the problem is the JDK11+ environments does not have the JAXB runtime, so we have to explicit provide them. Maybe this is just a temporal solution, till the whole Jakarta upgrade can be done.\r\n\r\n{panel:title=Full error log}\r\n{code}\r\n[ERROR] testRobotsText  Time elapsed: 0.064 s  <<< ERROR!\r\norg.apache.hadoop.yarn.webapp.WebAppException: Error starting http server\r\n\tat org.apache.hadoop.yarn.webapp.WebApps$Builder.start(WebApps.java:506)\r\n\tat org.apache.hadoop.yarn.webapp.WebApps$Builder.start(WebApps.java:492)\r\n\tat org.apache.hadoop.yarn.webapp.TestWebApp.testRobotsText(TestWebApp.java:324)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\r\n\tat org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:725)\r\n\tat org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)\r\n\tat org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)\r\n\tat org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:149)\r\n\tat org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:140)\r\n\tat org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestMethod(TimeoutExtension.java:84)\r\n\tat org.junit.jupiter.engine.execution.ExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(ExecutableInvoker.java:115)\r\n\tat org.junit.jupiter.engine.execution.ExecutableInvoker.lambda$invoke$0(ExecutableInvoker.java:105)\r\n\tat org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)\r\n\tat org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)\r\n\tat org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)\r\n\tat org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)\r\n\tat org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:104)\r\n\tat org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:98)\r\n\tat org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$7(TestMethodTestDescriptor.java:214)\r\n\tat org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)\r\n\tat org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:210)\r\n\tat org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:135)\r\n\tat org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:66)\r\n\tat org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)\r\n\tat org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)\r\n\tat org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)\r\n\tat org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)\r\n\tat org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)\r\n\tat org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)\r\n\tat org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)\r\n\tat org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)\r\n\tat java.base/java.util.ArrayList.forEach(ArrayList.java:1511)\r\n\tat org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:41)\r\n\tat org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)\r\n\tat org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)\r\n\tat org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)\r\n\tat org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)\r\n\tat org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)\r\n\tat org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)\r\n\tat org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)\r\n\tat org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)\r\n\tat java.base/java.util.ArrayList.forEach(ArrayList.java:1511)\r\n\tat org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:41)\r\n\tat org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)\r\n\tat org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)\r\n\tat org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)\r\n\tat org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)\r\n\tat org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)\r\n\tat org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)\r\n\tat org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)\r\n\tat org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)\r\n\tat org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.submit(SameThreadHierarchicalTestExecutorService.java:35)\r\n\tat org.junit.platform.engine.support.hierarchical.HierarchicalTestExecutor.execute(HierarchicalTestExecutor.java:57)\r\n\tat org.junit.platform.engine.support.hierarchical.HierarchicalTestEngine.execute(HierarchicalTestEngine.java:54)\r\n\tat org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:107)\r\n\tat org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)\r\n\tat org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)\r\n\tat org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)\r\n\tat org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)\r\n\tat org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)\r\n\tat org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)\r\n\tat org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)\r\n\tat org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)\r\n\tat org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:142)\r\n\tat org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:113)\r\n\tat org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)\r\n\tat org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)\r\n\tat org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)\r\n\tat org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)\r\nCaused by: java.io.IOException: Unable to initialize WebAppContext\r\n\tat org.apache.hadoop.http.HttpServer2.start(HttpServer2.java:1453)\r\n\tat org.apache.hadoop.yarn.webapp.WebApps$Builder.start(WebApps.java:503)\r\n\t... 71 more\r\nCaused by: javax.servlet.ServletException: org.glassfish.jersey.servlet.ServletContainer-640d604==org.glassfish.jersey.servlet.ServletContainer@f679d7ba{jsp=null,order=-1,inst=true,async=true,src=EMBEDDED:null,STARTED}\r\n\tat org.eclipse.jetty.servlet.ServletHolder.initServlet(ServletHolder.java:650)\r\n\tat org.eclipse.jetty.servlet.ServletHolder.initialize(ServletHolder.java:415)\r\n\tat org.eclipse.jetty.servlet.ServletHandler.lambda$initialize$0(ServletHandler.java:750)\r\n\tat java.base/java.util.stream.SortedOps$SizedRefSortingSink.end(SortedOps.java:357)\r\n\tat java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:510)\r\n\tat java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:499)\r\n\tat java.base/java.util.stream.StreamSpliterators$WrappingSpliterator.forEachRemaining(StreamSpliterators.java:310)\r\n\tat java.base/java.util.stream.Streams$ConcatSpliterator.forEachRemaining(Streams.java:735)\r\n\tat java.base/java.util.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:762)\r\n\tat org.eclipse.jetty.servlet.ServletHandler.initialize(ServletHandler.java:774)\r\n\tat org.eclipse.jetty.servlet.ServletContextHandler.startContext(ServletContextHandler.java:379)\r\n\tat org.eclipse.jetty.webapp.WebAppContext.startWebapp(WebAppContext.java:1449)\r\n\tat org.eclipse.jetty.webapp.WebAppContext.startContext(WebAppContext.java:1414)\r\n\tat org.eclipse.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:916)\r\n\tat org.eclipse.jetty.servlet.ServletContextHandler.doStart(ServletContextHandler.java:288)\r\n\tat org.eclipse.jetty.webapp.WebAppContext.doStart(WebAppContext.java:524)\r\n\tat org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:73)\r\n\tat org.eclipse.jetty.util.component.ContainerLifeCycle.start(ContainerLifeCycle.java:169)\r\n\tat org.eclipse.jetty.util.component.ContainerLifeCycle.doStart(ContainerLifeCycle.java:117)\r\n\tat org.eclipse.jetty.server.handler.AbstractHandler.doStart(AbstractHandler.java:97)\r\n\tat org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:73)\r\n\tat org.eclipse.jetty.util.component.ContainerLifeCycle.start(ContainerLifeCycle.java:169)\r\n\tat org.eclipse.jetty.util.component.ContainerLifeCycle.doStart(ContainerLifeCycle.java:110)\r\n\tat org.eclipse.jetty.server.handler.AbstractHandler.doStart(AbstractHandler.java:97)\r\n\tat org.eclipse.jetty.server.handler.StatisticsHandler.doStart(StatisticsHandler.java:264)\r\n\tat org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:73)\r\n\tat org.eclipse.jetty.util.component.ContainerLifeCycle.start(ContainerLifeCycle.java:169)\r\n\tat org.eclipse.jetty.server.Server.start(Server.java:423)\r\n\tat org.eclipse.jetty.util.component.ContainerLifeCycle.doStart(ContainerLifeCycle.java:110)\r\n\tat org.eclipse.jetty.server.handler.AbstractHandler.doStart(AbstractHandler.java:97)\r\n\tat org.eclipse.jetty.server.Server.doStart(Server.java:387)\r\n\tat org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:73)\r\n\tat org.apache.hadoop.http.HttpServer2.start(HttpServer2.java:1416)\r\n\t... 72 more\r\nCaused by: A MultiException has 2 exceptions.  They are:\r\n1. javax.xml.bind.JAXBException: Implementation of JAXB-API has not been found on module path or classpath.\r\n2. java.lang.IllegalStateException: Unable to perform operation: create on org.apache.hadoop.yarn.webapp.MyTestJAXBContextResolver\r\n\r\n\tat org.jvnet.hk2.internal.ClazzCreator.create(ClazzCreator.java:368)\r\n\tat org.jvnet.hk2.internal.SystemDescriptor.create(SystemDescriptor.java:463)\r\n\tat org.jvnet.hk2.internal.SingletonContext$1.compute(SingletonContext.java:59)\r\n\tat org.jvnet.hk2.internal.SingletonContext$1.compute(SingletonContext.java:47)\r\n\tat org.glassfish.hk2.utilities.cache.Cache$OriginThreadAwareFuture$1.call(Cache.java:74)\r\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\r\n\tat org.glassfish.hk2.utilities.cache.Cache$OriginThreadAwareFuture.run(Cache.java:131)\r\n\tat org.glassfish.hk2.utilities.cache.Cache.compute(Cache.java:176)\r\n\tat org.jvnet.hk2.internal.SingletonContext.findOrCreate(SingletonContext.java:98)\r\n\tat org.jvnet.hk2.internal.Utilities.createService(Utilities.java:2102)\r\n\tat org.jvnet.hk2.internal.ServiceLocatorImpl.internalGetAllServiceHandles(ServiceLocatorImpl.java:1481)\r\n\tat org.jvnet.hk2.internal.ServiceLocatorImpl.getAllServices(ServiceLocatorImpl.java:799)\r\n\tat org.glassfish.jersey.inject.hk2.AbstractHk2InjectionManager.getAllInstances(AbstractHk2InjectionManager.java:171)\r\n\tat org.glassfish.jersey.inject.hk2.ImmediateHk2InjectionManager.getAllInstances(ImmediateHk2InjectionManager.java:30)\r\n\tat org.glassfish.jersey.internal.ContextResolverFactory$ContextResolversConfigurator.postInit(ContextResolverFactory.java:69)\r\n\tat org.glassfish.jersey.server.ApplicationHandler.lambda$initialize$2(ApplicationHandler.java:353)\r\n\tat java.base/java.util.Arrays$ArrayList.forEach(Arrays.java:4204)\r\n\tat org.glassfish.jersey.server.ApplicationHandler.initialize(ApplicationHandler.java:353)\r\n\tat org.glassfish.jersey.server.ApplicationHandler.lambda$initialize$1(ApplicationHandler.java:297)\r\n\tat org.glassfish.jersey.internal.Errors.process(Errors.java:292)\r\n\tat org.glassfish.jersey.internal.Errors.process(Errors.java:274)\r\n\tat org.glassfish.jersey.internal.Errors.processWithException(Errors.java:232)\r\n\tat org.glassfish.jersey.server.ApplicationHandler.initialize(ApplicationHandler.java:296)\r\n\tat org.glassfish.jersey.server.ApplicationHandler.<init>(ApplicationHandler.java:261)\r\n\tat org.glassfish.jersey.servlet.WebComponent.<init>(WebComponent.java:314)\r\n\tat org.glassfish.jersey.servlet.ServletContainer.init(ServletContainer.java:154)\r\n\tat org.glassfish.jersey.servlet.ServletContainer.init(ServletContainer.java:360)\r\n\tat javax.servlet.GenericServlet.init(GenericServlet.java:244)\r\n\tat org.eclipse.jetty.servlet.ServletHolder.initServlet(ServletHolder.java:632)\r\n\t... 104 more\r\nCaused by: javax.xml.bind.JAXBException: Implementation of JAXB-API has not been found on module path or classpath.\r\n - with linked exception:\r\n[java.lang.ClassNotFoundException: com.sun.xml.bind.v2.ContextFactory]\r\n\tat javax.xml.bind.ContextFinder.newInstance(ContextFinder.java:232)\r\n\tat javax.xml.bind.ContextFinder.find(ContextFinder.java:375)\r\n\tat javax.xml.bind.JAXBContext.newInstance(JAXBContext.java:691)\r\n\tat javax.xml.bind.JAXBContext.newInstance(JAXBContext.java:632)\r\n\tat org.glassfish.jersey.jettison.JettisonJaxbContext.<init>(JettisonJaxbContext.java:73)\r\n\tat org.glassfish.jersey.jettison.JettisonJaxbContext.<init>(JettisonJaxbContext.java:54)\r\n\tat org.apache.hadoop.yarn.webapp.MyTestJAXBContextResolver.<init>(MyTestJAXBContextResolver.java:45)\r\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)\r\n\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r\n\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)\r\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)\r\n\tat org.glassfish.hk2.utilities.reflection.ReflectionHelper.makeMe(ReflectionHelper.java:1356)\r\n\tat org.jvnet.hk2.internal.ClazzCreator.createMe(ClazzCreator.java:248)\r\n\tat org.jvnet.hk2.internal.ClazzCreator.create(ClazzCreator.java:342)\r\n\t... 132 more\r\nCaused by: java.lang.ClassNotFoundException: com.sun.xml.bind.v2.ContextFactory\r\n\tat java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:641)\r\n\tat java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:188)\r\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\r\n\tat org.eclipse.jetty.webapp.WebAppClassLoader.loadClass(WebAppClassLoader.java:538)\r\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\r\n\tat javax.xml.bind.ServiceLoaderUtil.nullSafeLoadClass(ServiceLoaderUtil.java:92)\r\n\tat javax.xml.bind.ServiceLoaderUtil.safeLoadClass(ServiceLoaderUtil.java:125)\r\n\tat javax.xml.bind.ContextFinder.newInstance(ContextFinder.java:230)\r\n\t... 146 more\r\n{code}\r\n{panel}\r\n\r\n"}}}
{"project": "HADOOP", "issue_key": "HADOOP-19673", "title": "BloomMapFile: invalid io.mapfile.bloom.error.rate (\u22640 or \u22651) causes NaN/zero vector size and writer construction failure", "status": "Patch Available", "reporter": "AMC-team", "assignee": null, "priority": "Major", "labels": [], "created": "2025-09-02T14:20:00.000+0000", "updated": "2025-09-07T07:57:06.000+0000", "description": "{{BloomMapFile.Writer#initBloomFilter(Configuration)}} computes the Bloom filter vector size as:\r\n{code:java}\r\nint numKeys = conf.getInt(IO_MAPFILE_BLOOM_SIZE_KEY, IO_MAPFILE_BLOOM_SIZE_DEFAULT);\r\nfloat errorRate = conf.getFloat(IO_MAPFILE_BLOOM_ERROR_RATE_KEY, IO_MAPFILE_BLOOM_ERROR_RATE_DEFAULT);\r\nint vectorSize = (int) Math.ceil(\r\n\u00a0 (double)(-HASH_COUNT * numKeys) /\r\n\u00a0 Math.log(1.0 - Math.pow(errorRate, 1.0 / HASH_COUNT))\r\n); {code}\r\nWhen {{io.mapfile.bloom.error.rate}} is *\u2264 0*\u00a0or {*}\u2265 1{*}:\r\n * {{Math.pow(errorRate, 1/k)}} produces *NaN* (negative base with non-integer exponent) or an invalid value;\r\n * {{Math.log(1 - NaN)}} becomes {*}NaN{*};\r\n * {{Math.ceil(NaN)}} cast to {{int}} yields {*}0{*}, so {{{}vectorSize == 0{}}};\r\n * constructing {{DynamicBloomFilter}} subsequently fails, and {{BloomMapFile.Writer}} construction fails (observed as assertion failure in tests).\r\n\r\nThe code misses input validation for {{io.mapfile.bloom.error.rate}} which should be strictly within {{{}(0, 1){}}}. With invalid values, the math silently degrades to NaN/0 and fails at runtime.\r\n\r\n*Reproduction*\r\n\r\nInjected values: {{io.mapfile.bloom.error.rate = 0,-1}}\r\n\r\nTest: {{org.apache.hadoop.io.TestBloomMapFile#testBloomMapFileConstructors}}\r\n{code:java}\r\n[INFO] Running org.apache.hadoop.io.TestBloomMapFile\r\n[ERROR] Tests run: 1, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 0.358 s <<< FAILURE! - in org.apache.hadoop.io.TestBloomMapFile\r\n[ERROR] org.apache.hadoop.io.TestBloomMapFile.testBloomMapFileConstructors \u00a0Time elapsed: 0.272 s \u00a0<<< FAILURE!\r\njava.lang.AssertionError: testBloomMapFileConstructors error !!!\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.io.TestBloomMapFile.testBloomMapFileConstructors(TestBloomMapFile.java:287{code}", "comments": [], "derived_tasks": {"summarization": "BloomMapFile: invalid io.mapfile.bloom.error.rate (\u22640 or \u22651) causes NaN/zero vector size and writer construction failure", "classification": "feature", "qna": {"question": "What is the issue 'BloomMapFile: invalid io.mapfile.bloom.error.rate (\u22640 or \u22651) causes NaN/zero vector size and writer construction failure' about?", "answer": "{{BloomMapFile.Writer#initBloomFilter(Configuration)}} computes the Bloom filter vector size as:\r\n{code:java}\r\nint numKeys = conf.getInt(IO_MAPFILE_BLOOM_SIZE_KEY, IO_MAPFILE_BLOOM_SIZE_DEFAULT);\r\nfloat errorRate = conf.getFloat(IO_MAPFILE_BLOOM_ERROR_RATE_KEY, IO_MAPFILE_BLOOM_ERROR_RATE_DEFAULT);\r\nint vectorSize = (int) Math.ceil(\r\n\u00a0 (double)(-HASH_COUNT * numKeys) /\r\n\u00a0 Math.log(1.0 - Math.pow(errorRate, 1.0 / HASH_COUNT))\r\n); {code}\r\nWhen {{io.mapfile.bloom.error.rate}} is *\u2264 0*\u00a0or {*}\u2265 1{*}:\r\n * {{Math.pow(errorRate, 1/k)}} produces *NaN* (negative base with non-integer exponent) or an invalid value;\r\n * {{Math.log(1 - NaN)}} becomes {*}NaN{*};\r\n * {{Math.ceil(NaN)}} cast to {{int}} yields {*}0{*}, so {{{}vectorSize == 0{}}};\r\n * constructing {{DynamicBloomFilter}} subsequently fails, and {{BloomMapFile.Writer}} construction fails (observed as assertion failure in tests).\r\n\r\nThe code misses input validation for {{io.mapfile.bloom.error.rate}} which should be strictly within {{{}(0, 1){}}}. With invalid values, the math silently degrades to NaN/0 and fails at runtime.\r\n\r\n*Reproduction*\r\n\r\nInjected values: {{io.mapfile.bloom.error.rate = 0,-1}}\r\n\r\nTest: {{org.apache.hadoop.io.TestBloomMapFile#testBloomMapFileConstructors}}\r\n{code:java}\r\n[INFO] Running org.apache.hadoop.io.TestBloomMapFile\r\n[ERROR] Tests run: 1, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 0.358 s <<< FAILURE! - in org.apache.hadoop.io.TestBloomMapFile\r\n[ERROR] org.apache.hadoop.io.TestBloomMapFile.testBloomMapFileConstructors \u00a0Time elapsed: 0.272 s \u00a0<<< FAILURE!\r\njava.lang.AssertionError: testBloomMapFileConstructors error !!!\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.io.TestBloomMapFile.testBloomMapFileConstructors(TestBloomMapFile.java:287{code}"}}}
{"project": "HADOOP", "issue_key": "HADOOP-19672", "title": "ABFS: Network Error-Based Client Switchover: Apache to JDK (continuous failure))", "status": "Open", "reporter": "Manish Bhatt", "assignee": "Manish Bhatt", "priority": "Major", "labels": ["pull-request-available"], "created": "2025-09-01T10:54:52.000+0000", "updated": "2025-10-23T04:38:43.000+0000", "description": null, "comments": [], "derived_tasks": {"summarization": "ABFS: Network Error-Based Client Switchover: Apache to JDK (continuous failure))", "classification": "feature", "qna": {"question": "What is the issue 'ABFS: Network Error-Based Client Switchover: Apache to JDK (continuous failure))' about?", "answer": null}}}
{"project": "HADOOP", "issue_key": "HADOOP-19671", "title": "Migrate to AssertJ for Assertion Verification", "status": "In Progress", "reporter": "Shilun Fan", "assignee": "Shilun Fan", "priority": "Major", "labels": [], "created": "2025-08-30T06:33:03.000+0000", "updated": "2025-08-30T06:39:18.000+0000", "description": "Currently, the unit tests in the project do not fully leverage modern assertion capabilities, resulting in lower readability and maintainability of the test code. To improve test clarity and extensibility, we plan to migrate the existing assertion library to {*}AssertJ{*}.\r\n\r\n\u00a0\r\n\r\n*Objective:*\r\n * Migrate all assertions in existing tests from JUnit or other assertion libraries to AssertJ.\r\n\r\n * Utilize AssertJ\u2019s rich assertion methods (e.g., fluent API, more readable assertions) to enhance the expressiveness of unit tests.\r\n\r\n * Ensure that all existing unit tests continue to run correctly after migration.\r\n\r\n\u00a0\r\n\r\n*Implementation Steps:*\r\n # Analyze existing unit test code to identify assertions that need to be replaced.\r\n\r\n # Replace existing assertions with AssertJ assertion syntax.\r\n\r\n # Run unit tests to ensure the tests pass and function correctly after migration.\r\n\r\n # Update relevant documentation to ensure the team is aware of how to use AssertJ for assertions.", "comments": [], "derived_tasks": {"summarization": "Migrate to AssertJ for Assertion Verification", "classification": "feature", "qna": {"question": "What is the issue 'Migrate to AssertJ for Assertion Verification' about?", "answer": "Currently, the unit tests in the project do not fully leverage modern assertion capabilities, resulting in lower readability and maintainability of the test code. To improve test clarity and extensibility, we plan to migrate the existing assertion library to {*}AssertJ{*}.\r\n\r\n\u00a0\r\n\r\n*Objective:*\r\n * Migrate all assertions in existing tests from JUnit or other assertion libraries to AssertJ.\r\n\r\n * Utilize AssertJ\u2019s rich assertion methods (e.g., fluent API, more readable assertions) to enhance the expressiveness of unit tests.\r\n\r\n * Ensure that all existing unit tests continue to run correctly after migration.\r\n\r\n\u00a0\r\n\r\n*Implementation Steps:*\r\n # Analyze existing unit test code to identify assertions that need to be replaced.\r\n\r\n # Replace existing assertions with AssertJ assertion syntax.\r\n\r\n # Run unit tests to ensure the tests pass and function correctly after migration.\r\n\r\n # Update relevant documentation to ensure the team is aware of how to use AssertJ for assertions."}}}
{"project": "HADOOP", "issue_key": "HADOOP-19670", "title": "Replace Thread with SubjectPreservingThread", "status": "Open", "reporter": "Istvan Toth", "assignee": null, "priority": "Major", "labels": [], "created": "2025-08-29T08:22:44.000+0000", "updated": "2025-09-24T16:11:48.000+0000", "description": null, "comments": [], "derived_tasks": {"summarization": "Replace Thread with SubjectPreservingThread", "classification": "feature", "qna": {"question": "What is the issue 'Replace Thread with SubjectPreservingThread' about?", "answer": null}}}
{"project": "HADOOP", "issue_key": "HADOOP-19669", "title": "Update Daemon to restore pre JDK22 Subject behaviour in Threads", "status": "Resolved", "reporter": "Istvan Toth", "assignee": "Istvan Toth", "priority": "Major", "labels": [], "created": "2025-08-29T07:57:27.000+0000", "updated": "2025-09-10T13:21:24.000+0000", "description": null, "comments": [], "derived_tasks": {"summarization": "Update Daemon to restore pre JDK22 Subject behaviour in Threads", "classification": "feature", "qna": {"question": "What is the issue 'Update Daemon to restore pre JDK22 Subject behaviour in Threads' about?", "answer": null}}}
{"project": "HADOOP", "issue_key": "HADOOP-19668", "title": "Add SubjectInheritingThread to restore pre JDK22 Subject behaviour in Threads", "status": "Open", "reporter": "Istvan Toth", "assignee": "Istvan Toth", "priority": "Major", "labels": [], "created": "2025-08-29T07:23:38.000+0000", "updated": "2025-09-10T13:21:24.000+0000", "description": "This is the first part of HADOOP-19574 which adds the compatibility code, and fixes Daemon, but does not replace native Thread instances.\r\n", "comments": [], "derived_tasks": {"summarization": "Add SubjectInheritingThread to restore pre JDK22 Subject behaviour in Threads", "classification": "feature", "qna": {"question": "What is the issue 'Add SubjectInheritingThread to restore pre JDK22 Subject behaviour in Threads' about?", "answer": "This is the first part of HADOOP-19574 which adds the compatibility code, and fixes Daemon, but does not replace native Thread instances.\r\n"}}}
{"project": "HADOOP", "issue_key": "HADOOP-19667", "title": "Clarify legal value constraints for hadoop.security.crypto.buffer.size (min=512; floor to cipher block size)", "status": "Open", "reporter": "AMC-team", "assignee": null, "priority": "Major", "labels": ["pull-request-available"], "created": "2025-08-27T10:08:02.000+0000", "updated": "2025-08-27T15:02:06.000+0000", "description": "In core-default.xml, the property hadoop.security.crypto.buffer.size is currently documented as \u201cThe buffer size used by CryptoInputStream and CryptoOutputStream.\u201d It does not specify the legal value constraints.\r\n{code:java}\r\n<property>\u00a0 \r\n<name>hadoop.security.crypto.buffer.size</name>\u00a0 \r\n<value>8192</value>\u00a0 \r\n<description>The buffer size used by CryptoInputStream and CryptoOutputStream.\u00a0 </description>\r\n</property> {code}\r\nThe runtime enforces two hidden constraints that are not documented:\r\n1. Minimum value is 512 bytes. Values below 512 cause IllegalArgumentException at stream construction time.\r\n2. Block-size flooring: The effective buffer size is floored to a multiple of the cipher algorithm\u2019s block size (e.g., 16 bytes for AES/CTR/NoPadding).\r\n\r\nAs a result, users may be surprised that:\r\n1. Setting a value like 4100 results in an actual capacity of 4096.\r\n2. Setting values <512 fails fast with IllegalArgumentException.\r\n\r\n*Expected*\r\ncore-default.xml (and user-facing docs) should explicitly document:\r\n1. Minimum legal value: 512 bytes.\r\n\r\n2. The effective value is floored to the nearest multiple of the cipher algorithm block size (e.g., 16 for AES).", "comments": [], "derived_tasks": {"summarization": "Clarify legal value constraints for hadoop.security.crypto.buffer.size (min=512; floor to cipher block size)", "classification": "feature", "qna": {"question": "What is the issue 'Clarify legal value constraints for hadoop.security.crypto.buffer.size (min=512; floor to cipher block size)' about?", "answer": "In core-default.xml, the property hadoop.security.crypto.buffer.size is currently documented as \u201cThe buffer size used by CryptoInputStream and CryptoOutputStream.\u201d It does not specify the legal value constraints.\r\n{code:java}\r\n<property>\u00a0 \r\n<name>hadoop.security.crypto.buffer.size</name>\u00a0 \r\n<value>8192</value>\u00a0 \r\n<description>The buffer size used by CryptoInputStream and CryptoOutputStream.\u00a0 </description>\r\n</property> {code}\r\nThe runtime enforces two hidden constraints that are not documented:\r\n1. Minimum value is 512 bytes. Values below 512 cause IllegalArgumentException at stream construction time.\r\n2. Block-size flooring: The effective buffer size is floored to a multiple of the cipher algorithm\u2019s block size (e.g., 16 bytes for AES/CTR/NoPadding).\r\n\r\nAs a result, users may be surprised that:\r\n1. Setting a value like 4100 results in an actual capacity of 4096.\r\n2. Setting values <512 fails fast with IllegalArgumentException.\r\n\r\n*Expected*\r\ncore-default.xml (and user-facing docs) should explicitly document:\r\n1. Minimum legal value: 512 bytes.\r\n\r\n2. The effective value is floored to the nearest multiple of the cipher algorithm block size (e.g., 16 for AES)."}}}
{"project": "HADOOP", "issue_key": "HADOOP-19666", "title": "Add hardware-accelerated CRC32 support for riscv64 using the v,zbc,zvbc extension", "status": "Open", "reporter": "Lei Wen", "assignee": null, "priority": "Major", "labels": ["pull-request-available"], "created": "2025-08-27T03:08:41.000+0000", "updated": "2025-10-26T05:02:39.000+0000", "description": "This PR implements vector-accelerated CRC32 using the RISC-V V, Zbc and Zvbc\r\ninstruction sets, with full functional verification and performance testing completed.\r\n\r\nThe implementation uses the vclmul.v and vclmulh.v (carry-less multiply) instructions for data folding and computes the final checksum via Barrett reduction.\r\n\r\nKey Features: \r\n1. Runtime Hardware Detection\r\nThe PR uses kernel hardware probing and cpuinfo parsing to dynamically detect hardware support for CRC32 acceleration (via v, zbc, and zvbc extensions) at runtime.\r\n\r\n2. Performance Improvement\r\nHardware-accelerated CRC32 achieves a performance boost of over *3x* compared to the software implementation.", "comments": [], "derived_tasks": {"summarization": "Add hardware-accelerated CRC32 support for riscv64 using the v,zbc,zvbc extension", "classification": "feature", "qna": {"question": "What is the issue 'Add hardware-accelerated CRC32 support for riscv64 using the v,zbc,zvbc extension' about?", "answer": "This PR implements vector-accelerated CRC32 using the RISC-V V, Zbc and Zvbc\r\ninstruction sets, with full functional verification and performance testing completed.\r\n\r\nThe implementation uses the vclmul.v and vclmulh.v (carry-less multiply) instructions for data folding and computes the final checksum via Barrett reduction.\r\n\r\nKey Features: \r\n1. Runtime Hardware Detection\r\nThe PR uses kernel hardware probing and cpuinfo parsing to dynamically detect hardware support for CRC32 acceleration (via v, zbc, and zvbc extensions) at runtime.\r\n\r\n2. Performance Improvement\r\nHardware-accelerated CRC32 achieves a performance boost of over *3x* compared to the software implementation."}}}
{"project": "HADOOP", "issue_key": "HADOOP-19665", "title": "[kms] Negative hadoop.security.kms.client.encrypted.key.cache.expiry causes KMSClientProvider init failure with generic IllegalArgumentException; ", "status": "Patch Available", "reporter": "AMC-team", "assignee": null, "priority": "Major", "labels": [], "created": "2025-08-26T14:11:12.000+0000", "updated": "2025-09-07T07:39:00.000+0000", "description": "When the client-side config hadoop.security.kms.client.encrypted.key.cache.expiry is set to a negative value in core-site.xml, any tool that initializes KMSClientProvider (e.g., KeyShell) fails immediately with:\r\n\r\n{code:java}\r\njava.lang.IllegalArgumentException: expiry must be > 0\r\n    at org.apache.hadoop.crypto.key.kms.ValueQueue.<init>(ValueQueue.java:xxx)\r\n    at org.apache.hadoop.crypto.key.kms.KMSClientProvider.<init>(KMSClientProvider.java:xxx)\r\n    ...\r\n\r\n{code}\r\n\r\nThis is a controlled failure (JVM doesn\u2019t crash), but the error message does not mention which property and what value triggered it. Users typically see a stack trace without a clear remediation hint.\r\n\r\n*Expected behavior*\r\n\r\nFail fast with a clear configuration error that names the property and value, e.g.:\r\nInvalid configuration: hadoop.security.kms.client.encrypted.key.cache.expiry = -1 (must be > 0 ms)\r\n\r\n*Steps to Reproduce*\r\n1. In the client core-site.xml, set:\r\n\r\n{code:xml}\r\n<property>\r\n  <name>hadoop.security.kms.client.encrypted.key.cache.expiry</name>\r\n  <value>-1</value>\r\n</property>\r\n{code}\r\n2. Ensure the conf is active (echo $HADOOP_CONF_DIR points to this dir).\r\n3. Run:\r\n\r\n{code:java}\r\n./bin/hadoop key list -provider kms://http@localhost:9600/kms -metadata\r\n{code}\r\n", "comments": [], "derived_tasks": {"summarization": "[kms] Negative hadoop.security.kms.client.encrypted.key.cache.expiry causes KMSClientProvider init failure with generic IllegalArgumentException; ", "classification": "feature", "qna": {"question": "What is the issue '[kms] Negative hadoop.security.kms.client.encrypted.key.cache.expiry causes KMSClientProvider init failure with generic IllegalArgumentException; ' about?", "answer": "When the client-side config hadoop.security.kms.client.encrypted.key.cache.expiry is set to a negative value in core-site.xml, any tool that initializes KMSClientProvider (e.g., KeyShell) fails immediately with:\r\n\r\n{code:java}\r\njava.lang.IllegalArgumentException: expiry must be > 0\r\n    at org.apache.hadoop.crypto.key.kms.ValueQueue.<init>(ValueQueue.java:xxx)\r\n    at org.apache.hadoop.crypto.key.kms.KMSClientProvider.<init>(KMSClientProvider.java:xxx)\r\n    ...\r\n\r\n{code}\r\n\r\nThis is a controlled failure (JVM doesn\u2019t crash), but the error message does not mention which property and what value triggered it. Users typically see a stack trace without a clear remediation hint.\r\n\r\n*Expected behavior*\r\n\r\nFail fast with a clear configuration error that names the property and value, e.g.:\r\nInvalid configuration: hadoop.security.kms.client.encrypted.key.cache.expiry = -1 (must be > 0 ms)\r\n\r\n*Steps to Reproduce*\r\n1. In the client core-site.xml, set:\r\n\r\n{code:xml}\r\n<property>\r\n  <name>hadoop.security.kms.client.encrypted.key.cache.expiry</name>\r\n  <value>-1</value>\r\n</property>\r\n{code}\r\n2. Ensure the conf is active (echo $HADOOP_CONF_DIR points to this dir).\r\n3. Run:\r\n\r\n{code:java}\r\n./bin/hadoop key list -provider kms://http@localhost:9600/kms -metadata\r\n{code}\r\n"}}}
{"project": "HADOOP", "issue_key": "HADOOP-19664", "title": "S3A Analytics-Accelerator: Move AAL to use Java sync client", "status": "Resolved", "reporter": "Ahmar Suhail", "assignee": "Ahmar Suhail", "priority": "Major", "labels": ["pull-request-available"], "created": "2025-08-26T10:26:15.000+0000", "updated": "2025-09-18T13:22:04.000+0000", "description": "Java sync client is giving the best performance for our use case, especially for readVectored() where a large number of requests can be made concurrently.\u00a0", "comments": [], "derived_tasks": {"summarization": "S3A Analytics-Accelerator: Move AAL to use Java sync client", "classification": "feature", "qna": {"question": "What is the issue 'S3A Analytics-Accelerator: Move AAL to use Java sync client' about?", "answer": "Java sync client is giving the best performance for our use case, especially for readVectored() where a large number of requests can be made concurrently.\u00a0"}}}
{"project": "HADOOP", "issue_key": "HADOOP-19663", "title": "Add RISC-V build infrastructure and placeholder implementation for CRC32 acceleration", "status": "Resolved", "reporter": "Ptroc", "assignee": "Ptroc", "priority": "Major", "labels": ["pull-request-available"], "created": "2025-08-26T09:30:19.000+0000", "updated": "2025-10-09T14:21:49.000+0000", "description": "Establish the foundational build infrastructure for RISC-V CRC32 hardware acceleration support while maintaining full backward compatibility with existing software implementations.", "comments": [], "derived_tasks": {"summarization": "Add RISC-V build infrastructure and placeholder implementation for CRC32 acceleration", "classification": "feature", "qna": {"question": "What is the issue 'Add RISC-V build infrastructure and placeholder implementation for CRC32 acceleration' about?", "answer": "Establish the foundational build infrastructure for RISC-V CRC32 hardware acceleration support while maintaining full backward compatibility with existing software implementations."}}}
{"project": "HADOOP", "issue_key": "HADOOP-19662", "title": " Remove duplicate include of gcc_optimizations.h in bulk_crc32_x86.c", "status": "Open", "reporter": "Ptroc", "assignee": null, "priority": "Major", "labels": ["pull-request-available"], "created": "2025-08-26T09:00:41.000+0000", "updated": "2025-08-26T16:23:08.000+0000", "description": "## Description\r\n\u00a0\r\nThere is a duplicate include statement for `gcc_optimizations.h` in `bulk_crc32_x86.c` at lines 29-30.\r\n\r\n## Current Code\r\n```c\r\n#include \"gcc_optimizations.h\"\r\n#include \"gcc_optimizations.h\"\r\n```", "comments": [], "derived_tasks": {"summarization": " Remove duplicate include of gcc_optimizations.h in bulk_crc32_x86.c", "classification": "feature", "qna": {"question": "What is the issue ' Remove duplicate include of gcc_optimizations.h in bulk_crc32_x86.c' about?", "answer": "## Description\r\n\u00a0\r\nThere is a duplicate include statement for `gcc_optimizations.h` in `bulk_crc32_x86.c` at lines 29-30.\r\n\r\n## Current Code\r\n```c\r\n#include \"gcc_optimizations.h\"\r\n#include \"gcc_optimizations.h\"\r\n```"}}}
{"project": "HADOOP", "issue_key": "HADOOP-19661", "title": "Migrate CentOS 8 to Rocky Linux 8 in build env Dockerfile", "status": "Resolved", "reporter": "Cheng Pan", "assignee": "Cheng Pan", "priority": "Major", "labels": ["pull-request-available"], "created": "2025-08-26T08:04:43.000+0000", "updated": "2025-09-11T22:56:17.000+0000", "description": null, "comments": [], "derived_tasks": {"summarization": "Migrate CentOS 8 to Rocky Linux 8 in build env Dockerfile", "classification": "feature", "qna": {"question": "What is the issue 'Migrate CentOS 8 to Rocky Linux 8 in build env Dockerfile' about?", "answer": null}}}
{"project": "HADOOP", "issue_key": "HADOOP-19660", "title": "ABFS: Proposed Enhancement in WorkloadIdentityTokenProvider", "status": "Open", "reporter": "Anuj Modi", "assignee": null, "priority": "Major", "labels": ["pull-request-available"], "created": "2025-08-26T06:53:52.000+0000", "updated": "2025-09-26T16:37:27.000+0000", "description": "Externally Reported Enhancement:\r\n\r\n*Current Limitation*\r\nThe current WorkloadIdentityTokenProvider implementation works well for file-based token scenarios, but it's tightly coupled to file system operations and cannot be easily extended for alternative token sources\r\n\r\n{*}Use Case{*}:\u00a0*Kubernetes TokenRequest API*\u00a0\r\nIn modern Kubernetes environments, the recommended approach is to use the TokenRequest API to generate short-lived, on-demand service account tokens rather than relying on projected volume mounts.\r\n\r\n*Proposed Enhancement*\u00a0\r\nI propose modifying WorkloadIdentityTokenProvider to accept a Supplier for token retrieval instead of being hardcoded to file operations:", "comments": [], "derived_tasks": {"summarization": "ABFS: Proposed Enhancement in WorkloadIdentityTokenProvider", "classification": "feature", "qna": {"question": "What is the issue 'ABFS: Proposed Enhancement in WorkloadIdentityTokenProvider' about?", "answer": "Externally Reported Enhancement:\r\n\r\n*Current Limitation*\r\nThe current WorkloadIdentityTokenProvider implementation works well for file-based token scenarios, but it's tightly coupled to file system operations and cannot be easily extended for alternative token sources\r\n\r\n{*}Use Case{*}:\u00a0*Kubernetes TokenRequest API*\u00a0\r\nIn modern Kubernetes environments, the recommended approach is to use the TokenRequest API to generate short-lived, on-demand service account tokens rather than relying on projected volume mounts.\r\n\r\n*Proposed Enhancement*\u00a0\r\nI propose modifying WorkloadIdentityTokenProvider to accept a Supplier for token retrieval instead of being hardcoded to file operations:"}}}
{"project": "HADOOP", "issue_key": "HADOOP-19659", "title": "Upgrade Debian 10 to 11 in build env Dockerfile", "status": "Resolved", "reporter": "Cheng Pan", "assignee": "Cheng Pan", "priority": "Major", "labels": ["pull-request-available"], "created": "2025-08-26T03:33:48.000+0000", "updated": "2025-09-14T07:13:37.000+0000", "description": "Debian 10 EOL, and the apt repo is unavailable\r\n{code:bash}\r\ndocker run --rm -it debian:10 bash\r\nroot@bc2a4c509cb3:/# apt update\r\nIgn:1 http://deb.debian.org/debian buster InRelease\r\nIgn:2 http://deb.debian.org/debian-security buster/updates InRelease\r\nIgn:3 http://deb.debian.org/debian buster-updates InRelease\r\nErr:4 http://deb.debian.org/debian buster Release\r\n  404  Not Found [IP: 151.101.90.132 80]\r\nErr:5 http://deb.debian.org/debian-security buster/updates Release\r\n  404  Not Found [IP: 151.101.90.132 80]\r\nErr:6 http://deb.debian.org/debian buster-updates Release\r\n  404  Not Found [IP: 151.101.90.132 80]\r\nReading package lists... Done\r\nE: The repository 'http://deb.debian.org/debian buster Release' does not have a Release file.\r\nN: Updating from such a repository can't be done securely, and is therefore disabled by default.\r\nN: See apt-secure(8) manpage for repository creation and user configuration details.\r\nE: The repository 'http://deb.debian.org/debian-security buster/updates Release' does not have a Release file.\r\nN: Updating from such a repository can't be done securely, and is therefore disabled by default.\r\nN: See apt-secure(8) manpage for repository creation and user configuration details.\r\nE: The repository 'http://deb.debian.org/debian buster-updates Release' does not have a Release file.\r\nN: Updating from such a repository can't be done securely, and is therefore disabled by default.\r\nN: See apt-secure(8) manpage for repository creation and user configuration details.\r\nroot@bc2a4c509cb3:/#\r\n{code}", "comments": [], "derived_tasks": {"summarization": "Upgrade Debian 10 to 11 in build env Dockerfile", "classification": "feature", "qna": {"question": "What is the issue 'Upgrade Debian 10 to 11 in build env Dockerfile' about?", "answer": "Debian 10 EOL, and the apt repo is unavailable\r\n{code:bash}\r\ndocker run --rm -it debian:10 bash\r\nroot@bc2a4c509cb3:/# apt update\r\nIgn:1 http://deb.debian.org/debian buster InRelease\r\nIgn:2 http://deb.debian.org/debian-security buster/updates InRelease\r\nIgn:3 http://deb.debian.org/debian buster-updates InRelease\r\nErr:4 http://deb.debian.org/debian buster Release\r\n  404  Not Found [IP: 151.101.90.132 80]\r\nErr:5 http://deb.debian.org/debian-security buster/updates Release\r\n  404  Not Found [IP: 151.101.90.132 80]\r\nErr:6 http://deb.debian.org/debian buster-updates Release\r\n  404  Not Found [IP: 151.101.90.132 80]\r\nReading package lists... Done\r\nE: The repository 'http://deb.debian.org/debian buster Release' does not have a Release file.\r\nN: Updating from such a repository can't be done securely, and is therefore disabled by default.\r\nN: See apt-secure(8) manpage for repository creation and user configuration details.\r\nE: The repository 'http://deb.debian.org/debian-security buster/updates Release' does not have a Release file.\r\nN: Updating from such a repository can't be done securely, and is therefore disabled by default.\r\nN: See apt-secure(8) manpage for repository creation and user configuration details.\r\nE: The repository 'http://deb.debian.org/debian buster-updates Release' does not have a Release file.\r\nN: Updating from such a repository can't be done securely, and is therefore disabled by default.\r\nN: See apt-secure(8) manpage for repository creation and user configuration details.\r\nroot@bc2a4c509cb3:/#\r\n{code}"}}}
{"project": "HADOOP", "issue_key": "HADOOP-19658", "title": "ABFS: [FNS Over Blob] Support create and rename idempotency on FNS Blob from client side", "status": "Open", "reporter": "Anmol Asrani", "assignee": "Anmol Asrani", "priority": "Major", "labels": ["pull-request-available"], "created": "2025-08-21T12:01:50.000+0000", "updated": "2025-09-03T04:12:38.000+0000", "description": "\u00a0Support create and rename idempotency on FNS Blob from client side", "comments": [], "derived_tasks": {"summarization": "ABFS: [FNS Over Blob] Support create and rename idempotency on FNS Blob from client side", "classification": "feature", "qna": {"question": "What is the issue 'ABFS: [FNS Over Blob] Support create and rename idempotency on FNS Blob from client side' about?", "answer": "\u00a0Support create and rename idempotency on FNS Blob from client side"}}}
{"project": "HADOOP", "issue_key": "HADOOP-19657", "title": "Update 3.4.2 docs landing page to highlight changes shipped in the release", "status": "Resolved", "reporter": "Ahmar Suhail", "assignee": "Ahmar Suhail", "priority": "Major", "labels": ["pull-request-available"], "created": "2025-08-19T13:16:06.000+0000", "updated": "2025-08-21T01:39:59.000+0000", "description": "The landing page for the 3.4.2 docs mostly lists changes that already shipped in 3.4.0. Update this to highlight 3.4.2 changes.", "comments": [], "derived_tasks": {"summarization": "Update 3.4.2 docs landing page to highlight changes shipped in the release", "classification": "feature", "qna": {"question": "What is the issue 'Update 3.4.2 docs landing page to highlight changes shipped in the release' about?", "answer": "The landing page for the 3.4.2 docs mostly lists changes that already shipped in 3.4.0. Update this to highlight 3.4.2 changes."}}}
{"project": "HADOOP", "issue_key": "HADOOP-19656", "title": "Fix hadoop-client-minicluster", "status": "Resolved", "reporter": "Cheng Pan", "assignee": null, "priority": "Major", "labels": [], "created": "2025-08-19T12:53:49.000+0000", "updated": "2025-09-05T09:18:52.000+0000", "description": null, "comments": [], "derived_tasks": {"summarization": "Fix hadoop-client-minicluster", "classification": "feature", "qna": {"question": "What is the issue 'Fix hadoop-client-minicluster' about?", "answer": null}}}
{"project": "HADOOP", "issue_key": "HADOOP-19655", "title": "Add RISC-V Zbc (CLMUL) hardware-accelerated CRC32/CRC32C implementation", "status": "Open", "reporter": "Ptroc", "assignee": null, "priority": "Major", "labels": ["pull-request-available"], "created": "2025-08-19T02:52:30.000+0000", "updated": "2025-10-17T16:49:52.000+0000", "description": "This patch introduces hardware-accelerated CRC32 and CRC32C algorithms for RISC-V platforms supporting the Zbc extension (CLMUL instructions) in bulk_crc32_riscv.c.\r\nKey changes:\r\n * Implements optimized CRC32 and CRC32C routines using CLMUL instructions for zlib and Castagnoli polynomials.\r\n * Automatically switches to hardware acceleration when Zbc is available, otherwise falls back to generic table-based software implementation.\r\n * Maintains compatibility with platforms lacking Zbc support.\r\n\r\nThis optimization improves CRC performance on RISC-V CPUs with Zbc extension.", "comments": [], "derived_tasks": {"summarization": "Add RISC-V Zbc (CLMUL) hardware-accelerated CRC32/CRC32C implementation", "classification": "feature", "qna": {"question": "What is the issue 'Add RISC-V Zbc (CLMUL) hardware-accelerated CRC32/CRC32C implementation' about?", "answer": "This patch introduces hardware-accelerated CRC32 and CRC32C algorithms for RISC-V platforms supporting the Zbc extension (CLMUL instructions) in bulk_crc32_riscv.c.\r\nKey changes:\r\n * Implements optimized CRC32 and CRC32C routines using CLMUL instructions for zlib and Castagnoli polynomials.\r\n * Automatically switches to hardware acceleration when Zbc is available, otherwise falls back to generic table-based software implementation.\r\n * Maintains compatibility with platforms lacking Zbc support.\r\n\r\nThis optimization improves CRC performance on RISC-V CPUs with Zbc extension."}}}
{"project": "HADOOP", "issue_key": "HADOOP-19654", "title": "Upgrade AWS SDK to 2.35.4", "status": "In Progress", "reporter": "Steve Loughran", "assignee": "Steve Loughran", "priority": "Major", "labels": ["pull-request-available"], "created": "2025-08-18T16:47:04.000+0000", "updated": "2025-10-25T09:26:53.000+0000", "description": "Upgrade to a recent version of 2.33.x or later while off the critical path of things.\r\n\r\n\r\nHADOOP-19485 froze the sdk at a version which worked with third party stores. Apparently the new version works; early tests show that Bulk Delete calls with third party stores complain about lack of md5 headers, so some tuning is clearly going to be needed.", "comments": [], "derived_tasks": {"summarization": "Upgrade AWS SDK to 2.35.4", "classification": "feature", "qna": {"question": "What is the issue 'Upgrade AWS SDK to 2.35.4' about?", "answer": "Upgrade to a recent version of 2.33.x or later while off the critical path of things.\r\n\r\n\r\nHADOOP-19485 froze the sdk at a version which worked with third party stores. Apparently the new version works; early tests show that Bulk Delete calls with third party stores complain about lack of md5 headers, so some tuning is clearly going to be needed."}}}
{"project": "HADOOP", "issue_key": "HADOOP-19653", "title": "[JDK25] Bump ByteBuddy 1.17.6 and ASM 9.8 to support Java 25 bytecode", "status": "Resolved", "reporter": "Cheng Pan", "assignee": "Cheng Pan", "priority": "Major", "labels": ["pull-request-available"], "created": "2025-08-18T03:11:13.000+0000", "updated": "2025-08-19T00:44:29.000+0000", "description": null, "comments": [], "derived_tasks": {"summarization": "[JDK25] Bump ByteBuddy 1.17.6 and ASM 9.8 to support Java 25 bytecode", "classification": "feature", "qna": {"question": "What is the issue '[JDK25] Bump ByteBuddy 1.17.6 and ASM 9.8 to support Java 25 bytecode' about?", "answer": null}}}
{"project": "HADOOP", "issue_key": "HADOOP-19652", "title": "Fix dependency exclusion list of hadoop-client-runtime.", "status": "Resolved", "reporter": "Cheng Pan", "assignee": "Cheng Pan", "priority": "Major", "labels": ["pull-request-available"], "created": "2025-08-18T02:31:16.000+0000", "updated": "2025-09-29T11:05:01.000+0000", "description": null, "comments": [], "derived_tasks": {"summarization": "Fix dependency exclusion list of hadoop-client-runtime.", "classification": "feature", "qna": {"question": "What is the issue 'Fix dependency exclusion list of hadoop-client-runtime.' about?", "answer": null}}}
{"project": "HADOOP", "issue_key": "HADOOP-19651", "title": "Upgrade libopenssl to 3.5.2-1 needed for rsync", "status": "Resolved", "reporter": "Gautham Banasandra", "assignee": "Gautham Banasandra", "priority": "Blocker", "labels": ["pull-request-available"], "created": "2025-08-15T19:19:04.000+0000", "updated": "2025-08-16T19:37:06.000+0000", "description": "The currently used libopenssl-3.1.4-1 is no longer available on the msys repo - https://repo.msys2.org/msys/x86_64/libopenssl-3.1.4-1-x86_64.pkg.tar.zst\r\n\r\nThe Jenkins CI thus fails to download the same and thus it fails to build the docker image for Windows.\r\n\r\n{code}\r\n00:25:33 SUCCESS: Specified value was saved.\r\n00:25:46 Removing intermediate container 5ce7355571a1\r\n00:25:46 ---> a13a4bc69545\r\n00:25:46 Step 21/78 : RUN powershell Invoke-WebRequest -Uri https://repo.msys2.org/msys/x86_64/libopenssl-3.1.4-1-x86_64.pkg.tar.zst -OutFile $Env:TEMP\\libopenssl-3.1.4-1-x86_64.pkg.tar.zst\r\n00:25:46 ---> Running in d2dafad446f9\r\n00:25:54 \u001b[91mInvoke-WebRequest : The remote server returned an error: (404) Not Found.\r\n00:25:54 \u001b[0m\u001b[91mAt line:1 char:1\r\n00:25:54 \u001b[0m\u001b[91m+ Invoke-WebRequest -Uri https://repo.msys2.org/msys/x86_64/libopenssl- ...\r\n00:25:54 + ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n00:25:54 \u001b[0m\u001b[91m + CategoryInfo : InvalidOperation: (System.Net.HttpWebRequest:Htt\r\n{code}\r\n\r\nThus, we need to upgrade to the latest version to address this.", "comments": [], "derived_tasks": {"summarization": "Upgrade libopenssl to 3.5.2-1 needed for rsync", "classification": "feature", "qna": {"question": "What is the issue 'Upgrade libopenssl to 3.5.2-1 needed for rsync' about?", "answer": "The currently used libopenssl-3.1.4-1 is no longer available on the msys repo - https://repo.msys2.org/msys/x86_64/libopenssl-3.1.4-1-x86_64.pkg.tar.zst\r\n\r\nThe Jenkins CI thus fails to download the same and thus it fails to build the docker image for Windows.\r\n\r\n{code}\r\n00:25:33 SUCCESS: Specified value was saved.\r\n00:25:46 Removing intermediate container 5ce7355571a1\r\n00:25:46 ---> a13a4bc69545\r\n00:25:46 Step 21/78 : RUN powershell Invoke-WebRequest -Uri https://repo.msys2.org/msys/x86_64/libopenssl-3.1.4-1-x86_64.pkg.tar.zst -OutFile $Env:TEMP\\libopenssl-3.1.4-1-x86_64.pkg.tar.zst\r\n00:25:46 ---> Running in d2dafad446f9\r\n00:25:54 \u001b[91mInvoke-WebRequest : The remote server returned an error: (404) Not Found.\r\n00:25:54 \u001b[0m\u001b[91mAt line:1 char:1\r\n00:25:54 \u001b[0m\u001b[91m+ Invoke-WebRequest -Uri https://repo.msys2.org/msys/x86_64/libopenssl- ...\r\n00:25:54 + ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n00:25:54 \u001b[0m\u001b[91m + CategoryInfo : InvalidOperation: (System.Net.HttpWebRequest:Htt\r\n{code}\r\n\r\nThus, we need to upgrade to the latest version to address this."}}}
{"project": "HADOOP", "issue_key": "HADOOP-19650", "title": "ABFS: NPE when close() called on uninitialized filesystem", "status": "Open", "reporter": "Steve Loughran", "assignee": "Anuj Modi", "priority": "Minor", "labels": ["pull-request-available"], "created": "2025-08-15T17:27:57.000+0000", "updated": "2025-08-29T11:44:15.000+0000", "description": "code\r\n{code}\r\n  public void testABFSConstructor() throws Throwable {\r\n    new AzureBlobFileSystem().close();\r\n  }\r\n{code}\r\n\r\nstack\r\n{code}\r\n[ERROR] org.apache.hadoop.validator.TestRuntimeValid.testABFSConstructor -- Time elapsed: 0.003 s <<< ERROR!\r\njava.lang.NullPointerException: Cannot invoke \"org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.getClient()\" because \"this.abfsStore\" is null\r\n        at org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem.close(AzureBlobFileSystem.java:800)\r\n        at org.apache.hadoop.validator.TestRuntimeValid.testABFSConstructor(TestRuntimeValid.java:49)\r\n{code}\r\n\r\n\r\n", "comments": [], "derived_tasks": {"summarization": "ABFS: NPE when close() called on uninitialized filesystem", "classification": "feature", "qna": {"question": "What is the issue 'ABFS: NPE when close() called on uninitialized filesystem' about?", "answer": "code\r\n{code}\r\n  public void testABFSConstructor() throws Throwable {\r\n    new AzureBlobFileSystem().close();\r\n  }\r\n{code}\r\n\r\nstack\r\n{code}\r\n[ERROR] org.apache.hadoop.validator.TestRuntimeValid.testABFSConstructor -- Time elapsed: 0.003 s <<< ERROR!\r\njava.lang.NullPointerException: Cannot invoke \"org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.getClient()\" because \"this.abfsStore\" is null\r\n        at org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem.close(AzureBlobFileSystem.java:800)\r\n        at org.apache.hadoop.validator.TestRuntimeValid.testABFSConstructor(TestRuntimeValid.java:49)\r\n{code}\r\n\r\n\r\n"}}}
{"project": "HADOOP", "issue_key": "HADOOP-19649", "title": "ABFS: Fixing Test Failures and Wrong assumptions after Junit Upgrade", "status": "Resolved", "reporter": "Anuj Modi", "assignee": "Anuj Modi", "priority": "Major", "labels": ["pull-request-available"], "created": "2025-08-13T06:11:03.000+0000", "updated": "2025-08-26T03:57:39.000+0000", "description": "After https://issues.apache.org/jira/browse/HADOOP-19425\r\n\r\nmost of the integration tests are getting skipped. All tests need to be fixed with this PR\u00a0", "comments": [], "derived_tasks": {"summarization": "ABFS: Fixing Test Failures and Wrong assumptions after Junit Upgrade", "classification": "feature", "qna": {"question": "What is the issue 'ABFS: Fixing Test Failures and Wrong assumptions after Junit Upgrade' about?", "answer": "After https://issues.apache.org/jira/browse/HADOOP-19425\r\n\r\nmost of the integration tests are getting skipped. All tests need to be fixed with this PR\u00a0"}}}
{"project": "HADOOP", "issue_key": "HADOOP-19648", "title": "cos use token credential will lost token field", "status": "Resolved", "reporter": "sanqingleo", "assignee": "sanqingleo", "priority": "Critical", "labels": ["pull-request-available"], "created": "2025-08-11T02:44:05.000+0000", "updated": "2025-09-02T15:14:00.000+0000", "description": "Hi,\r\n\r\nI've discovered a bug when accessing COSN using temporary credentials (access key, secret key, and session token).\r\n\r\nIn the org.apache.hadoop.fs.cosn.CosNativeFileSystemStore#initCOSClient method, when the client is initialized, it only passes the access key and secret key, completely ignoring the session token. This causes all subsequent operations that rely on these temporary credentials to fail.\r\n\r\nFurthermore, this re-initialization step seems unnecessary. Instead of creating a new client with incomplete credentials, the existing credential provider (which already contains the AK, SK, and token) should be passed down directly.\r\n!image-2025-08-11-10-37-12-451.png|width=1048,height=540! \r\n!image-2025-08-11-10-42-36-375.png!", "comments": [], "derived_tasks": {"summarization": "cos use token credential will lost token field", "classification": "feature", "qna": {"question": "What is the issue 'cos use token credential will lost token field' about?", "answer": "Hi,\r\n\r\nI've discovered a bug when accessing COSN using temporary credentials (access key, secret key, and session token).\r\n\r\nIn the org.apache.hadoop.fs.cosn.CosNativeFileSystemStore#initCOSClient method, when the client is initialized, it only passes the access key and secret key, completely ignoring the session token. This causes all subsequent operations that rely on these temporary credentials to fail.\r\n\r\nFurthermore, this re-initialization step seems unnecessary. Instead of creating a new client with incomplete credentials, the existing credential provider (which already contains the AK, SK, and token) should be passed down directly.\r\n!image-2025-08-11-10-37-12-451.png|width=1048,height=540! \r\n!image-2025-08-11-10-42-36-375.png!"}}}
{"project": "HADOOP", "issue_key": "HADOOP-19647", "title": "ABFS: Read Policy set in openFileOptions should be considered for enabling various optimizations", "status": "Open", "reporter": "Anuj Modi", "assignee": "Anuj Modi", "priority": "Major", "labels": [], "created": "2025-08-07T07:54:02.000+0000", "updated": "2025-08-08T16:09:59.000+0000", "description": "AbfsInputStream should take in account the Read Policy set by user with Open File Options. Based on the read policy set, appropriate optimizations should be enabled and kicked in.", "comments": [], "derived_tasks": {"summarization": "ABFS: Read Policy set in openFileOptions should be considered for enabling various optimizations", "classification": "feature", "qna": {"question": "What is the issue 'ABFS: Read Policy set in openFileOptions should be considered for enabling various optimizations' about?", "answer": "AbfsInputStream should take in account the Read Policy set by user with Open File Options. Based on the read policy set, appropriate optimizations should be enabled and kicked in."}}}
{"project": "HADOOP", "issue_key": "HADOOP-19646", "title": "S3A: Migrate hadoop-aws module from JUnit4 Assume to JUnit5 Assumptions", "status": "Resolved", "reporter": "Shilun Fan", "assignee": "Shilun Fan", "priority": "Major", "labels": ["pull-request-available"], "created": "2025-08-06T05:12:27.000+0000", "updated": "2025-09-12T16:49:49.000+0000", "description": "This task aims to migrate the test code in the {{hadoop-aws}} module from JUnit4's {{org.junit.Assume}} API to JUnit5's {{org.junit.jupiter.api.Assumptions}} API in order to unify the testing framework and improve maintainability.\r\nh4. Scope of changes:\r\n * Replace usages of {{{}Assume.assumeTrue(...){}}}, {{{}Assume.assumeFalse(...){}}}, etc., with the corresponding methods from JUnit5, such as {{{}Assumptions.assumeTrue(...){}}};\r\n\r\n * Ensure the assertion logic remains consistent with the original behavior;\r\n\r\n * Update any outdated import statements referencing JUnit4's {{{}Assume{}}};\r\n\r\n * Verify that all affected unit tests pass correctly under JUnit5.", "comments": [], "derived_tasks": {"summarization": "S3A: Migrate hadoop-aws module from JUnit4 Assume to JUnit5 Assumptions", "classification": "feature", "qna": {"question": "What is the issue 'S3A: Migrate hadoop-aws module from JUnit4 Assume to JUnit5 Assumptions' about?", "answer": "This task aims to migrate the test code in the {{hadoop-aws}} module from JUnit4's {{org.junit.Assume}} API to JUnit5's {{org.junit.jupiter.api.Assumptions}} API in order to unify the testing framework and improve maintainability.\r\nh4. Scope of changes:\r\n * Replace usages of {{{}Assume.assumeTrue(...){}}}, {{{}Assume.assumeFalse(...){}}}, etc., with the corresponding methods from JUnit5, such as {{{}Assumptions.assumeTrue(...){}}};\r\n\r\n * Ensure the assertion logic remains consistent with the original behavior;\r\n\r\n * Update any outdated import statements referencing JUnit4's {{{}Assume{}}};\r\n\r\n * Verify that all affected unit tests pass correctly under JUnit5."}}}
{"project": "HADOOP", "issue_key": "HADOOP-19645", "title": "ABFS: [ReadAheadV2] Improve Metrics for Read Calls to identify type of read done.", "status": "Resolved", "reporter": "Anuj Modi", "assignee": "Anuj Modi", "priority": "Major", "labels": ["pull-request-available"], "created": "2025-07-29T12:45:56.000+0000", "updated": "2025-08-26T03:58:21.000+0000", "description": "There are a number of ways in which ABFS driver can trigger a network call to read data. We need a way to identify what type of read call was made from client. Plan is to add an indication for this in already present ClientRequestId header.\r\n\r\nFollowing are types of read we want to identify:\r\n # Direct Read: Read from a given position in remote file. This will be synchronous read\r\n # Normal Read: Read from current seeked position where read ahead was bypassed. This will be synchronous read.\r\n # Prefetch Read: Read triggered from background threads filling up in memory cache. This will be asynchronous read.\r\n # Missed Cache Read: Read triggered after nothing was received from read ahead. This will be synchronous read.\r\n # Footer Read: Read triggered as part of footer read optimization. This will be synchronous.\r\n # Small File Read: Read triggered as a part of small file read. This will be synchronous read.\r\n\r\nWe will add another field in the Tracing Header (Client Request Id) for each request. We can call this field \"Operation Specific Header\" very similar to how we have \"Retry Header\" today. As part of this we will only use it for read operations keeping it empty for other operations. Moving ahead f we need to publish any operation specific info, same header can be used.", "comments": [], "derived_tasks": {"summarization": "ABFS: [ReadAheadV2] Improve Metrics for Read Calls to identify type of read done.", "classification": "feature", "qna": {"question": "What is the issue 'ABFS: [ReadAheadV2] Improve Metrics for Read Calls to identify type of read done.' about?", "answer": "There are a number of ways in which ABFS driver can trigger a network call to read data. We need a way to identify what type of read call was made from client. Plan is to add an indication for this in already present ClientRequestId header.\r\n\r\nFollowing are types of read we want to identify:\r\n # Direct Read: Read from a given position in remote file. This will be synchronous read\r\n # Normal Read: Read from current seeked position where read ahead was bypassed. This will be synchronous read.\r\n # Prefetch Read: Read triggered from background threads filling up in memory cache. This will be asynchronous read.\r\n # Missed Cache Read: Read triggered after nothing was received from read ahead. This will be synchronous read.\r\n # Footer Read: Read triggered as part of footer read optimization. This will be synchronous.\r\n # Small File Read: Read triggered as a part of small file read. This will be synchronous read.\r\n\r\nWe will add another field in the Tracing Header (Client Request Id) for each request. We can call this field \"Operation Specific Header\" very similar to how we have \"Retry Header\" today. As part of this we will only use it for read operations keeping it empty for other operations. Moving ahead f we need to publish any operation specific info, same header can be used."}}}
{"project": "HADOOP", "issue_key": "HADOOP-19644", "title": "ABFS: [ReadAheadV2] Negative tests for Read Buffer Manager V2 and dynamic scaling", "status": "Open", "reporter": "Anuj Modi", "assignee": "Anuj Modi", "priority": "Major", "labels": [], "created": "2025-07-29T10:52:56.000+0000", "updated": "2025-07-29T10:53:52.000+0000", "description": null, "comments": [], "derived_tasks": {"summarization": "ABFS: [ReadAheadV2] Negative tests for Read Buffer Manager V2 and dynamic scaling", "classification": "feature", "qna": {"question": "What is the issue 'ABFS: [ReadAheadV2] Negative tests for Read Buffer Manager V2 and dynamic scaling' about?", "answer": null}}}
{"project": "HADOOP", "issue_key": "HADOOP-19643", "title": "upgrade gson due to security fixes", "status": "Open", "reporter": "PJ Fanning", "assignee": null, "priority": "Major", "labels": ["pull-request-available"], "created": "2025-07-29T10:11:17.000+0000", "updated": "2025-07-29T20:51:34.000+0000", "description": "not sure why https://github.com/google/gson/pull/2588 didn't attract a CVE\r\n\r\nlinked to https://issues.apache.org/jira/browse/HADOOP-19632 - nimbus-jose-jwt uses gson", "comments": [], "derived_tasks": {"summarization": "upgrade gson due to security fixes", "classification": "feature", "qna": {"question": "What is the issue 'upgrade gson due to security fixes' about?", "answer": "not sure why https://github.com/google/gson/pull/2588 didn't attract a CVE\r\n\r\nlinked to https://issues.apache.org/jira/browse/HADOOP-19632 - nimbus-jose-jwt uses gson"}}}
{"project": "HADOOP", "issue_key": "HADOOP-19642", "title": "upgrade nimbus-jose-jwt due to CVE-2025-53864", "status": "Resolved", "reporter": "PJ Fanning", "assignee": null, "priority": "Major", "labels": [], "created": "2025-07-29T09:37:38.000+0000", "updated": "2025-07-29T09:54:35.000+0000", "description": "https://www.cve.org/CVERecord?id=CVE-2025-53864", "comments": [], "derived_tasks": {"summarization": "upgrade nimbus-jose-jwt due to CVE-2025-53864", "classification": "feature", "qna": {"question": "What is the issue 'upgrade nimbus-jose-jwt due to CVE-2025-53864' about?", "answer": "https://www.cve.org/CVERecord?id=CVE-2025-53864"}}}
{"project": "HADOOP", "issue_key": "HADOOP-19641", "title": "ABFS: [ReadAheadV2] First Read should bypass ReadBufferManager", "status": "Open", "reporter": "Anuj Modi", "assignee": "Anuj Modi", "priority": "Major", "labels": ["Performance"], "created": "2025-07-29T07:01:30.000+0000", "updated": "2025-08-07T07:55:08.000+0000", "description": "We have observed this across multiple workload runs that when we start reading data from input stream. The first read which came to input stream has to be read synchronously even if we trigger prefetch request for that particular offset. Most of the times we end up doing extra work of checking if the prefetch is trigerred, removing prefetch from the pending queue and go ahead to do a direct remote read in workload thread itself.\r\n\r\nTo avoid all this overhead, we will always bypass read ahead for the very first read of each input stream and trigger read aheads for second read onwards.", "comments": [], "derived_tasks": {"summarization": "ABFS: [ReadAheadV2] First Read should bypass ReadBufferManager", "classification": "feature", "qna": {"question": "What is the issue 'ABFS: [ReadAheadV2] First Read should bypass ReadBufferManager' about?", "answer": "We have observed this across multiple workload runs that when we start reading data from input stream. The first read which came to input stream has to be read synchronously even if we trigger prefetch request for that particular offset. Most of the times we end up doing extra work of checking if the prefetch is trigerred, removing prefetch from the pending queue and go ahead to do a direct remote read in workload thread itself.\r\n\r\nTo avoid all this overhead, we will always bypass read ahead for the very first read of each input stream and trigger read aheads for second read onwards."}}}
{"project": "HADOOP", "issue_key": "HADOOP-19640", "title": "Resource leak in AssumedRoleCredentialProvider", "status": "Open", "reporter": "Antoni Reus", "assignee": null, "priority": "Minor", "labels": [], "created": "2025-07-24T08:01:58.000+0000", "updated": "2025-07-24T08:02:55.000+0000", "description": "When `org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider` is used in a Hadoop Configuration object, it will attempt to `resolveCredentials()` inside the constructor.\r\n\r\n(lines 165-167)\r\n{code:java}\r\n// and force in a fail-fast check just to keep the stack traces less\r\n// convoluted\r\nresolveCredentials();{code}\r\n\r\nIf this method fails, because the current identity is not able to assume role, the constructor will throw an exception, and fail to close the `stsClient`, `stsProvider` and any other resources that are created in the constructor, leaking threads and other resources.\r\n\u00a0\r\nIn a long running application, that handles Hadoop S3 file systems, where the user can dynamically change to configured role to assume, and external id, this will lead to eventually the system running out of resources due to the leaked threads created by the AWS SDK clients that are not closed when a wrong role or external id is used.\r\n\u00a0\r\n\r\nThere are two potential fixes for this problem:\r\n\r\n\u00a0- Don't attempt to `resolveCredentials()` inside the constructor\r\n\r\n\u00a0- Wrap the `resolveCredentials()` in the constructor in a try/catch block, that cleans the resources and rethrows the exception in the catch block.", "comments": [], "derived_tasks": {"summarization": "Resource leak in AssumedRoleCredentialProvider", "classification": "feature", "qna": {"question": "What is the issue 'Resource leak in AssumedRoleCredentialProvider' about?", "answer": "When `org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider` is used in a Hadoop Configuration object, it will attempt to `resolveCredentials()` inside the constructor.\r\n\r\n(lines 165-167)\r\n{code:java}\r\n// and force in a fail-fast check just to keep the stack traces less\r\n// convoluted\r\nresolveCredentials();{code}\r\n\r\nIf this method fails, because the current identity is not able to assume role, the constructor will throw an exception, and fail to close the `stsClient`, `stsProvider` and any other resources that are created in the constructor, leaking threads and other resources.\r\n\u00a0\r\nIn a long running application, that handles Hadoop S3 file systems, where the user can dynamically change to configured role to assume, and external id, this will lead to eventually the system running out of resources due to the leaked threads created by the AWS SDK clients that are not closed when a wrong role or external id is used.\r\n\u00a0\r\n\r\nThere are two potential fixes for this problem:\r\n\r\n\u00a0- Don't attempt to `resolveCredentials()` inside the constructor\r\n\r\n\u00a0- Wrap the `resolveCredentials()` in the constructor in a try/catch block, that cleans the resources and rethrows the exception in the catch block."}}}
{"project": "HADOOP", "issue_key": "HADOOP-19639", "title": "SecretManager configuration at runtime", "status": "Resolved", "reporter": "Bence Kosztolnik", "assignee": "Bence Kosztolnik", "priority": "Major", "labels": ["pull-request-available"], "created": "2025-07-23T16:17:56.000+0000", "updated": "2025-08-02T02:39:45.000+0000", "description": "In case of TEZ *DAGAppMaster* the Hadoop *SecretManager* code can not read yarn config xml file, therefore the SELECTED_ALGORITHM and SELECTED_LENGTH variables in SecretManager can not be set at runtime.\r\nThis can results with the following exception in FIPS environment:\r\n\r\n{code:java}\r\njava.security.InvalidParameterException: Key size for HMAC must be at least 112 bits in approved mode: SHA-1/HMAC\r\n\tat com.safelogic.cryptocomply.fips.core/com.safelogic.cryptocomply.jcajce.provider.BaseKeyGenerator.engineInit(Unknown Source)\r\n\tat java.base/javax.crypto.KeyGenerator.init(KeyGenerator.java:540)\r\n\tat java.base/javax.crypto.KeyGenerator.init(KeyGenerator.java:517)\r\n\tat org.apache.hadoop.security.token.SecretManager.<init>(SecretManager.java:157)\r\n\tat org.apache.hadoop.yarn.security.client.BaseClientToAMTokenSecretManager.<init>(BaseClientToAMTokenSecretManager.java:38)\r\n\tat org.apache.hadoop.yarn.security.client.ClientToAMTokenSecretManager.<init>(ClientToAMTokenSecretManager.java:46)\r\n\tat org.apache.tez.common.security.TezClientToAMTokenSecretManager.<init>(TezClientToAMTokenSecretManager.java:33)\r\n\tat org.apache.tez.dag.app.DAGAppMaster.serviceInit(DAGAppMaster.java:493)\r\n\tat org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)\r\n\tat org.apache.tez.dag.app.DAGAppMaster$9.run(DAGAppMaster.java:2649)\r\n\tat java.base/java.security.AccessController.doPrivileged(Native Method)\r\n\tat java.base/javax.security.auth.Subject.doAs(Subject.java:423)\r\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1910)\r\n\tat org.apache.tez.dag.app.DAGAppMaster.initAndStartAppMaster(DAGAppMaster.java:2646)\r\n\tat org.apache.tez.dag.app.DAGAppMaster.main(DAGAppMaster.java:2440)\r\n{code}\r\n\r\nTo mitigate the problem we should provide some ability for the component to be able to modify the configuration without corresponding config files on class path.", "comments": [], "derived_tasks": {"summarization": "SecretManager configuration at runtime", "classification": "feature", "qna": {"question": "What is the issue 'SecretManager configuration at runtime' about?", "answer": "In case of TEZ *DAGAppMaster* the Hadoop *SecretManager* code can not read yarn config xml file, therefore the SELECTED_ALGORITHM and SELECTED_LENGTH variables in SecretManager can not be set at runtime.\r\nThis can results with the following exception in FIPS environment:\r\n\r\n{code:java}\r\njava.security.InvalidParameterException: Key size for HMAC must be at least 112 bits in approved mode: SHA-1/HMAC\r\n\tat com.safelogic.cryptocomply.fips.core/com.safelogic.cryptocomply.jcajce.provider.BaseKeyGenerator.engineInit(Unknown Source)\r\n\tat java.base/javax.crypto.KeyGenerator.init(KeyGenerator.java:540)\r\n\tat java.base/javax.crypto.KeyGenerator.init(KeyGenerator.java:517)\r\n\tat org.apache.hadoop.security.token.SecretManager.<init>(SecretManager.java:157)\r\n\tat org.apache.hadoop.yarn.security.client.BaseClientToAMTokenSecretManager.<init>(BaseClientToAMTokenSecretManager.java:38)\r\n\tat org.apache.hadoop.yarn.security.client.ClientToAMTokenSecretManager.<init>(ClientToAMTokenSecretManager.java:46)\r\n\tat org.apache.tez.common.security.TezClientToAMTokenSecretManager.<init>(TezClientToAMTokenSecretManager.java:33)\r\n\tat org.apache.tez.dag.app.DAGAppMaster.serviceInit(DAGAppMaster.java:493)\r\n\tat org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)\r\n\tat org.apache.tez.dag.app.DAGAppMaster$9.run(DAGAppMaster.java:2649)\r\n\tat java.base/java.security.AccessController.doPrivileged(Native Method)\r\n\tat java.base/javax.security.auth.Subject.doAs(Subject.java:423)\r\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1910)\r\n\tat org.apache.tez.dag.app.DAGAppMaster.initAndStartAppMaster(DAGAppMaster.java:2646)\r\n\tat org.apache.tez.dag.app.DAGAppMaster.main(DAGAppMaster.java:2440)\r\n{code}\r\n\r\nTo mitigate the problem we should provide some ability for the component to be able to modify the configuration without corresponding config files on class path."}}}
{"project": "HADOOP", "issue_key": "HADOOP-19638", "title": "[JDK17] Set Up CI Support JDK17 & JDK21", "status": "Resolved", "reporter": "Shilun Fan", "assignee": "Shilun Fan", "priority": "Major", "labels": ["pull-request-available"], "created": "2025-07-23T13:49:51.000+0000", "updated": "2025-10-23T00:11:53.000+0000", "description": "Plan to establish a new build pipeline using JDK 17 for Apache Hadoop, as part of the ongoing effort to upgrade the project\u2019s Java compatibility. This pipeline will serve as the foundation for testing and validating future JDK 17 support and ensuring smooth integration with existing CI/CD processes.", "comments": [], "derived_tasks": {"summarization": "[JDK17] Set Up CI Support JDK17 & JDK21", "classification": "feature", "qna": {"question": "What is the issue '[JDK17] Set Up CI Support JDK17 & JDK21' about?", "answer": "Plan to establish a new build pipeline using JDK 17 for Apache Hadoop, as part of the ongoing effort to upgrade the project\u2019s Java compatibility. This pipeline will serve as the foundation for testing and validating future JDK 17 support and ensuring smooth integration with existing CI/CD processes."}}}
{"project": "HADOOP", "issue_key": "HADOOP-19637", "title": "[JDK17] Attempted to build on CentOS Stream 9.", "status": "In Progress", "reporter": "Shilun Fan", "assignee": "Shilun Fan", "priority": "Major", "labels": [], "created": "2025-07-23T13:37:59.000+0000", "updated": "2025-07-23T13:38:21.000+0000", "description": "We aim to successfully compile Apache Hadoop on CentOS Stream 9 to verify the feasibility of using this platform in future development and deployment environments.\r\n\r\nThis task may involve addressing potential compatibility issues related to GCC and libc versions. Initial attempts will focus on resolving compilation challenges and ensuring the platform meets the necessary requirements for Hadoop builds.", "comments": [], "derived_tasks": {"summarization": "[JDK17] Attempted to build on CentOS Stream 9.", "classification": "feature", "qna": {"question": "What is the issue '[JDK17] Attempted to build on CentOS Stream 9.' about?", "answer": "We aim to successfully compile Apache Hadoop on CentOS Stream 9 to verify the feasibility of using this platform in future development and deployment environments.\r\n\r\nThis task may involve addressing potential compatibility issues related to GCC and libc versions. Initial attempts will focus on resolving compilation challenges and ensuring the platform meets the necessary requirements for Hadoop builds."}}}
{"project": "HADOOP", "issue_key": "HADOOP-19636", "title": "[JDK17] Remove CentOS 7 Support and Clean Up Dockerfile. ", "status": "Resolved", "reporter": "Shilun Fan", "assignee": "Shilun Fan", "priority": "Major", "labels": ["pull-request-available"], "created": "2025-07-23T13:31:22.000+0000", "updated": "2025-09-18T06:26:42.000+0000", "description": "Removed build support for the following EOL (End-of-Life) operating system versions (e.g., CentOS 7, Debian 9, etc.).\r\n\r\nCleaned up redundant code and dependency configurations in the Dockerfile related to these operating systems.\r\n\r\nOptimized the build logic to ensure that currently supported OS versions build successfully.", "comments": [], "derived_tasks": {"summarization": "[JDK17] Remove CentOS 7 Support and Clean Up Dockerfile. ", "classification": "feature", "qna": {"question": "What is the issue '[JDK17] Remove CentOS 7 Support and Clean Up Dockerfile. ' about?", "answer": "Removed build support for the following EOL (End-of-Life) operating system versions (e.g., CentOS 7, Debian 9, etc.).\r\n\r\nCleaned up redundant code and dependency configurations in the Dockerfile related to these operating systems.\r\n\r\nOptimized the build logic to ensure that currently supported OS versions build successfully."}}}
{"project": "HADOOP", "issue_key": "HADOOP-19635", "title": "ABFS: [FNS Over Blob] Marker creation fail exception should not be propagated", "status": "Open", "reporter": "Anmol Asrani", "assignee": "Anmol Asrani", "priority": "Major", "labels": ["pull-request-available"], "created": "2025-07-23T12:14:47.000+0000", "updated": "2025-09-30T10:09:13.000+0000", "description": "Marker creation is attempted during certain operations such as {*}create{*}, {*}getPathStatus{*}, {*}setPathProperties{*}, and {*}rename{*}, in order to add a 0-byte file that signifies the existence of a folder at that path. However, if marker creation fails, the failure should not be propagated to the user.", "comments": [], "derived_tasks": {"summarization": "ABFS: [FNS Over Blob] Marker creation fail exception should not be propagated", "classification": "feature", "qna": {"question": "What is the issue 'ABFS: [FNS Over Blob] Marker creation fail exception should not be propagated' about?", "answer": "Marker creation is attempted during certain operations such as {*}create{*}, {*}getPathStatus{*}, {*}setPathProperties{*}, and {*}rename{*}, in order to add a 0-byte file that signifies the existence of a folder at that path. However, if marker creation fails, the failure should not be propagated to the user."}}}
{"project": "HADOOP", "issue_key": "HADOOP-19634", "title": "acknowledge Guava license on LimitInputStream", "status": "Resolved", "reporter": "PJ Fanning", "assignee": "PJ Fanning", "priority": "Major", "labels": ["pull-request-available"], "created": "2025-07-23T11:57:14.000+0000", "updated": "2025-07-23T16:42:06.000+0000", "description": "When ASF projects copy 3rd party code into their code bases, they are meant to:\r\n* check the orginal license is Category A - https://www.apache.org/legal/resolved.html\r\n* keep the original source code headers\r\n* add something to their LICENSE that mentions the source file and what license is on it\r\n* if the 3rd party project is Apache licensed but has a NOTICE file, we need to copy its contents to our NOTICE\r\n* these requirements are only negated if the original code is submitted to the ASF project by the code's copyright holder (the individual or company that wrote the original code).\r\n\r\n* Hadoop copy https://github.com/apache/hadoop/blob/c357e435fd691b4c184b82325bef6d7c65e5f32b/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/LimitInputStream.java#L39\r\n* Original code has a Guava copyright that we probably need to keep https://github.com/google/guava/blob/master/guava/src/com/google/common/io/ByteStreams.java", "comments": [], "derived_tasks": {"summarization": "acknowledge Guava license on LimitInputStream", "classification": "feature", "qna": {"question": "What is the issue 'acknowledge Guava license on LimitInputStream' about?", "answer": "When ASF projects copy 3rd party code into their code bases, they are meant to:\r\n* check the orginal license is Category A - https://www.apache.org/legal/resolved.html\r\n* keep the original source code headers\r\n* add something to their LICENSE that mentions the source file and what license is on it\r\n* if the 3rd party project is Apache licensed but has a NOTICE file, we need to copy its contents to our NOTICE\r\n* these requirements are only negated if the original code is submitted to the ASF project by the code's copyright holder (the individual or company that wrote the original code).\r\n\r\n* Hadoop copy https://github.com/apache/hadoop/blob/c357e435fd691b4c184b82325bef6d7c65e5f32b/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/LimitInputStream.java#L39\r\n* Original code has a Guava copyright that we probably need to keep https://github.com/google/guava/blob/master/guava/src/com/google/common/io/ByteStreams.java"}}}
{"project": "HADOOP", "issue_key": "HADOOP-19633", "title": "upgrade commons-beanutils to 1.11.0", "status": "Open", "reporter": "Ananya Singh", "assignee": "Ananya Singh", "priority": "Major", "labels": [], "created": "2025-07-23T09:01:52.000+0000", "updated": "2025-07-23T09:01:52.000+0000", "description": null, "comments": [], "derived_tasks": {"summarization": "upgrade commons-beanutils to 1.11.0", "classification": "feature", "qna": {"question": "What is the issue 'upgrade commons-beanutils to 1.11.0' about?", "answer": null}}}
{"project": "HADOOP", "issue_key": "HADOOP-19632", "title": "Upgrade nimbusds to 10.0.2", "status": "Resolved", "reporter": "Ananya Singh", "assignee": "Rohit Kumar", "priority": "Major", "labels": ["pull-request-available"], "created": "2025-07-23T09:00:38.000+0000", "updated": "2025-09-24T15:17:03.000+0000", "description": "Includes fix for CVE-2025-53864", "comments": [], "derived_tasks": {"summarization": "Upgrade nimbusds to 10.0.2", "classification": "feature", "qna": {"question": "What is the issue 'Upgrade nimbusds to 10.0.2' about?", "answer": "Includes fix for CVE-2025-53864"}}}
{"project": "KAFKA", "issue_key": "KAFKA-19853", "title": "StreamThread blocks on StateUpdater during onAssignment()", "status": "Open", "reporter": "Colt McNealy", "assignee": null, "priority": "Major", "labels": [], "created": "2025-11-01T04:19:06.000+0000", "updated": "2025-11-01T04:19:44.000+0000", "description": "We've observed that the `StreamThread` blocks waiting for a `Future` from the `StateUpdater` in the `StreamsPartitionAssigner#onAssignment()` method when we are moving a task out of the `StateUpdater` and onto the `StreamThread`.\r\n\r\n\u00a0\r\n\r\nThis can cause problems because, during restoration or with warmup replicas, the `StateUpdater#runOnce()` method can take a long time (upwards of 20 seconds) when RocksDB stalls writes to allow compaction to keep up. In EOS this blockage may cause the transaction to time out, which is a big mess. This is because the `StreamThread` may have an open transaction before the `StreamsPartitionAssignor#onAssignment()` method is called.\r\n\r\n\u00a0\r\n\r\nSome screenshots from the JFR below (credit to [~eduwerc]).", "comments": [], "derived_tasks": {"summarization": "StreamThread blocks on StateUpdater during onAssignment()", "classification": "feature", "qna": {"question": "What is the issue 'StreamThread blocks on StateUpdater during onAssignment()' about?", "answer": "We've observed that the `StreamThread` blocks waiting for a `Future` from the `StateUpdater` in the `StreamsPartitionAssigner#onAssignment()` method when we are moving a task out of the `StateUpdater` and onto the `StreamThread`.\r\n\r\n\u00a0\r\n\r\nThis can cause problems because, during restoration or with warmup replicas, the `StateUpdater#runOnce()` method can take a long time (upwards of 20 seconds) when RocksDB stalls writes to allow compaction to keep up. In EOS this blockage may cause the transaction to time out, which is a big mess. This is because the `StreamThread` may have an open transaction before the `StreamsPartitionAssignor#onAssignment()` method is called.\r\n\r\n\u00a0\r\n\r\nSome screenshots from the JFR below (credit to [~eduwerc])."}}}
{"project": "KAFKA", "issue_key": "KAFKA-19852", "title": "MetadataShell read 0000-0000.checkpoint will cause infinite loop", "status": "Open", "reporter": "TaiJuWu", "assignee": "TaiJuWu", "priority": "Major", "labels": [], "created": "2025-10-31T23:40:10.000+0000", "updated": "2025-10-31T23:40:10.000+0000", "description": null, "comments": [], "derived_tasks": {"summarization": "MetadataShell read 0000-0000.checkpoint will cause infinite loop", "classification": "feature", "qna": {"question": "What is the issue 'MetadataShell read 0000-0000.checkpoint will cause infinite loop' about?", "answer": null}}}
{"project": "KAFKA", "issue_key": "KAFKA-19851", "title": "Delete dynamic config that were removed by Kafka", "status": "Open", "reporter": "Jos\u00e9 Armando Garc\u00eda Sancio", "assignee": "Jos\u00e9 Armando Garc\u00eda Sancio", "priority": "Major", "labels": [], "created": "2025-10-31T14:02:35.000+0000", "updated": "2025-10-31T14:29:47.000+0000", "description": "[KIP-724: Drop support for message formats v0 and v1|https://cwiki.apache.org/confluence/display/KAFKA/KIP-724%3A+Drop+support+for+message+formats+v0+and+v1]\u00a0 and Kafka 4.0.0 removed support for the dynamic configs like message.format.version.\r\n\r\nWhen the user upgrades from 3.x to 4.x it can cause the dynamic configuration validation to fail if there are dynamic configurations that are no longer supported.\r\n{code:java}\r\nCaused by: org.apache.kafka.common.errors.InvalidConfigurationException: Unknown topic config name: message.format.version {code}\r\nOne solution for solving this problem is to have a well define list of allowed dynamic configuration. Any config that doesn't match this list will be automatically deleted.\r\n\r\nThis deletion can be done implicitly when applying the ConfigRecord record. This behavior can be further guarded by checking that the metadata version is greater than 4.0. This also means that upgrading the metadata version to 4.0 would cause all of the removed config to get deleted.", "comments": [], "derived_tasks": {"summarization": "Delete dynamic config that were removed by Kafka", "classification": "feature", "qna": {"question": "What is the issue 'Delete dynamic config that were removed by Kafka' about?", "answer": "[KIP-724: Drop support for message formats v0 and v1|https://cwiki.apache.org/confluence/display/KAFKA/KIP-724%3A+Drop+support+for+message+formats+v0+and+v1]\u00a0 and Kafka 4.0.0 removed support for the dynamic configs like message.format.version.\r\n\r\nWhen the user upgrades from 3.x to 4.x it can cause the dynamic configuration validation to fail if there are dynamic configurations that are no longer supported.\r\n{code:java}\r\nCaused by: org.apache.kafka.common.errors.InvalidConfigurationException: Unknown topic config name: message.format.version {code}\r\nOne solution for solving this problem is to have a well define list of allowed dynamic configuration. Any config that doesn't match this list will be automatically deleted.\r\n\r\nThis deletion can be done implicitly when applying the ConfigRecord record. This behavior can be further guarded by checking that the metadata version is greater than 4.0. This also means that upgrading the metadata version to 4.0 would cause all of the removed config to get deleted."}}}
{"project": "KAFKA", "issue_key": "KAFKA-19850", "title": "KRaft voter auto join will add a removed voter immediately", "status": "Open", "reporter": "Luke Chen", "assignee": null, "priority": "Major", "labels": [], "created": "2025-10-31T10:23:35.000+0000", "updated": "2025-10-31T11:39:52.000+0000", "description": "In v4.2.0, we are able to auto join a controller with the configuration `controller.quorum.auto.join.enable=true` set ([KIP-853|https://cwiki.apache.org/confluence/display/KAFKA/KIP-853%3A+KRaft+Controller+Membership+Changes#KIP853:KRaftControllerMembershipChanges-Controllerautojoining](KAFKA-19078)). This is a good improvement for controller addition, but it has a UX issue, which is that when a controller is removed via removeVoterRequest, it will be added immediately due to `controller.quorum.auto.join.enable=true`. In the KIP, we also mention you have to stop the controller before removing the controller:\r\n\r\n\u00a0\r\n{noformat}\r\ncontroller.quorum.auto.join.enable:\r\n\r\nControls whether a KRaft controller should automatically join the cluster \r\nmetadata partition for its cluster id. If the configuration is set to \r\ntrue the controller must be stopped before removing the controller with kafka-metadata-quorum remove-controller.{noformat}\r\n\u00a0\r\n\r\nThis is not a user friendly behavior in my opinion. And it will cause many confusion to users and thought there is something wrong in the controller removal. Furthermore, in the kubernetes environment which is controlled by the operator, it is not the cloud native way to shutdown a node, do some operation, then start it up.\u00a0\r\n\r\n\u00a0\r\n\r\nSo, I propose we can improve it by \"the removed controller will not be auto joined before this controller restarted\". That is:\r\n1. Once the controller is removed from voters set, it won't be auto joined even if `controller.quorum.auto.join.enable=true`\r\n\r\n2. The controller can be manually join the voters in this state\r\n\r\n3. The controller node will be auto join the voters set after node restarted.\r\n\r\n\u00a0\r\n\r\nSo basically, the semantics is not changed, it just add some unexpected remove/add loop. Thoughts?\r\n\r\n\u00a0\r\n\r\n\u00a0", "comments": [], "derived_tasks": {"summarization": "KRaft voter auto join will add a removed voter immediately", "classification": "feature", "qna": {"question": "What is the issue 'KRaft voter auto join will add a removed voter immediately' about?", "answer": "In v4.2.0, we are able to auto join a controller with the configuration `controller.quorum.auto.join.enable=true` set ([KIP-853|https://cwiki.apache.org/confluence/display/KAFKA/KIP-853%3A+KRaft+Controller+Membership+Changes#KIP853:KRaftControllerMembershipChanges-Controllerautojoining](KAFKA-19078)). This is a good improvement for controller addition, but it has a UX issue, which is that when a controller is removed via removeVoterRequest, it will be added immediately due to `controller.quorum.auto.join.enable=true`. In the KIP, we also mention you have to stop the controller before removing the controller:\r\n\r\n\u00a0\r\n{noformat}\r\ncontroller.quorum.auto.join.enable:\r\n\r\nControls whether a KRaft controller should automatically join the cluster \r\nmetadata partition for its cluster id. If the configuration is set to \r\ntrue the controller must be stopped before removing the controller with kafka-metadata-quorum remove-controller.{noformat}\r\n\u00a0\r\n\r\nThis is not a user friendly behavior in my opinion. And it will cause many confusion to users and thought there is something wrong in the controller removal. Furthermore, in the kubernetes environment which is controlled by the operator, it is not the cloud native way to shutdown a node, do some operation, then start it up.\u00a0\r\n\r\n\u00a0\r\n\r\nSo, I propose we can improve it by \"the removed controller will not be auto joined before this controller restarted\". That is:\r\n1. Once the controller is removed from voters set, it won't be auto joined even if `controller.quorum.auto.join.enable=true`\r\n\r\n2. The controller can be manually join the voters in this state\r\n\r\n3. The controller node will be auto join the voters set after node restarted.\r\n\r\n\u00a0\r\n\r\nSo basically, the semantics is not changed, it just add some unexpected remove/add loop. Thoughts?\r\n\r\n\u00a0\r\n\r\n\u00a0"}}}
{"project": "KAFKA", "issue_key": "KAFKA-19849", "title": "Move ThrottledChannelExpirationTest to server-common module", "status": "Open", "reporter": "Chia-Ping Tsai", "assignee": "Lan Ding", "priority": "Minor", "labels": [], "created": "2025-10-31T08:28:28.000+0000", "updated": "2025-10-31T08:33:17.000+0000", "description": "With *KAFKA-19817 moving all related components to `server-common`, we can rewrite and migrate `ThrottledChannelExpirationTest`. Also, the test that was moved in KAFKA-17372* should be merged into it.", "comments": [], "derived_tasks": {"summarization": "Move ThrottledChannelExpirationTest to server-common module", "classification": "feature", "qna": {"question": "What is the issue 'Move ThrottledChannelExpirationTest to server-common module' about?", "answer": "With *KAFKA-19817 moving all related components to `server-common`, we can rewrite and migrate `ThrottledChannelExpirationTest`. Also, the test that was moved in KAFKA-17372* should be merged into it."}}}
{"project": "KAFKA", "issue_key": "KAFKA-19848", "title": "Revert KIP-939 API's and Client Code for 4.2", "status": "Open", "reporter": "Ritika Reddy", "assignee": "Ritika Reddy", "priority": "Blocker", "labels": [], "created": "2025-10-30T23:42:05.000+0000", "updated": "2025-10-30T23:42:05.000+0000", "description": "KIP-939 has been paused but some of the client code and new API's have already been merged to trunk. We need to revert the changes in the 4.2 release branch.", "comments": [], "derived_tasks": {"summarization": "Revert KIP-939 API's and Client Code for 4.2", "classification": "feature", "qna": {"question": "What is the issue 'Revert KIP-939 API's and Client Code for 4.2' about?", "answer": "KIP-939 has been paused but some of the client code and new API's have already been merged to trunk. We need to revert the changes in the 4.2 release branch."}}}
{"project": "KAFKA", "issue_key": "KAFKA-19847", "title": "Adjust quorum-related config lower bounds", "status": "Open", "reporter": "TaiJuWu", "assignee": "TaiJuWu", "priority": "Major", "labels": ["need-kip"], "created": "2025-10-30T14:36:04.000+0000", "updated": "2025-10-30T15:03:14.000+0000", "description": "Some config settings are related to each other, or certain configuration constraints haven\u2019t been discussed.\r\n\r\n\r\nSee:\r\n[https://github.com/apache/kafka/pull/18998#discussion_r1988001109]\r\nhttps://github.com/apache/kafka/pull/20318/files#r2465660429", "comments": [], "derived_tasks": {"summarization": "Adjust quorum-related config lower bounds", "classification": "feature", "qna": {"question": "What is the issue 'Adjust quorum-related config lower bounds' about?", "answer": "Some config settings are related to each other, or certain configuration constraints haven\u2019t been discussed.\r\n\r\n\r\nSee:\r\n[https://github.com/apache/kafka/pull/18998#discussion_r1988001109]\r\nhttps://github.com/apache/kafka/pull/20318/files#r2465660429"}}}
{"project": "KAFKA", "issue_key": "KAFKA-19846", "title": "Fix ShareFetch RPC doesn't allow '/' and '+' in memberId field", "status": "Open", "reporter": "Pranav Rathi", "assignee": "Abhinav Dixit", "priority": "Major", "labels": [], "created": "2025-10-30T11:41:43.000+0000", "updated": "2025-10-30T16:40:52.000+0000", "description": "While working on QfK implementation in librdkafka, I found that\u00a0{{memberId}}\u00a0field doesn't accept\u00a0{{/}}\u00a0and\u00a0{{{}+{}}}.\r\n\r\nI get following error on the {*}broker side{*}.\r\n{code:java}\r\n[2025-10-23 15:10:55,319] ERROR [KafkaApi-1] Unexpected error handling request RequestHeader(apiKey=SHARE_FETCH, apiVersion=1, clientId=rdkafka, correlationId=4, headerVersion=2) -- ShareFetchRequestData(groupId='share-group-1', memberId='/FvjN95PRhGaAuA8aQMuTw', shareSessionEpoch=0, maxWaitMs=500, minBytes=1, maxBytes=52428800, maxRecords=500, batchSize=500, topics=[FetchTopic(topicId=sgd0qCnHRL-t80afMzN9nA, partitions=[FetchPartition(partitionIndex=0, partitionMaxBytes=0, acknowledgementBatches=[])])], forgottenTopicsData=[]) with context RequestContext(header=RequestHeader(apiKey=SHARE_FETCH, apiVersion=1, clientId=rdkafka, correlationId=4, headerVersion=2), connectionId='127.0.0.1:9092-127.0.0.1:41442-0-4', clientAddress=/127.0.0.1, principal=User:ANONYMOUS, listenerName=ListenerName(PLAINTEXT), securityProtocol=PLAINTEXT, clientInformation=ClientInformation(softwareName=librdkafka, softwareVersion=2.12.0-RC1-13-g82dbc3-dirty-devel-O0), fromPrivilegedListener=true, principalSerde=Optional[org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder@1648925]) (kafka.server.KafkaApis) java.lang.IllegalArgumentException: Illegal base64 character 2f at java.base/java.util.Base64$Decoder.decode0(Base64.java:848) ~[?:?] at java.base/java.util.Base64$Decoder.decode(Base64.java:566) ~[?:?] at java.base/java.util.Base64$Decoder.decode(Base64.java:589) ~[?:?] at org.apache.kafka.common.Uuid.fromString(Uuid.java:136) ~[kafka-clients-4.1.0.jar:?] at kafka.server.KafkaApis.handleShareFetchRequest(KafkaApis.scala:3157) [kafka_2.13-4.1.0.jar:?] at kafka.server.KafkaApis.handle(KafkaApis.scala:236) [kafka_2.13-4.1.0.jar:?] at kafka.server.KafkaRequestHandler.run(KafkaRequestHandler.scala:158) [kafka_2.13-4.1.0.jar:?] at java.base/java.lang.Thread.run(Thread.java:840) [?:?]\u00a0{code}\r\n*Client* gets\u00a0{{UNKNOWN_SERVER_ERROR}}\u00a0as response from the broker.\r\n\r\n\u00a0\r\n\r\nDigging deeper I found that Uuid\u00a0*{{fromString()}}*\u00a0uses Base64 Url decoding and similarly\u00a0*{{toString()}}*\u00a0uses Base64 Url encoding.\r\n{code:java}\r\npublic static Uuid fromString(String str) {\r\n        if (str.length() > 24) {\r\n            throw new IllegalArgumentException(\"Input string with prefix `\"\r\n                + str.substring(0, 24) + \"` is too long to be decoded as a base64 UUID\");\r\n        }\r\n\r\n        ByteBuffer uuidBytes = ByteBuffer.wrap(Base64.getUrlDecoder().decode(str));\r\n        if (uuidBytes.remaining() != 16) {\r\n            throw new IllegalArgumentException(\"Input string `\" + str + \"` decoded as \"\r\n                + uuidBytes.remaining() + \" bytes, which is not equal to the expected 16 bytes \"\r\n                + \"of a base64-encoded UUID\");\r\n        }\r\n\r\n        return new Uuid(uuidBytes.getLong(), uuidBytes.getLong());\r\n} {code}\r\n{code:java}\r\npublic String toString() {\r\n        return Base64.getUrlEncoder().withoutPadding().encodeToString(getBytesFromUuid());\r\n} {code}\r\n{{I feel that Uuid should use normal Base64 encoding or decoding instead of Url one. If we use Url encoding and decoding then the Uuid itself changes. Generated Uuid and Url Base64 encoding for the Uuid will be different.\u00a0}}{{}}", "comments": [], "derived_tasks": {"summarization": "Fix ShareFetch RPC doesn't allow '/' and '+' in memberId field", "classification": "feature", "qna": {"question": "What is the issue 'Fix ShareFetch RPC doesn't allow '/' and '+' in memberId field' about?", "answer": "While working on QfK implementation in librdkafka, I found that\u00a0{{memberId}}\u00a0field doesn't accept\u00a0{{/}}\u00a0and\u00a0{{{}+{}}}.\r\n\r\nI get following error on the {*}broker side{*}.\r\n{code:java}\r\n[2025-10-23 15:10:55,319] ERROR [KafkaApi-1] Unexpected error handling request RequestHeader(apiKey=SHARE_FETCH, apiVersion=1, clientId=rdkafka, correlationId=4, headerVersion=2) -- ShareFetchRequestData(groupId='share-group-1', memberId='/FvjN95PRhGaAuA8aQMuTw', shareSessionEpoch=0, maxWaitMs=500, minBytes=1, maxBytes=52428800, maxRecords=500, batchSize=500, topics=[FetchTopic(topicId=sgd0qCnHRL-t80afMzN9nA, partitions=[FetchPartition(partitionIndex=0, partitionMaxBytes=0, acknowledgementBatches=[])])], forgottenTopicsData=[]) with context RequestContext(header=RequestHeader(apiKey=SHARE_FETCH, apiVersion=1, clientId=rdkafka, correlationId=4, headerVersion=2), connectionId='127.0.0.1:9092-127.0.0.1:41442-0-4', clientAddress=/127.0.0.1, principal=User:ANONYMOUS, listenerName=ListenerName(PLAINTEXT), securityProtocol=PLAINTEXT, clientInformation=ClientInformation(softwareName=librdkafka, softwareVersion=2.12.0-RC1-13-g82dbc3-dirty-devel-O0), fromPrivilegedListener=true, principalSerde=Optional[org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder@1648925]) (kafka.server.KafkaApis) java.lang.IllegalArgumentException: Illegal base64 character 2f at java.base/java.util.Base64$Decoder.decode0(Base64.java:848) ~[?:?] at java.base/java.util.Base64$Decoder.decode(Base64.java:566) ~[?:?] at java.base/java.util.Base64$Decoder.decode(Base64.java:589) ~[?:?] at org.apache.kafka.common.Uuid.fromString(Uuid.java:136) ~[kafka-clients-4.1.0.jar:?] at kafka.server.KafkaApis.handleShareFetchRequest(KafkaApis.scala:3157) [kafka_2.13-4.1.0.jar:?] at kafka.server.KafkaApis.handle(KafkaApis.scala:236) [kafka_2.13-4.1.0.jar:?] at kafka.server.KafkaRequestHandler.run(KafkaRequestHandler.scala:158) [kafka_2.13-4.1.0.jar:?] at java.base/java.lang.Thread.run(Thread.java:840) [?:?]\u00a0{code}\r\n*Client* gets\u00a0{{UNKNOWN_SERVER_ERROR}}\u00a0as response from the broker.\r\n\r\n\u00a0\r\n\r\nDigging deeper I found that Uuid\u00a0*{{fromString()}}*\u00a0uses Base64 Url decoding and similarly\u00a0*{{toString()}}*\u00a0uses Base64 Url encoding.\r\n{code:java}\r\npublic static Uuid fromString(String str) {\r\n        if (str.length() > 24) {\r\n            throw new IllegalArgumentException(\"Input string with prefix `\"\r\n                + str.substring(0, 24) + \"` is too long to be decoded as a base64 UUID\");\r\n        }\r\n\r\n        ByteBuffer uuidBytes = ByteBuffer.wrap(Base64.getUrlDecoder().decode(str));\r\n        if (uuidBytes.remaining() != 16) {\r\n            throw new IllegalArgumentException(\"Input string `\" + str + \"` decoded as \"\r\n                + uuidBytes.remaining() + \" bytes, which is not equal to the expected 16 bytes \"\r\n                + \"of a base64-encoded UUID\");\r\n        }\r\n\r\n        return new Uuid(uuidBytes.getLong(), uuidBytes.getLong());\r\n} {code}\r\n{code:java}\r\npublic String toString() {\r\n        return Base64.getUrlEncoder().withoutPadding().encodeToString(getBytesFromUuid());\r\n} {code}\r\n{{I feel that Uuid should use normal Base64 encoding or decoding instead of Url one. If we use Url encoding and decoding then the Uuid itself changes. Generated Uuid and Url Base64 encoding for the Uuid will be different.\u00a0}}{{}}"}}}
{"project": "KAFKA", "issue_key": "KAFKA-19845", "title": "Share consumer changes to support renew ack.", "status": "Open", "reporter": "Sushant Mahajan", "assignee": "Andrew Schofield", "priority": "Major", "labels": [], "created": "2025-10-29T12:49:27.000+0000", "updated": "2025-10-29T15:41:31.000+0000", "description": null, "comments": [], "derived_tasks": {"summarization": "Share consumer changes to support renew ack.", "classification": "feature", "qna": {"question": "What is the issue 'Share consumer changes to support renew ack.' about?", "answer": null}}}
{"project": "KAFKA", "issue_key": "KAFKA-19844", "title": "Modification of share fetch to accommodate piggybacked renewals.", "status": "Open", "reporter": "Sushant Mahajan", "assignee": "Sushant Mahajan", "priority": "Major", "labels": [], "created": "2025-10-29T12:48:45.000+0000", "updated": "2025-10-29T12:48:45.000+0000", "description": null, "comments": [], "derived_tasks": {"summarization": "Modification of share fetch to accommodate piggybacked renewals.", "classification": "feature", "qna": {"question": "What is the issue 'Modification of share fetch to accommodate piggybacked renewals.' about?", "answer": null}}}
{"project": "KAFKA", "issue_key": "KAFKA-19843", "title": "SharePartition changes to support renew ack.", "status": "Open", "reporter": "Sushant Mahajan", "assignee": "Sushant Mahajan", "priority": "Major", "labels": [], "created": "2025-10-29T12:47:48.000+0000", "updated": "2025-10-29T12:47:48.000+0000", "description": null, "comments": [], "derived_tasks": {"summarization": "SharePartition changes to support renew ack.", "classification": "feature", "qna": {"question": "What is the issue 'SharePartition changes to support renew ack.' about?", "answer": null}}}
{"project": "KAFKA", "issue_key": "KAFKA-19842", "title": "Additional tests in ShareConsumerTest", "status": "Open", "reporter": "Sushant Mahajan", "assignee": "Sushant Mahajan", "priority": "Major", "labels": [], "created": "2025-10-29T12:47:01.000+0000", "updated": "2025-10-29T12:47:01.000+0000", "description": null, "comments": [], "derived_tasks": {"summarization": "Additional tests in ShareConsumerTest", "classification": "feature", "qna": {"question": "What is the issue 'Additional tests in ShareConsumerTest' about?", "answer": null}}}
{"project": "KAFKA", "issue_key": "KAFKA-19841", "title": "Fix the negative remote bytesLag and segmentsLag metric", "status": "Resolved", "reporter": "Kamal Chandraprakash", "assignee": "Kamal Chandraprakash", "priority": "Minor", "labels": [], "created": "2025-10-29T08:36:51.000+0000", "updated": "2025-10-29T09:11:55.000+0000", "description": "* Each broker rotates the segment independently.\r\n * When the leadership changes to the next replica, then the highest offset uploaded to the remote storage might already be higher than the baseOffset of the current leader active segment.\r\n * During this case, the {{onlyLocalLogSegmentsSize}}\u00a0and\u00a0{{onlyLocalLogSegmentsCount}}\u00a0returns 0, so the metrics shows negative value (0 - activeSegment.size).", "comments": [], "derived_tasks": {"summarization": "Fix the negative remote bytesLag and segmentsLag metric", "classification": "feature", "qna": {"question": "What is the issue 'Fix the negative remote bytesLag and segmentsLag metric' about?", "answer": "* Each broker rotates the segment independently.\r\n * When the leadership changes to the next replica, then the highest offset uploaded to the remote storage might already be higher than the baseOffset of the current leader active segment.\r\n * During this case, the {{onlyLocalLogSegmentsSize}}\u00a0and\u00a0{{onlyLocalLogSegmentsCount}}\u00a0returns 0, so the metrics shows negative value (0 - activeSegment.size)."}}}
{"project": "KAFKA", "issue_key": "KAFKA-19840", "title": "Flaky test ShareConsumerTest. testShareGroupMaxSizeConfigExceeded", "status": "Open", "reporter": "Sanskar Jhajharia", "assignee": "Andrew Schofield", "priority": "Major", "labels": [], "created": "2025-10-28T16:15:28.000+0000", "updated": "2025-10-28T16:45:05.000+0000", "description": "The test has been observed as flaky in the recent runs: [https://develocity.apache.org/scans/tests?search.names=CI%20workflow%2CGit%20repository&search.relativeStartTime=P28D&search.rootProjectNames=kafka&search.tags=github%2Ctrunk&search.tasks=test&search.timeZoneId=Asia%2FCalcutta&search.values=CI%2Chttps:%2F%2Fgithub.com%2Fapache%2Fkafka&tests.container=org.apache.kafka.clients.consumer.ShareConsumerTest&tests.sortField=FLAKY&tests.test=testShareGroupMaxSizeConfigExceeded()%5B1%5D]\u00a0\r\n\r\n\u00a0\r\nAfter tracing the failures, the root cause was narrowed down to commit\u00a0[https://github.com/apache/kafka/commit/87657fdfc721055835f5b1f22151c461e85eab4a]. This change introduced a new try/catch around\u00a0{{handleCompletedAcknowledgements()}}\u00a0in\u00a0{{{}ShareConsumerImpl.java{}}}. The side effect is that all exceptions coming from the commit callback \u2014 including GroupMaxSizeReachedException \u2014 are now swallowed and only logged, preventing the test from ever receiving the exception.\r\n - If the ack callback fires while {{poll()}}\u00a0is executing \u2192 exception is caught & swallowed \u2192 test times out\r\n - If the callback fires outside that path \u2192 exception escapes \u2192 test passes\r\n\r\nSo the same test randomly passes/fails depending on scheduling of the ack callback.\r\n\r\nAlso, the test testAcknowledgementCommitCallbackThrowsException has been marked as Flaky for the same Root cause.", "comments": [], "derived_tasks": {"summarization": "Flaky test ShareConsumerTest. testShareGroupMaxSizeConfigExceeded", "classification": "feature", "qna": {"question": "What is the issue 'Flaky test ShareConsumerTest. testShareGroupMaxSizeConfigExceeded' about?", "answer": "The test has been observed as flaky in the recent runs: [https://develocity.apache.org/scans/tests?search.names=CI%20workflow%2CGit%20repository&search.relativeStartTime=P28D&search.rootProjectNames=kafka&search.tags=github%2Ctrunk&search.tasks=test&search.timeZoneId=Asia%2FCalcutta&search.values=CI%2Chttps:%2F%2Fgithub.com%2Fapache%2Fkafka&tests.container=org.apache.kafka.clients.consumer.ShareConsumerTest&tests.sortField=FLAKY&tests.test=testShareGroupMaxSizeConfigExceeded()%5B1%5D]\u00a0\r\n\r\n\u00a0\r\nAfter tracing the failures, the root cause was narrowed down to commit\u00a0[https://github.com/apache/kafka/commit/87657fdfc721055835f5b1f22151c461e85eab4a]. This change introduced a new try/catch around\u00a0{{handleCompletedAcknowledgements()}}\u00a0in\u00a0{{{}ShareConsumerImpl.java{}}}. The side effect is that all exceptions coming from the commit callback \u2014 including GroupMaxSizeReachedException \u2014 are now swallowed and only logged, preventing the test from ever receiving the exception.\r\n - If the ack callback fires while {{poll()}}\u00a0is executing \u2192 exception is caught & swallowed \u2192 test times out\r\n - If the callback fires outside that path \u2192 exception escapes \u2192 test passes\r\n\r\nSo the same test randomly passes/fails depending on scheduling of the ack callback.\r\n\r\nAlso, the test testAcknowledgementCommitCallbackThrowsException has been marked as Flaky for the same Root cause."}}}
{"project": "KAFKA", "issue_key": "KAFKA-19839", "title": "Native-image (dockerimage) does not work with compression.type=zstd", "status": "Open", "reporter": "Morten B\u00f8geskov", "assignee": null, "priority": "Minor", "labels": [], "created": "2025-10-28T07:59:52.000+0000", "updated": "2025-10-28T08:27:04.000+0000", "description": "When sending records to a topic created like this:\r\n{quote}\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 admin.createTopics(List.of(new NewTopic(\"foo\", 24, (short) 1)\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .configs(Map.of(TopicConfig.COMPRESSION_TYPE_CONFIG,\"zstd\"))));\r\n{quote}\r\nWith a producer configured with:\r\n{quote}producerProps.put(ProducerConfig.COMPRESSION_TYPE_CONFIG, \"zstd\");\r\n{quote}\r\nThe server fails with:\r\n{quote}[TIMESTAMP] ERROR [ReplicaManager broker=1] Error processing append operation on partition Z6OCnSwIS9KVFCN2gLaxnQ:foo-1 (kafka.server.ReplicaManager)\r\njava.lang.NoSuchFieldError: com.github.luben.zstd.ZstdOutputStreamNoFinalizer.dstPos\r\nat org.graalvm.nativeimage.builder/com.oracle.svm.core.jni.functions.JNIFunctions$Support.getFieldID(JNIFunctions.java:1357) ~[?:?]\r\nat org.graalvm.nativeimage.builder/com.oracle.svm.core.jni.functions.JNIFunctions.GetFieldID(JNIFunctions.java:449) ~[?:?]\r\nat com.github.luben.zstd.ZstdOutputStreamNoFinalizer.resetCStream(Native Method) ~[?:?]\r\nat com.github.luben.zstd.ZstdOutputStreamNoFinalizer.close(ZstdOutputStreamNoFinalizer.java:423) ~[?:?]\r\nat com.github.luben.zstd.ZstdOutputStreamNoFinalizer.close(ZstdOutputStreamNoFinalizer.java:405) ~[?:?]\r\n...\r\nat org.graalvm.nativeimage.builder/com.oracle.svm.core.posix.thread.PosixPlatformThreads.pthreadStartRoutine(PosixPlatformThreads.java:211) [kafka.Kafka:?]\r\nSuppressed: java.lang.NoSuchFieldError: com.github.luben.zstd.ZstdOutputStreamNoFinalizer.dstPos\r\nat org.graalvm.nativeimage.builder/com.oracle.svm.core.jni.functions.JNIFunctions$Support.getFieldID(JNIFunctions.java:1357) ~[?:?]\r\nat org.graalvm.nativeimage.builder/com.oracle.svm.core.jni.functions.JNIFunctions.GetFieldID(JNIFunctions.java:449) ~[?:?]\r\nat com.github.luben.zstd.ZstdOutputStreamNoFinalizer.resetCStream(Native Method) ~[?:?]\r\n...\r\nat org.graalvm.nativeimage.builder/com.oracle.svm.core.posix.thread.PosixPlatformThreads.pthreadStartRoutine(PosixPlatformThreads.java:211) [kafka.Kafka:?]\r\nSuppressed: java.lang.NoSuchFieldError: com.github.luben.zstd.ZstdOutputStreamNoFinalizer.dstPos\r\nat org.graalvm.nativeimage.builder/com.oracle.svm.core.jni.functions.JNIFunctions$Support.getFieldID(JNIFunctions.java:1357) ~[?:?]\r\nat org.graalvm.nativeimage.builder/com.oracle.svm.core.jni.functions.JNIFunctions.GetFieldID(JNIFunctions.java:449) ~[?:?]\r\nat com.github.luben.zstd.ZstdOutputStreamNoFinalizer.resetCStream(Native Method) ~[?:?]\r\n{quote}\r\nThe file: {{docker/native/native-image-configs/jni-config.json}} seems to have it referred:\r\n{quote}{\r\n\u00a0 \"name\":\"com.github.luben.zstd.ZstdInputStreamNoFinalizer\",\r\n\u00a0 \"fields\":[\\{ \"name\":\"dstPos\"}, \\{ \"name\":\"srcPos\"}]\r\n},{quote}", "comments": [], "derived_tasks": {"summarization": "Native-image (dockerimage) does not work with compression.type=zstd", "classification": "feature", "qna": {"question": "What is the issue 'Native-image (dockerimage) does not work with compression.type=zstd' about?", "answer": "When sending records to a topic created like this:\r\n{quote}\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 admin.createTopics(List.of(new NewTopic(\"foo\", 24, (short) 1)\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .configs(Map.of(TopicConfig.COMPRESSION_TYPE_CONFIG,\"zstd\"))));\r\n{quote}\r\nWith a producer configured with:\r\n{quote}producerProps.put(ProducerConfig.COMPRESSION_TYPE_CONFIG, \"zstd\");\r\n{quote}\r\nThe server fails with:\r\n{quote}[TIMESTAMP] ERROR [ReplicaManager broker=1] Error processing append operation on partition Z6OCnSwIS9KVFCN2gLaxnQ:foo-1 (kafka.server.ReplicaManager)\r\njava.lang.NoSuchFieldError: com.github.luben.zstd.ZstdOutputStreamNoFinalizer.dstPos\r\nat org.graalvm.nativeimage.builder/com.oracle.svm.core.jni.functions.JNIFunctions$Support.getFieldID(JNIFunctions.java:1357) ~[?:?]\r\nat org.graalvm.nativeimage.builder/com.oracle.svm.core.jni.functions.JNIFunctions.GetFieldID(JNIFunctions.java:449) ~[?:?]\r\nat com.github.luben.zstd.ZstdOutputStreamNoFinalizer.resetCStream(Native Method) ~[?:?]\r\nat com.github.luben.zstd.ZstdOutputStreamNoFinalizer.close(ZstdOutputStreamNoFinalizer.java:423) ~[?:?]\r\nat com.github.luben.zstd.ZstdOutputStreamNoFinalizer.close(ZstdOutputStreamNoFinalizer.java:405) ~[?:?]\r\n...\r\nat org.graalvm.nativeimage.builder/com.oracle.svm.core.posix.thread.PosixPlatformThreads.pthreadStartRoutine(PosixPlatformThreads.java:211) [kafka.Kafka:?]\r\nSuppressed: java.lang.NoSuchFieldError: com.github.luben.zstd.ZstdOutputStreamNoFinalizer.dstPos\r\nat org.graalvm.nativeimage.builder/com.oracle.svm.core.jni.functions.JNIFunctions$Support.getFieldID(JNIFunctions.java:1357) ~[?:?]\r\nat org.graalvm.nativeimage.builder/com.oracle.svm.core.jni.functions.JNIFunctions.GetFieldID(JNIFunctions.java:449) ~[?:?]\r\nat com.github.luben.zstd.ZstdOutputStreamNoFinalizer.resetCStream(Native Method) ~[?:?]\r\n...\r\nat org.graalvm.nativeimage.builder/com.oracle.svm.core.posix.thread.PosixPlatformThreads.pthreadStartRoutine(PosixPlatformThreads.java:211) [kafka.Kafka:?]\r\nSuppressed: java.lang.NoSuchFieldError: com.github.luben.zstd.ZstdOutputStreamNoFinalizer.dstPos\r\nat org.graalvm.nativeimage.builder/com.oracle.svm.core.jni.functions.JNIFunctions$Support.getFieldID(JNIFunctions.java:1357) ~[?:?]\r\nat org.graalvm.nativeimage.builder/com.oracle.svm.core.jni.functions.JNIFunctions.GetFieldID(JNIFunctions.java:449) ~[?:?]\r\nat com.github.luben.zstd.ZstdOutputStreamNoFinalizer.resetCStream(Native Method) ~[?:?]\r\n{quote}\r\nThe file: {{docker/native/native-image-configs/jni-config.json}} seems to have it referred:\r\n{quote}{\r\n\u00a0 \"name\":\"com.github.luben.zstd.ZstdInputStreamNoFinalizer\",\r\n\u00a0 \"fields\":[\\{ \"name\":\"dstPos\"}, \\{ \"name\":\"srcPos\"}]\r\n},{quote}"}}}
{"project": "KAFKA", "issue_key": "KAFKA-19838", "title": "Follow up for KIP-1217", "status": "Open", "reporter": "Mickael Maison", "assignee": "Mickael Maison", "priority": "Major", "labels": [], "created": "2025-10-27T13:38:37.000+0000", "updated": "2025-10-27T13:41:39.000+0000", "description": "[KIP-1217|https://cwiki.apache.org/confluence/x/6QnxFg] deprecates the ClientTelemetryReceiver and ClientTelemetry interfaces. They should be removed in Kafka 5.0.0.", "comments": [], "derived_tasks": {"summarization": "Follow up for KIP-1217", "classification": "feature", "qna": {"question": "What is the issue 'Follow up for KIP-1217' about?", "answer": "[KIP-1217|https://cwiki.apache.org/confluence/x/6QnxFg] deprecates the ClientTelemetryReceiver and ClientTelemetry interfaces. They should be removed in Kafka 5.0.0."}}}
{"project": "KAFKA", "issue_key": "KAFKA-19837", "title": "Follow up for KIP-1188", "status": "Open", "reporter": "Mickael Maison", "assignee": "Mickael Maison", "priority": "Major", "labels": [], "created": "2025-10-27T13:37:10.000+0000", "updated": "2025-10-27T13:39:57.000+0000", "description": "[KIP-1188|https://cwiki.apache.org/confluence/x/2IkvFg] deprecated the Principal connector client override policy. This policy should be removed in 5.0.0. Also the default policy should switch to Allowlist in 5.0.0.", "comments": [], "derived_tasks": {"summarization": "Follow up for KIP-1188", "classification": "feature", "qna": {"question": "What is the issue 'Follow up for KIP-1188' about?", "answer": "[KIP-1188|https://cwiki.apache.org/confluence/x/2IkvFg] deprecated the Principal connector client override policy. This policy should be removed in 5.0.0. Also the default policy should switch to Allowlist in 5.0.0."}}}
{"project": "KAFKA", "issue_key": "KAFKA-19836", "title": "Decouple ConsumerConfig and ShareConsumerConfig", "status": "Resolved", "reporter": "TaiJuWu", "assignee": "TaiJuWu", "priority": "Major", "labels": [], "created": "2025-10-26T07:31:58.000+0000", "updated": "2025-10-26T11:19:09.000+0000", "description": "ShareConsumerConfig and ConsumerConfig are inherent at the moment.\r\nThe drawback is the config logic is mixed, for example:\u00a0\r\nShareAcknowledgementMode.\r\nWe can decouple to prevent the logic is complicated in the future.", "comments": [], "derived_tasks": {"summarization": "Decouple ConsumerConfig and ShareConsumerConfig", "classification": "feature", "qna": {"question": "What is the issue 'Decouple ConsumerConfig and ShareConsumerConfig' about?", "answer": "ShareConsumerConfig and ConsumerConfig are inherent at the moment.\r\nThe drawback is the config logic is mixed, for example:\u00a0\r\nShareAcknowledgementMode.\r\nWe can decouple to prevent the logic is complicated in the future."}}}
{"project": "KAFKA", "issue_key": "KAFKA-19835", "title": "The Content-Security-Policy header must not be overridden", "status": "Open", "reporter": "Sebb", "assignee": "Kuan Po Tseng", "priority": "Major", "labels": [], "created": "2025-10-25T20:24:10.000+0000", "updated": "2025-10-28T23:10:19.000+0000", "description": "[https://github.com/apache/kafka-site/blob/905299a0b7e2e3892d9493c7dcaaf78dce035c00/.htaccess#L13]\r\n\r\nThe Content-Security-Policy header must not be overridden.\r\n\r\nThere is now a standard way to add local exceptions to the CSP:\r\n\r\n[https://infra.apache.org/tools/csp.html]\r\n\r\nPlease update the .htaccess file accordingly.\r\n\r\nPlease note that the policy says that any exceptions must have been approved, and a reference to the approval must be commented in the .htaccess file\r\n\r\n\u00a0", "comments": [], "derived_tasks": {"summarization": "The Content-Security-Policy header must not be overridden", "classification": "feature", "qna": {"question": "What is the issue 'The Content-Security-Policy header must not be overridden' about?", "answer": "[https://github.com/apache/kafka-site/blob/905299a0b7e2e3892d9493c7dcaaf78dce035c00/.htaccess#L13]\r\n\r\nThe Content-Security-Policy header must not be overridden.\r\n\r\nThere is now a standard way to add local exceptions to the CSP:\r\n\r\n[https://infra.apache.org/tools/csp.html]\r\n\r\nPlease update the .htaccess file accordingly.\r\n\r\nPlease note that the policy says that any exceptions must have been approved, and a reference to the approval must be commented in the .htaccess file\r\n\r\n\u00a0"}}}
{"project": "KAFKA", "issue_key": "KAFKA-19834", "title": "Cleanup suppressions.xml", "status": "Open", "reporter": "majialong", "assignee": "majialong", "priority": "Minor", "labels": [], "created": "2025-10-25T16:30:53.000+0000", "updated": "2025-10-25T16:30:53.000+0000", "description": "Currently the rules in suppressions.xml are a bit messy and need to be cleaned up.", "comments": [], "derived_tasks": {"summarization": "Cleanup suppressions.xml", "classification": "feature", "qna": {"question": "What is the issue 'Cleanup suppressions.xml' about?", "answer": "Currently the rules in suppressions.xml are a bit messy and need to be cleaned up."}}}
{"project": "KAFKA", "issue_key": "KAFKA-19833", "title": "Refactor Nullable Types to Use a Unified Pattern", "status": "Open", "reporter": "Lan Ding", "assignee": "Lan Ding", "priority": "Major", "labels": [], "created": "2025-10-25T03:21:54.000+0000", "updated": "2025-10-25T03:21:54.000+0000", "description": "see [https://github.com/apache/kafka/pull/20614#pullrequestreview-3379156676]\r\n\r\nRegarding the implementation of the nullable vs non-nullable types. We use 3 different approaches.\r\n # For bytes, we implement two independent classes BYTES and NULLABLE_BYTES.\r\n # For array, we use one class ArraryOf, which takes a nullable param.\r\n # For schema, we implement NULLABLE_SCHEMA as a subclass of SCHEMA.\r\n\r\nWe need\u00a0 to pick one approach to implement all nullable types in a consistent way.", "comments": [], "derived_tasks": {"summarization": "Refactor Nullable Types to Use a Unified Pattern", "classification": "feature", "qna": {"question": "What is the issue 'Refactor Nullable Types to Use a Unified Pattern' about?", "answer": "see [https://github.com/apache/kafka/pull/20614#pullrequestreview-3379156676]\r\n\r\nRegarding the implementation of the nullable vs non-nullable types. We use 3 different approaches.\r\n # For bytes, we implement two independent classes BYTES and NULLABLE_BYTES.\r\n # For array, we use one class ArraryOf, which takes a nullable param.\r\n # For schema, we implement NULLABLE_SCHEMA as a subclass of SCHEMA.\r\n\r\nWe need\u00a0 to pick one approach to implement all nullable types in a consistent way."}}}
{"project": "KAFKA", "issue_key": "KAFKA-19832", "title": "Move ClientOAuthIntegrationTest to clients-integration-tests", "status": "Open", "reporter": "Kirk True", "assignee": "Kirk True", "priority": "Major", "labels": ["integration-test", "oauth2"], "created": "2025-10-23T23:45:08.000+0000", "updated": "2025-10-23T23:45:21.000+0000", "description": "This is continue with the effort to move the clients test to {{clients-integration-tests}} a la KAFKA-19042.", "comments": [], "derived_tasks": {"summarization": "Move ClientOAuthIntegrationTest to clients-integration-tests", "classification": "feature", "qna": {"question": "What is the issue 'Move ClientOAuthIntegrationTest to clients-integration-tests' about?", "answer": "This is continue with the effort to move the clients test to {{clients-integration-tests}} a la KAFKA-19042."}}}
{"project": "KAFKA", "issue_key": "KAFKA-19831", "title": "Failures in the StateUpdater thread may lead to inability to shut down a stream thread", "status": "In Progress", "reporter": "Nikita Shupletsov", "assignee": "Nikita Shupletsov", "priority": "Major", "labels": [], "created": "2025-10-23T18:25:27.000+0000", "updated": "2025-10-24T23:16:05.000+0000", "description": "If during rebalance a failure occurs in the StateUpdater thread, and this failure leads to the thread shutdown, the Stream thread may get into an infinite wait state during it's shutdown.\r\n\r\nSee the attached test that reproduces the issue.", "comments": [], "derived_tasks": {"summarization": "Failures in the StateUpdater thread may lead to inability to shut down a stream thread", "classification": "feature", "qna": {"question": "What is the issue 'Failures in the StateUpdater thread may lead to inability to shut down a stream thread' about?", "answer": "If during rebalance a failure occurs in the StateUpdater thread, and this failure leads to the thread shutdown, the Stream thread may get into an infinite wait state during it's shutdown.\r\n\r\nSee the attached test that reproduces the issue."}}}
{"project": "KAFKA", "issue_key": "KAFKA-19830", "title": "Refactor KafkaRaftClient to use event scheduler framework", "status": "Open", "reporter": "Kevin Wu", "assignee": "Kevin Wu", "priority": "Major", "labels": [], "created": "2025-10-23T15:54:07.000+0000", "updated": "2025-10-23T15:54:07.000+0000", "description": null, "comments": [], "derived_tasks": {"summarization": "Refactor KafkaRaftClient to use event scheduler framework", "classification": "feature", "qna": {"question": "What is the issue 'Refactor KafkaRaftClient to use event scheduler framework' about?", "answer": null}}}
{"project": "KAFKA", "issue_key": "KAFKA-19829", "title": "Implement group-level initial rebalance delay", "status": "Patch Available", "reporter": "travis", "assignee": "travis", "priority": "Major", "labels": [], "created": "2025-10-23T03:58:54.000+0000", "updated": "2025-10-30T18:19:20.000+0000", "description": "During testing, an artifact of the new rebalance protocol showed up. In some cases, the first joining member gets all active tasks assigned, and is slow to revoke the tasks after more member has joined the group. This affects in particular cases where the first member is slow (possibly overloaded in the case of cloudlimits benchmarks) and there are a lot of tasks to be assigned.\r\n\r\nTo help with this situation, we want to introduce a new group-specific configuration to delay the initial rebalance.", "comments": [], "derived_tasks": {"summarization": "Implement group-level initial rebalance delay", "classification": "feature", "qna": {"question": "What is the issue 'Implement group-level initial rebalance delay' about?", "answer": "During testing, an artifact of the new rebalance protocol showed up. In some cases, the first joining member gets all active tasks assigned, and is slow to revoke the tasks after more member has joined the group. This affects in particular cases where the first member is slow (possibly overloaded in the case of cloudlimits benchmarks) and there are a lot of tasks to be assigned.\r\n\r\nTo help with this situation, we want to introduce a new group-specific configuration to delay the initial rebalance."}}}
{"project": "KAFKA", "issue_key": "KAFKA-19828", "title": "Intermittent test failures when using chained emit strategy on window close", "status": "Open", "reporter": "Greg F", "assignee": null, "priority": "Major", "labels": [], "created": "2025-10-23T00:51:44.000+0000", "updated": "2025-10-30T00:11:21.000+0000", "description": "Hi,\r\n\r\nI have a test case that contains a topology with 2 time windows and chained emitStrategy calls. The standalone reproduction use case is attached to this issue\r\nThe problem is that about 25% of the time the test fails. About 75% of the time the test succeeds. I followed the conclusions of\u00a0\r\n!https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype|width=16,height=16! KAFKA-19810\r\nto make sure that I am sending events at the correct times (at least I think my calculations are correct). I really need to get to the bottom of why the test fails intermittently, as the test somewhat reflects a real life topology of a project I am working on, and we cannot have an intermittently failing test in our build pipeline.\u00a0\r\nGreg\r\n\u00a0", "comments": [], "derived_tasks": {"summarization": "Intermittent test failures when using chained emit strategy on window close", "classification": "feature", "qna": {"question": "What is the issue 'Intermittent test failures when using chained emit strategy on window close' about?", "answer": "Hi,\r\n\r\nI have a test case that contains a topology with 2 time windows and chained emitStrategy calls. The standalone reproduction use case is attached to this issue\r\nThe problem is that about 25% of the time the test fails. About 75% of the time the test succeeds. I followed the conclusions of\u00a0\r\n!https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21133&avatarType=issuetype|width=16,height=16! KAFKA-19810\r\nto make sure that I am sending events at the correct times (at least I think my calculations are correct). I really need to get to the bottom of why the test fails intermittently, as the test somewhat reflects a real life topology of a project I am working on, and we cannot have an intermittently failing test in our build pipeline.\u00a0\r\nGreg\r\n\u00a0"}}}
{"project": "KAFKA", "issue_key": "KAFKA-19827", "title": "Call acknowledgement commit callback at end of waiting calls", "status": "Resolved", "reporter": "Andrew Schofield", "assignee": "Andrew Schofield", "priority": "Major", "labels": [], "created": "2025-10-22T14:53:09.000+0000", "updated": "2025-10-28T14:45:21.000+0000", "description": "The acknowledgement commit callback in the share consumer gets called on the application thread at the start of the poll, commitSync and commitAsync methods. Specifically in the peculiar case of using the callback together with commitSync, the acknowledgement callback for the committed records is called at the start of the next eligible call, even though the information is already known at the end of the commitSync's execution. The results are correct already, but the timing could be improved in some situations.", "comments": [], "derived_tasks": {"summarization": "Call acknowledgement commit callback at end of waiting calls", "classification": "feature", "qna": {"question": "What is the issue 'Call acknowledgement commit callback at end of waiting calls' about?", "answer": "The acknowledgement commit callback in the share consumer gets called on the application thread at the start of the poll, commitSync and commitAsync methods. Specifically in the peculiar case of using the callback together with commitSync, the acknowledgement callback for the committed records is called at the start of the next eligible call, even though the information is already known at the end of the commitSync's execution. The results are correct already, but the timing could be improved in some situations."}}}
{"project": "KAFKA", "issue_key": "KAFKA-19826", "title": "KIP-1224: Implement adaptive append.linger.ms for the group coordinator and share coordinator", "status": "Open", "reporter": "Sean Quah", "assignee": "Sean Quah", "priority": "Minor", "labels": [], "created": "2025-10-22T13:48:59.000+0000", "updated": "2025-10-28T19:55:28.000+0000", "description": "Add a new allowed value for group.coordinator.append.linger.ms and share.coordinator.append.linger.ms of -1.\r\nWhen append.linger.ms is set to -1, use the flush strategy outlined in the KIP.", "comments": [], "derived_tasks": {"summarization": "KIP-1224: Implement adaptive append.linger.ms for the group coordinator and share coordinator", "classification": "feature", "qna": {"question": "What is the issue 'KIP-1224: Implement adaptive append.linger.ms for the group coordinator and share coordinator' about?", "answer": "Add a new allowed value for group.coordinator.append.linger.ms and share.coordinator.append.linger.ms of -1.\r\nWhen append.linger.ms is set to -1, use the flush strategy outlined in the KIP."}}}
{"project": "KAFKA", "issue_key": "KAFKA-19825", "title": "KIP-1224: Add batch-linger-time and batch-flush-time metrics", "status": "Open", "reporter": "Sean Quah", "assignee": "Sean Quah", "priority": "Minor", "labels": [], "created": "2025-10-22T13:47:41.000+0000", "updated": "2025-10-22T18:56:33.000+0000", "description": null, "comments": [], "derived_tasks": {"summarization": "KIP-1224: Add batch-linger-time and batch-flush-time metrics", "classification": "feature", "qna": {"question": "What is the issue 'KIP-1224: Add batch-linger-time and batch-flush-time metrics' about?", "answer": null}}}
{"project": "KAFKA", "issue_key": "KAFKA-19824", "title": "New ConnectorClientConfigOverridePolicy with allowlist of configurations", "status": "Resolved", "reporter": "Mickael Maison", "assignee": "Mickael Maison", "priority": "Major", "labels": [], "created": "2025-10-22T12:25:12.000+0000", "updated": "2025-10-30T08:53:09.000+0000", "description": "Jira for KIP-1188: https://cwiki.apache.org/confluence/x/2IkvFg", "comments": [], "derived_tasks": {"summarization": "New ConnectorClientConfigOverridePolicy with allowlist of configurations", "classification": "feature", "qna": {"question": "What is the issue 'New ConnectorClientConfigOverridePolicy with allowlist of configurations' about?", "answer": "Jira for KIP-1188: https://cwiki.apache.org/confluence/x/2IkvFg"}}}
{"project": "KAFKA", "issue_key": "KAFKA-19823", "title": "PartitionMaxBytesStrategy bug when request bytes is lesser than acquired topic partitions", "status": "Resolved", "reporter": "Abhinav Dixit", "assignee": "Abhinav Dixit", "priority": "Major", "labels": [], "created": "2025-10-22T10:55:44.000+0000", "updated": "2025-10-27T21:41:42.000+0000", "description": "There is a bug in the broker logic of splitting bytes in {{PartitionMaxBytesStrategy.java}}\u00a0due to which we are setting\u00a0{{partitionMaxBytes}} as 0 in case requestMaxBytes is lesser than\u00a0\r\nacquiredPartitionsSize", "comments": [], "derived_tasks": {"summarization": "PartitionMaxBytesStrategy bug when request bytes is lesser than acquired topic partitions", "classification": "feature", "qna": {"question": "What is the issue 'PartitionMaxBytesStrategy bug when request bytes is lesser than acquired topic partitions' about?", "answer": "There is a bug in the broker logic of splitting bytes in {{PartitionMaxBytesStrategy.java}}\u00a0due to which we are setting\u00a0{{partitionMaxBytes}} as 0 in case requestMaxBytes is lesser than\u00a0\r\nacquiredPartitionsSize"}}}
{"project": "KAFKA", "issue_key": "KAFKA-19822", "title": "Remove all static classes in Field except TaggedFieldsSection", "status": "Open", "reporter": "Lan Ding", "assignee": "Lan Ding", "priority": "Major", "labels": [], "created": "2025-10-22T10:05:17.000+0000", "updated": "2025-11-01T08:55:53.000+0000", "description": "All static classes in Field except TaggedFieldsSection are not really being used. We should remove them.", "comments": [], "derived_tasks": {"summarization": "Remove all static classes in Field except TaggedFieldsSection", "classification": "feature", "qna": {"question": "What is the issue 'Remove all static classes in Field except TaggedFieldsSection' about?", "answer": "All static classes in Field except TaggedFieldsSection are not really being used. We should remove them."}}}
{"project": "KAFKA", "issue_key": "KAFKA-19821", "title": "Duplicated batches should be logged", "status": "Resolved", "reporter": "Luke Chen", "assignee": "Kuan Po Tseng", "priority": "Major", "labels": [], "created": "2025-10-22T07:43:55.000+0000", "updated": "2025-10-22T15:07:52.000+0000", "description": "When writing records with idempotent producer, the broker can help de-duplicate records. But when records being de-duplicated, there is no log in the broker side. This will confuse the producer because the records sent going to nowhere. Sometimes it's caused by the buggy producer that keeps using the wrong sequence number to cause the duplicated records. We should at least log the duplicated records info instead of pretending nothing happened.", "comments": [], "derived_tasks": {"summarization": "Duplicated batches should be logged", "classification": "feature", "qna": {"question": "What is the issue 'Duplicated batches should be logged' about?", "answer": "When writing records with idempotent producer, the broker can help de-duplicate records. But when records being de-duplicated, there is no log in the broker side. This will confuse the producer because the records sent going to nowhere. Sometimes it's caused by the buggy producer that keeps using the wrong sequence number to cause the duplicated records. We should at least log the duplicated records info instead of pretending nothing happened."}}}
{"project": "KAFKA", "issue_key": "KAFKA-19820", "title": "remove the unnecessary copy from AbstractFetch#fetchablePartitions ", "status": "Resolved", "reporter": "Chia-Ping Tsai", "assignee": "TaiJuWu", "priority": "Minor", "labels": [], "created": "2025-10-21T13:32:11.000+0000", "updated": "2025-10-29T13:21:04.000+0000", "description": "see https://github.com/apache/kafka/pull/14359#issuecomment-3372367774", "comments": [], "derived_tasks": {"summarization": "remove the unnecessary copy from AbstractFetch#fetchablePartitions ", "classification": "feature", "qna": {"question": "What is the issue 'remove the unnecessary copy from AbstractFetch#fetchablePartitions ' about?", "answer": "see https://github.com/apache/kafka/pull/14359#issuecomment-3372367774"}}}
{"project": "KAFKA", "issue_key": "KAFKA-19819", "title": "Move BrokerMetadataPublisher to metadata module", "status": "Open", "reporter": "Jimmy Wang", "assignee": "Jimmy Wang", "priority": "Major", "labels": [], "created": "2025-10-21T09:12:15.000+0000", "updated": "2025-10-21T09:12:15.000+0000", "description": null, "comments": [], "derived_tasks": {"summarization": "Move BrokerMetadataPublisher to metadata module", "classification": "feature", "qna": {"question": "What is the issue 'Move BrokerMetadataPublisher to metadata module' about?", "answer": null}}}
{"project": "KAFKA", "issue_key": "KAFKA-19818", "title": "Move ClientQuotaMetadataManager to metadata module", "status": "Open", "reporter": "Jimmy Wang", "assignee": "Jimmy Wang", "priority": "Major", "labels": [], "created": "2025-10-21T09:08:07.000+0000", "updated": "2025-10-21T09:08:07.000+0000", "description": null, "comments": [], "derived_tasks": {"summarization": "Move ClientQuotaMetadataManager to metadata module", "classification": "feature", "qna": {"question": "What is the issue 'Move ClientQuotaMetadataManager to metadata module' about?", "answer": null}}}
{"project": "KAFKA", "issue_key": "KAFKA-19817", "title": "Move DynamicTopicClusterQuotaPublisher to metadata module", "status": "In Progress", "reporter": "Jimmy Wang", "assignee": "Jimmy Wang", "priority": "Major", "labels": [], "created": "2025-10-21T08:55:30.000+0000", "updated": "2025-10-31T15:33:26.000+0000", "description": null, "comments": [], "derived_tasks": {"summarization": "Move DynamicTopicClusterQuotaPublisher to metadata module", "classification": "feature", "qna": {"question": "What is the issue 'Move DynamicTopicClusterQuotaPublisher to metadata module' about?", "answer": null}}}
{"project": "KAFKA", "issue_key": "KAFKA-19816", "title": "Resolve flaky test: org.apache.kafka.streams.integration.EosIntegrationTest.shouldNotViolateEosIfOneTaskFails", "status": "Open", "reporter": "Evan Zhou", "assignee": "Lucas Brutschy", "priority": "Minor", "labels": [], "created": "2025-10-20T18:27:06.000+0000", "updated": "2025-10-22T17:06:49.000+0000", "description": "EosIntegrationTest.shouldNotViolateEosIfOneTaskFails is currently flaky for tests run with the \"streams\" group protocol.\u00a0", "comments": [], "derived_tasks": {"summarization": "Resolve flaky test: org.apache.kafka.streams.integration.EosIntegrationTest.shouldNotViolateEosIfOneTaskFails", "classification": "feature", "qna": {"question": "What is the issue 'Resolve flaky test: org.apache.kafka.streams.integration.EosIntegrationTest.shouldNotViolateEosIfOneTaskFails' about?", "answer": "EosIntegrationTest.shouldNotViolateEosIfOneTaskFails is currently flaky for tests run with the \"streams\" group protocol.\u00a0"}}}
{"project": "KAFKA", "issue_key": "KAFKA-19815", "title": "Implementation of ShareConsumer.acquisitionLockTimeoutMs() method", "status": "Open", "reporter": "Andrew Schofield", "assignee": "Andrew Schofield", "priority": "Major", "labels": [], "created": "2025-10-20T12:00:17.000+0000", "updated": "2025-10-20T12:00:17.000+0000", "description": null, "comments": [], "derived_tasks": {"summarization": "Implementation of ShareConsumer.acquisitionLockTimeoutMs() method", "classification": "feature", "qna": {"question": "What is the issue 'Implementation of ShareConsumer.acquisitionLockTimeoutMs() method' about?", "answer": null}}}
{"project": "KAFKA", "issue_key": "KAFKA-19814", "title": "Protocol schema and public API changes", "status": "In Progress", "reporter": "Andrew Schofield", "assignee": "Andrew Schofield", "priority": "Major", "labels": [], "created": "2025-10-20T11:58:18.000+0000", "updated": "2025-10-30T00:35:26.000+0000", "description": "This just adds the RPC schema and public API changes to let the implementation progress with multiple teams.", "comments": [], "derived_tasks": {"summarization": "Protocol schema and public API changes", "classification": "feature", "qna": {"question": "What is the issue 'Protocol schema and public API changes' about?", "answer": "This just adds the RPC schema and public API changes to let the implementation progress with multiple teams."}}}
{"project": "KAFKA", "issue_key": "KAFKA-19813", "title": "Incorrect jitter value in StreamsGroupHeartbeatRequestManager and AbstractHeartbeatRequestManager", "status": "Open", "reporter": "Kuan Po Tseng", "assignee": "Kuan Po Tseng", "priority": "Major", "labels": [], "created": "2025-10-20T11:28:24.000+0000", "updated": "2025-10-31T16:13:14.000+0000", "description": "see https://github.com/apache/kafka/pull/14873#discussion_r2442794887, \r\n\r\nIn below code snippets\r\nhttps://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/clients/consumer/internals/AbstractHeartbeatRequestManager.java#L116\r\nhttps://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/clients/consumer/internals/StreamsGroupHeartbeatRequestManager.java#L333\r\n\r\nwe use max.poll.interval.ms (default 300000) as the jitter value. Although RequestState.remainingBackoffMs guards with Math.max(0, \u2026), which won\u2019t make the value become negative, using a jitter that isn\u2019t in (0\u20131) is unexpected.\r\n\r\nIn addition, we should validate that ExponentialBackoff receives a jitter strictly within (0, 1) to prevent this scenario in the future.", "comments": [], "derived_tasks": {"summarization": "Incorrect jitter value in StreamsGroupHeartbeatRequestManager and AbstractHeartbeatRequestManager", "classification": "feature", "qna": {"question": "What is the issue 'Incorrect jitter value in StreamsGroupHeartbeatRequestManager and AbstractHeartbeatRequestManager' about?", "answer": "see https://github.com/apache/kafka/pull/14873#discussion_r2442794887, \r\n\r\nIn below code snippets\r\nhttps://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/clients/consumer/internals/AbstractHeartbeatRequestManager.java#L116\r\nhttps://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/clients/consumer/internals/StreamsGroupHeartbeatRequestManager.java#L333\r\n\r\nwe use max.poll.interval.ms (default 300000) as the jitter value. Although RequestState.remainingBackoffMs guards with Math.max(0, \u2026), which won\u2019t make the value become negative, using a jitter that isn\u2019t in (0\u20131) is unexpected.\r\n\r\nIn addition, we should validate that ExponentialBackoff receives a jitter strictly within (0, 1) to prevent this scenario in the future."}}}
{"project": "KAFKA", "issue_key": "KAFKA-19812", "title": "Unbound Error Thrown if some variables are not set for SASL/SSL configuration", "status": "Open", "reporter": "Manas Poddar", "assignee": null, "priority": "Minor", "labels": [], "created": "2025-10-19T14:13:48.000+0000", "updated": "2025-10-25T19:40:27.000+0000", "description": "I was trying to set up a {*}Kafka container with SASL_SSL configuration{*}, but I {*}missed setting up some environment variables{*}, and {*}Kafka kept exiting with an unbound variable error{*}. I checked the *{{/etc/kafka/docker/configure}}* file \u2014 it has an *{{ensure}} function* to check and instruct the user when a particular environment variable is not set. I also {*}checked the Docker history and the parent running file{*}, but they {*}gave no clue about running the {{.sh}} files with {{set -u}}{*}.\r\n\r\nThis issue is {*}just an enhancement to properly log the error details{*}.", "comments": [], "derived_tasks": {"summarization": "Unbound Error Thrown if some variables are not set for SASL/SSL configuration", "classification": "feature", "qna": {"question": "What is the issue 'Unbound Error Thrown if some variables are not set for SASL/SSL configuration' about?", "answer": "I was trying to set up a {*}Kafka container with SASL_SSL configuration{*}, but I {*}missed setting up some environment variables{*}, and {*}Kafka kept exiting with an unbound variable error{*}. I checked the *{{/etc/kafka/docker/configure}}* file \u2014 it has an *{{ensure}} function* to check and instruct the user when a particular environment variable is not set. I also {*}checked the Docker history and the parent running file{*}, but they {*}gave no clue about running the {{.sh}} files with {{set -u}}{*}.\r\n\r\nThis issue is {*}just an enhancement to properly log the error details{*}."}}}
{"project": "KAFKA", "issue_key": "KAFKA-19811", "title": "Acked record on new topic not immediately visible to consumer", "status": "Open", "reporter": "Erik van Oosten", "assignee": null, "priority": "Minor", "labels": [], "created": "2025-10-19T08:50:06.000+0000", "updated": "2025-10-23T13:05:53.000+0000", "description": "h2. Steps to reproduce\r\n * program uses a single broker (we see the issue with an in-JVM embedded kafka server)\r\n * create a new topic with 1 partition\r\n * produce a record with {{acks=all}}\r\n * await acknowledgement from the broker\r\n * start a consumer (configured to read from beginning of topic)\r\n * spuriously, _the consumer never sees the record_\r\n\r\nThe problem seems to occur more on a very busy server (e.g. a build server running many tests in parallel). We have not seen it on a modern laptop.\r\n\r\nA delay of 10ms after receiving the acknowledgement, and before starting the consumer, makes the record always visible.\r\n\r\nWe observe this problem in ~1 in 6 runs of [zio-kafka|https://github.com/zio/zio-kafka]'s test suite, when run from a GitHub action. Since we run the test-suite for 3 different JVM versions, more than half of the zio-kafka builds fails due to this issue.\r\nh2. Expected behavior\r\n\r\nA record, produced with {{{}acks=all{}}}, of which producing was acknowledged, should be immediately visible for all consumers.\r\n\r\n\u00a0", "comments": [], "derived_tasks": {"summarization": "Acked record on new topic not immediately visible to consumer", "classification": "feature", "qna": {"question": "What is the issue 'Acked record on new topic not immediately visible to consumer' about?", "answer": "h2. Steps to reproduce\r\n * program uses a single broker (we see the issue with an in-JVM embedded kafka server)\r\n * create a new topic with 1 partition\r\n * produce a record with {{acks=all}}\r\n * await acknowledgement from the broker\r\n * start a consumer (configured to read from beginning of topic)\r\n * spuriously, _the consumer never sees the record_\r\n\r\nThe problem seems to occur more on a very busy server (e.g. a build server running many tests in parallel). We have not seen it on a modern laptop.\r\n\r\nA delay of 10ms after receiving the acknowledgement, and before starting the consumer, makes the record always visible.\r\n\r\nWe observe this problem in ~1 in 6 runs of [zio-kafka|https://github.com/zio/zio-kafka]'s test suite, when run from a GitHub action. Since we run the test-suite for 3 different JVM versions, more than half of the zio-kafka builds fails due to this issue.\r\nh2. Expected behavior\r\n\r\nA record, produced with {{{}acks=all{}}}, of which producing was acknowledged, should be immediately visible for all consumers.\r\n\r\n\u00a0"}}}
{"project": "KAFKA", "issue_key": "KAFKA-19810", "title": "Kafka streams with chained emitStrategy(onWindowClose) example does not work", "status": "Resolved", "reporter": "Greg F", "assignee": null, "priority": "Major", "labels": [], "created": "2025-10-19T06:01:45.000+0000", "updated": "2025-10-25T22:06:00.000+0000", "description": "Hi, I got this example by using the following prompt in Google:\r\n # kafka streams unit testing with chained \"emitStrategy\"\r\n # Provide an example of testing chained suppress with different grace periods\r\n\r\n[https://gist.github.com/gregfichtenholtz-illumio/81fb537e24f7187e9de37686bb8eca7d]\r\n\r\nCompiled and ran the example using latest kafka jars only to get\r\n\r\n[ERROR] Tests run: 1, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 1.103 s <<< FAILURE! -- in com.k8sflowprocessor.ChainedEmitStrategyTopologyTest\r\n[ERROR] com.foo.bar.ChainedEmitStrategyTopologyTest.testChainedWindowedAggregationsWithDifferentGracePeriods -- Time elapsed: 1.096 s <<< FAILURE!\r\norg.opentest4j.AssertionFailedError: Final output should contain one result ==> expected: <1> but was: <0>\r\nat org.junit.jupiter.api.AssertionFailureBuilder.build(AssertionFailureBuilder.java:158)\r\nat org.junit.jupiter.api.AssertionFailureBuilder.buildAndThrow(AssertionFailureBuilder.java:139)\r\nat org.junit.jupiter.api.AssertEquals.failNotEqual(AssertEquals.java:201)\r\nat org.junit.jupiter.api.AssertEquals.assertEquals(AssertEquals.java:168)\r\nat org.junit.jupiter.api.Assertions.assertEquals(Assertions.java:694)\r\nat com.foo.bar.ChainedEmitStrategyTopologyTest.testChainedWindowedAggregationsWithDifferentGracePeriods(ChainedEmitStrategyTopologyTest.java:123)\r\n\r\nIt appears that the test is not able to drive the kafka stream to emit the 2nd event.\r\nCould be a bug in test code/test driver/kafka streams?\r\nThanks in advance\r\nGreg\r\n\u00a0", "comments": [], "derived_tasks": {"summarization": "Kafka streams with chained emitStrategy(onWindowClose) example does not work", "classification": "feature", "qna": {"question": "What is the issue 'Kafka streams with chained emitStrategy(onWindowClose) example does not work' about?", "answer": "Hi, I got this example by using the following prompt in Google:\r\n # kafka streams unit testing with chained \"emitStrategy\"\r\n # Provide an example of testing chained suppress with different grace periods\r\n\r\n[https://gist.github.com/gregfichtenholtz-illumio/81fb537e24f7187e9de37686bb8eca7d]\r\n\r\nCompiled and ran the example using latest kafka jars only to get\r\n\r\n[ERROR] Tests run: 1, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 1.103 s <<< FAILURE! -- in com.k8sflowprocessor.ChainedEmitStrategyTopologyTest\r\n[ERROR] com.foo.bar.ChainedEmitStrategyTopologyTest.testChainedWindowedAggregationsWithDifferentGracePeriods -- Time elapsed: 1.096 s <<< FAILURE!\r\norg.opentest4j.AssertionFailedError: Final output should contain one result ==> expected: <1> but was: <0>\r\nat org.junit.jupiter.api.AssertionFailureBuilder.build(AssertionFailureBuilder.java:158)\r\nat org.junit.jupiter.api.AssertionFailureBuilder.buildAndThrow(AssertionFailureBuilder.java:139)\r\nat org.junit.jupiter.api.AssertEquals.failNotEqual(AssertEquals.java:201)\r\nat org.junit.jupiter.api.AssertEquals.assertEquals(AssertEquals.java:168)\r\nat org.junit.jupiter.api.Assertions.assertEquals(Assertions.java:694)\r\nat com.foo.bar.ChainedEmitStrategyTopologyTest.testChainedWindowedAggregationsWithDifferentGracePeriods(ChainedEmitStrategyTopologyTest.java:123)\r\n\r\nIt appears that the test is not able to drive the kafka stream to emit the 2nd event.\r\nCould be a bug in test code/test driver/kafka streams?\r\nThanks in advance\r\nGreg\r\n\u00a0"}}}
{"project": "KAFKA", "issue_key": "KAFKA-19809", "title": "CheckStyle version upgrade: 10 -->> 12", "status": "Patch Available", "reporter": "Dejan Stojadinovi\u0107", "assignee": "Dejan Stojadinovi\u0107", "priority": "Minor", "labels": ["checkstyle", "update", "updates", "upgrade", "upgrades"], "created": "2025-10-18T13:16:55.000+0000", "updated": "2025-10-20T18:38:40.000+0000", "description": "*Task:* upgrade checkstyle version from *10.20.2* to *12.0.1*\r\n\r\n*Related link:* [https://checkstyle.org/releasenotes.html#Release_12.0.1]\r\n\r\n\u00a0*Note:*\u00a0difference between versions is quite big:\r\n * version *10.20.2* (published in Novemeber 2024)\r\n * version *12.0.1* (published in Octoober 2025)\r\n * git diff between versions: [https://github.com/checkstyle/checkstyle/compare/checkstyle-10.20.2...checkstyle-12.0.1]", "comments": [], "derived_tasks": {"summarization": "CheckStyle version upgrade: 10 -->> 12", "classification": "feature", "qna": {"question": "What is the issue 'CheckStyle version upgrade: 10 -->> 12' about?", "answer": "*Task:* upgrade checkstyle version from *10.20.2* to *12.0.1*\r\n\r\n*Related link:* [https://checkstyle.org/releasenotes.html#Release_12.0.1]\r\n\r\n\u00a0*Note:*\u00a0difference between versions is quite big:\r\n * version *10.20.2* (published in Novemeber 2024)\r\n * version *12.0.1* (published in Octoober 2025)\r\n * git diff between versions: [https://github.com/checkstyle/checkstyle/compare/checkstyle-10.20.2...checkstyle-12.0.1]"}}}
{"project": "KAFKA", "issue_key": "KAFKA-19808", "title": "Handle batch alignment when share partition is at capacity", "status": "Resolved", "reporter": "Apoorv Mittal", "assignee": "Apoorv Mittal", "priority": "Major", "labels": [], "created": "2025-10-17T22:40:10.000+0000", "updated": "2025-10-21T14:42:53.000+0000", "description": null, "comments": [], "derived_tasks": {"summarization": "Handle batch alignment when share partition is at capacity", "classification": "feature", "qna": {"question": "What is the issue 'Handle batch alignment when share partition is at capacity' about?", "answer": null}}}
{"project": "KAFKA", "issue_key": "KAFKA-19807", "title": "Add RPC-level integration tests for StreamsGroupHeartbeat", "status": "Open", "reporter": "Lucy Liu", "assignee": "Lucy Liu", "priority": "Major", "labels": [], "created": "2025-10-17T20:09:30.000+0000", "updated": "2025-10-30T19:46:19.000+0000", "description": "Add integration test similar to `ShareGroupHeartbeatRequestTest` and `ConsumerGroupHeartbeatRequestTest` for `StreamsGroupHeartbeat`", "comments": [], "derived_tasks": {"summarization": "Add RPC-level integration tests for StreamsGroupHeartbeat", "classification": "feature", "qna": {"question": "What is the issue 'Add RPC-level integration tests for StreamsGroupHeartbeat' about?", "answer": "Add integration test similar to `ShareGroupHeartbeatRequestTest` and `ConsumerGroupHeartbeatRequestTest` for `StreamsGroupHeartbeat`"}}}
{"project": "KAFKA", "issue_key": "KAFKA-19806", "title": "Hook to enable / disable multi-partition remote fetch feature", "status": "Patch Available", "reporter": "Kamal Chandraprakash", "assignee": "Kamal Chandraprakash", "priority": "Major", "labels": [], "created": "2025-10-17T08:58:11.000+0000", "updated": "2025-10-21T06:19:48.000+0000", "description": null, "comments": [], "derived_tasks": {"summarization": "Hook to enable / disable multi-partition remote fetch feature", "classification": "feature", "qna": {"question": "What is the issue 'Hook to enable / disable multi-partition remote fetch feature' about?", "answer": null}}}
{"project": "KAFKA", "issue_key": "KAFKA-19805", "title": "Add acks dimension to BrokerTopicMetrics for produce requests", "status": "Open", "reporter": "\u4f0d\u5b66\u660e", "assignee": "\u4f0d\u5b66\u660e", "priority": "Major", "labels": ["features"], "created": "2025-10-17T05:54:23.000+0000", "updated": "2025-10-17T09:58:52.000+0000", "description": "h3. *Title*\r\n\r\nAdd {{acks}} dimension to {{BrokerTopicMetrics}} to measure per-acks produce performance impact\r\n----\r\nh3. *Summary*\r\n\r\nCurrently, Kafka\u2019s {{BrokerTopicMetrics}} only tracks produce metrics such as {{{}BytesInPerSec{}}}, {{{}MessagesInPerSec{}}}, and {{ProduceRequestsPerSec}} at the topic level. However, these metrics do not distinguish between different producer acknowledgment ({{{}acks{}}}) configurations ({{{}acks=0{}}}, {{{}acks=1{}}}, {{{}acks=-1{}}}).\r\nIn high-throughput environments, different {{acks}} levels have significantly different impacts on broker CPU, I/O, and network utilization.\r\nThis proposal introduces an additional {{acks}} label to existing topic-level metrics, enabling more granular visibility into broker performance under various producer reliability modes.\r\n----\r\nh3. *Motivation*\r\n\r\nThe current aggregated produce metrics make it difficult to assess the performance and stability implications of different {{acks}} settings on brokers.\r\nFor example, asynchronous ({{{}acks=0{}}}) and fully acknowledged ({{{}acks=-1{}}}) produces can have very different effects on disk I/O, request queues, and replication latency, but these effects are hidden in current metrics.\r\n\r\nBy introducing an {{acks}} dimension, operators and performance engineers can:\r\n * Quantify the resource cost of different producer acknowledgment strategies.\r\n\r\n * Analyze how {{acks}} configuration affects cluster throughput, replication load, and latency.\r\n\r\n * Perform fine-grained benchmarking and capacity planning.\r\n\r\n----\r\nh3. *Proposed Changes*\r\n # *Extend {{BrokerTopicStats}}*\r\nAdd a new {{perTopicAcksStats}} structure to track metrics per {{(topic, acks)}} combination:\r\n\r\n{code:java}\r\nval perTopicAcksStats = new Pool[(String, Short), BrokerTopicMetrics](\r\nSome((key) => new BrokerTopicMetrics(Some(s\"${key._1},ack=${key._2}\")))\r\n){code}\r\n # *Instrument Produce Handling*\r\nIn {{{}KafkaApis.handleProduceRequest{}}}, extract the producer {{acks}} value and record metrics accordingly:\r\n\r\n{code:java}\r\nval ackVal = produceRequest.acks()\r\nbrokerTopicStats.topicStats(topic).bytesInRate.mark(bytes)\r\nbrokerTopicStats.topicAcksStats(topic, ackVal).bytesInRate.mark(bytes){code}\r\nThe same logic applies to:\r\n * \r\n ** {{messagesInRate}}\r\n\r\n * \r\n ** {{produceRequestsRate}}\r\n\r\n # *Automatic Metric Naming*\r\nSince {{BrokerTopicMetrics}} extends {{{}KafkaMetricsGroup{}}}, the new label will automatically generate JMX metrics like:\r\n\r\n{{kafka.server:type=BrokerTopicMetrics,name=BytesInPerSec,topic=perf-test,ack=-1\r\nkafka.server:type=BrokerTopicMetrics,name=BytesInPerSec,topic=perf-test,ack=1\r\nkafka.server:type=BrokerTopicMetrics,name=BytesInPerSec,topic=perf-test,ack=0}}\r\n # *Performance Considerations*\r\n\r\n * \r\n ** {{perTopicAcksStats}} uses lazy initialization and caching via {{Pool}} to avoid excessive metric object creation.\r\n\r\n * \r\n ** Expiration or cleanup logic can be added for inactive metrics.\r\n\r\n----\r\nh3. *Example Metrics Output*\r\n\r\n\u00a0\r\n{code:java}\r\nkafka.server:type=BrokerTopicMetrics,name=BytesInPerSec,topic=perf-test,ack=0\r\nkafka.server:type=BrokerTopicMetrics,name=BytesInPerSec,topic=perf-test,ack=1\r\nkafka.server:type=BrokerTopicMetrics,name=BytesInPerSec,topic=perf-test,ack=-1{code}\r\n----\r\nh3. *Compatibility & Impact*\r\n * No breaking changes to existing metrics.\r\n\r\n * Existing metric names and topic-level aggregation remain unaffected.\r\n\r\n * New metrics are additive and optional.", "comments": [], "derived_tasks": {"summarization": "Add acks dimension to BrokerTopicMetrics for produce requests", "classification": "feature", "qna": {"question": "What is the issue 'Add acks dimension to BrokerTopicMetrics for produce requests' about?", "answer": "h3. *Title*\r\n\r\nAdd {{acks}} dimension to {{BrokerTopicMetrics}} to measure per-acks produce performance impact\r\n----\r\nh3. *Summary*\r\n\r\nCurrently, Kafka\u2019s {{BrokerTopicMetrics}} only tracks produce metrics such as {{{}BytesInPerSec{}}}, {{{}MessagesInPerSec{}}}, and {{ProduceRequestsPerSec}} at the topic level. However, these metrics do not distinguish between different producer acknowledgment ({{{}acks{}}}) configurations ({{{}acks=0{}}}, {{{}acks=1{}}}, {{{}acks=-1{}}}).\r\nIn high-throughput environments, different {{acks}} levels have significantly different impacts on broker CPU, I/O, and network utilization.\r\nThis proposal introduces an additional {{acks}} label to existing topic-level metrics, enabling more granular visibility into broker performance under various producer reliability modes.\r\n----\r\nh3. *Motivation*\r\n\r\nThe current aggregated produce metrics make it difficult to assess the performance and stability implications of different {{acks}} settings on brokers.\r\nFor example, asynchronous ({{{}acks=0{}}}) and fully acknowledged ({{{}acks=-1{}}}) produces can have very different effects on disk I/O, request queues, and replication latency, but these effects are hidden in current metrics.\r\n\r\nBy introducing an {{acks}} dimension, operators and performance engineers can:\r\n * Quantify the resource cost of different producer acknowledgment strategies.\r\n\r\n * Analyze how {{acks}} configuration affects cluster throughput, replication load, and latency.\r\n\r\n * Perform fine-grained benchmarking and capacity planning.\r\n\r\n----\r\nh3. *Proposed Changes*\r\n # *Extend {{BrokerTopicStats}}*\r\nAdd a new {{perTopicAcksStats}} structure to track metrics per {{(topic, acks)}} combination:\r\n\r\n{code:java}\r\nval perTopicAcksStats = new Pool[(String, Short), BrokerTopicMetrics](\r\nSome((key) => new BrokerTopicMetrics(Some(s\"${key._1},ack=${key._2}\")))\r\n){code}\r\n # *Instrument Produce Handling*\r\nIn {{{}KafkaApis.handleProduceRequest{}}}, extract the producer {{acks}} value and record metrics accordingly:\r\n\r\n{code:java}\r\nval ackVal = produceRequest.acks()\r\nbrokerTopicStats.topicStats(topic).bytesInRate.mark(bytes)\r\nbrokerTopicStats.topicAcksStats(topic, ackVal).bytesInRate.mark(bytes){code}\r\nThe same logic applies to:\r\n * \r\n ** {{messagesInRate}}\r\n\r\n * \r\n ** {{produceRequestsRate}}\r\n\r\n # *Automatic Metric Naming*\r\nSince {{BrokerTopicMetrics}} extends {{{}KafkaMetricsGroup{}}}, the new label will automatically generate JMX metrics like:\r\n\r\n{{kafka.server:type=BrokerTopicMetrics,name=BytesInPerSec,topic=perf-test,ack=-1\r\nkafka.server:type=BrokerTopicMetrics,name=BytesInPerSec,topic=perf-test,ack=1\r\nkafka.server:type=BrokerTopicMetrics,name=BytesInPerSec,topic=perf-test,ack=0}}\r\n # *Performance Considerations*\r\n\r\n * \r\n ** {{perTopicAcksStats}} uses lazy initialization and caching via {{Pool}} to avoid excessive metric object creation.\r\n\r\n * \r\n ** Expiration or cleanup logic can be added for inactive metrics.\r\n\r\n----\r\nh3. *Example Metrics Output*\r\n\r\n\u00a0\r\n{code:java}\r\nkafka.server:type=BrokerTopicMetrics,name=BytesInPerSec,topic=perf-test,ack=0\r\nkafka.server:type=BrokerTopicMetrics,name=BytesInPerSec,topic=perf-test,ack=1\r\nkafka.server:type=BrokerTopicMetrics,name=BytesInPerSec,topic=perf-test,ack=-1{code}\r\n----\r\nh3. *Compatibility & Impact*\r\n * No breaking changes to existing metrics.\r\n\r\n * Existing metric names and topic-level aggregation remain unaffected.\r\n\r\n * New metrics are additive and optional."}}}
{"project": "KAFKA", "issue_key": "KAFKA-19804", "title": "Improve heartbeat request manager initial HB interval ", "status": "Open", "reporter": "Lianet Magrans", "assignee": "Kuan Po Tseng", "priority": "Major", "labels": [], "created": "2025-10-16T20:49:52.000+0000", "updated": "2025-10-30T16:21:43.000+0000", "description": "With KIP-848, consumer HB interval config moved to the broker, so currently, the consumer HB request manager starts with a 0ms interval (mainly to send a first HB right away after the consumer subscribe + poll). Once a response is received, the consumer takes the interval from the response and starts using it.\u00a0\r\n\r\nThat 0ms initial interval makes that the HB mgr poll continuously executes logic on a tight loop that may not really be needed. It mostly has to wait for a response (or a failure).\r\n\r\nProbably worse than this, is the impact on the app thread, given that pollTimeout takes into account the maxTimeToWait from the network thread, that is directly impacted by the timeToNextHeartbeat\r\n * [https://github.com/apache/kafka/blob/388739f5d847d7a16e389d9891f806547f023476/clients/src/main/java/org/apache/kafka/clients/consumer/internals/AsyncKafkaConsumer.java#L1764-L1766]\r\n * [https://github.com/apache/kafka/blob/781bc7a54b8c4f7c86f0d6bb9ef8399d86d0735e/clients/src/main/java/org/apache/kafka/clients/consumer/internals/AbstractHeartbeatRequestManager.java#L255]\r\n\r\nWe should review and consider setting a non-zero initial interval (while we wait for the actual interval from the broker). One option to consider would be using the request timeout maybe (just a first thought)\r\n\r\nHigh level goals here would be to:\r\n * maintain the behaviour of sending a first HB without delay\u00a0\r\n * ensure no unneeded activity on the HB mgr poll in the background, in tight loop, while we're just waiting for the first HB response with an interval\r\n * ensure the app thread poll timeout is not affected", "comments": [], "derived_tasks": {"summarization": "Improve heartbeat request manager initial HB interval ", "classification": "feature", "qna": {"question": "What is the issue 'Improve heartbeat request manager initial HB interval ' about?", "answer": "With KIP-848, consumer HB interval config moved to the broker, so currently, the consumer HB request manager starts with a 0ms interval (mainly to send a first HB right away after the consumer subscribe + poll). Once a response is received, the consumer takes the interval from the response and starts using it.\u00a0\r\n\r\nThat 0ms initial interval makes that the HB mgr poll continuously executes logic on a tight loop that may not really be needed. It mostly has to wait for a response (or a failure).\r\n\r\nProbably worse than this, is the impact on the app thread, given that pollTimeout takes into account the maxTimeToWait from the network thread, that is directly impacted by the timeToNextHeartbeat\r\n * [https://github.com/apache/kafka/blob/388739f5d847d7a16e389d9891f806547f023476/clients/src/main/java/org/apache/kafka/clients/consumer/internals/AsyncKafkaConsumer.java#L1764-L1766]\r\n * [https://github.com/apache/kafka/blob/781bc7a54b8c4f7c86f0d6bb9ef8399d86d0735e/clients/src/main/java/org/apache/kafka/clients/consumer/internals/AbstractHeartbeatRequestManager.java#L255]\r\n\r\nWe should review and consider setting a non-zero initial interval (while we wait for the actual interval from the broker). One option to consider would be using the request timeout maybe (just a first thought)\r\n\r\nHigh level goals here would be to:\r\n * maintain the behaviour of sending a first HB without delay\u00a0\r\n * ensure no unneeded activity on the HB mgr poll in the background, in tight loop, while we're just waiting for the first HB response with an interval\r\n * ensure the app thread poll timeout is not affected"}}}
{"project": "KAFKA", "issue_key": "KAFKA-19803", "title": "Relax state directory file system restrictions", "status": "Resolved", "reporter": "Matthias J. Sax", "assignee": "Nikita Shupletsov", "priority": "Minor", "labels": ["kip"], "created": "2025-10-16T18:36:27.000+0000", "updated": "2025-10-25T16:14:42.000+0000", "description": "The implementation of permission restriction by https://issues.apache.org/jira/browse/KAFKA-10705 is very restrictive on groups. Existing group write permissions should be kept.\r\n\r\nWe could also make this configurable, which would require a KIP.\r\n\r\nKIP-1230 [https://cwiki.apache.org/confluence/display/KAFKA/KIP-1230%3A+Add+config+for+file+system+permissions]\u00a0", "comments": [], "derived_tasks": {"summarization": "Relax state directory file system restrictions", "classification": "feature", "qna": {"question": "What is the issue 'Relax state directory file system restrictions' about?", "answer": "The implementation of permission restriction by https://issues.apache.org/jira/browse/KAFKA-10705 is very restrictive on groups. Existing group write permissions should be kept.\r\n\r\nWe could also make this configurable, which would require a KIP.\r\n\r\nKIP-1230 [https://cwiki.apache.org/confluence/display/KAFKA/KIP-1230%3A+Add+config+for+file+system+permissions]\u00a0"}}}
{"project": "KAFKA", "issue_key": "KAFKA-19802", "title": "Update ShareGroupCommand to use share partition lag information", "status": "Open", "reporter": "Chirag Wadhwa", "assignee": "Andrew Schofield", "priority": "Minor", "labels": [], "created": "2025-10-16T08:29:07.000+0000", "updated": "2025-10-25T11:09:24.000+0000", "description": null, "comments": [], "derived_tasks": {"summarization": "Update ShareGroupCommand to use share partition lag information", "classification": "feature", "qna": {"question": "What is the issue 'Update ShareGroupCommand to use share partition lag information' about?", "answer": null}}}
{"project": "KAFKA", "issue_key": "KAFKA-19801", "title": "Introduce deliveryCompleteCount in DescribeShareGroupStateOffsets", "status": "Open", "reporter": "Chirag Wadhwa", "assignee": "Chirag Wadhwa", "priority": "Minor", "labels": [], "created": "2025-10-16T08:28:25.000+0000", "updated": "2025-10-16T08:28:25.000+0000", "description": null, "comments": [], "derived_tasks": {"summarization": "Introduce deliveryCompleteCount in DescribeShareGroupStateOffsets", "classification": "feature", "qna": {"question": "What is the issue 'Introduce deliveryCompleteCount in DescribeShareGroupStateOffsets' about?", "answer": null}}}
{"project": "KAFKA", "issue_key": "KAFKA-19800", "title": "Compute share partition lag in GroupCoordinatorService", "status": "Open", "reporter": "Chirag Wadhwa", "assignee": "Chirag Wadhwa", "priority": "Minor", "labels": [], "created": "2025-10-16T08:28:14.000+0000", "updated": "2025-10-16T08:28:47.000+0000", "description": null, "comments": [], "derived_tasks": {"summarization": "Compute share partition lag in GroupCoordinatorService", "classification": "feature", "qna": {"question": "What is the issue 'Compute share partition lag in GroupCoordinatorService' about?", "answer": null}}}
{"project": "KAFKA", "issue_key": "KAFKA-19799", "title": "Introduce deliveryCompleteCount in ReadShareGroupStateSummary", "status": "Open", "reporter": "Chirag Wadhwa", "assignee": "Chirag Wadhwa", "priority": "Minor", "labels": [], "created": "2025-10-16T08:28:01.000+0000", "updated": "2025-10-16T08:28:38.000+0000", "description": null, "comments": [], "derived_tasks": {"summarization": "Introduce deliveryCompleteCount in ReadShareGroupStateSummary", "classification": "feature", "qna": {"question": "What is the issue 'Introduce deliveryCompleteCount in ReadShareGroupStateSummary' about?", "answer": null}}}
{"project": "KAFKA", "issue_key": "KAFKA-19798", "title": "Persist deliveryCompleteCount in ShareSnapshot and ShareUpdate records", "status": "Open", "reporter": "Chirag Wadhwa", "assignee": "Chirag Wadhwa", "priority": "Minor", "labels": [], "created": "2025-10-16T08:27:50.000+0000", "updated": "2025-10-16T08:27:50.000+0000", "description": null, "comments": [], "derived_tasks": {"summarization": "Persist deliveryCompleteCount in ShareSnapshot and ShareUpdate records", "classification": "feature", "qna": {"question": "What is the issue 'Persist deliveryCompleteCount in ShareSnapshot and ShareUpdate records' about?", "answer": null}}}
{"project": "KAFKA", "issue_key": "KAFKA-19797", "title": "Implement deliveryCompleteCount in writeShareGroupStateRPC", "status": "Open", "reporter": "Chirag Wadhwa", "assignee": "Chirag Wadhwa", "priority": "Minor", "labels": [], "created": "2025-10-16T08:27:22.000+0000", "updated": "2025-10-30T18:01:01.000+0000", "description": null, "comments": [], "derived_tasks": {"summarization": "Implement deliveryCompleteCount in writeShareGroupStateRPC", "classification": "feature", "qna": {"question": "What is the issue 'Implement deliveryCompleteCount in writeShareGroupStateRPC' about?", "answer": null}}}
{"project": "KAFKA", "issue_key": "KAFKA-19796", "title": "Introduce computations for inFlightTerminalRecords", "status": "Resolved", "reporter": "Chirag Wadhwa", "assignee": "Chirag Wadhwa", "priority": "Minor", "labels": [], "created": "2025-10-16T08:27:04.000+0000", "updated": "2025-10-29T20:42:14.000+0000", "description": null, "comments": [], "derived_tasks": {"summarization": "Introduce computations for inFlightTerminalRecords", "classification": "feature", "qna": {"question": "What is the issue 'Introduce computations for inFlightTerminalRecords' about?", "answer": null}}}
{"project": "KAFKA", "issue_key": "KAFKA-19795", "title": "Mark the minOneMessage as false when delayedRemoteFetch is present in the first partition", "status": "Resolved", "reporter": "Kamal Chandraprakash", "assignee": "Kamal Chandraprakash", "priority": "Major", "labels": [], "created": "2025-10-16T04:36:54.000+0000", "updated": "2025-10-16T04:49:48.000+0000", "description": null, "comments": [], "derived_tasks": {"summarization": "Mark the minOneMessage as false when delayedRemoteFetch is present in the first partition", "classification": "feature", "qna": {"question": "What is the issue 'Mark the minOneMessage as false when delayedRemoteFetch is present in the first partition' about?", "answer": null}}}
{"project": "KAFKA", "issue_key": "KAFKA-19794", "title": "MirrorCheckpointTask causes consumers on target cluster to rewind offsets or skip messages due to erroneous offset commits", "status": "Open", "reporter": "Ravindranath Kakarla", "assignee": null, "priority": "Major", "labels": [], "created": "2025-10-15T22:17:29.000+0000", "updated": "2025-10-15T22:24:41.000+0000", "description": "h2. *Description*\r\n\r\nThe MirrorCheckpointTask in Mirror Maker 2 commits offsets for active consumer groups on the target cluster, causing consumers to rewind their offsets or skip messages. Typically brokers prevent committing offsets for consumer groups that are not in `EMPTY` state by throwing {{{}UnknownMemberIdException{}}}. In addition, {{MirrorCheckpointTask}} has logic in place to prevent committing offsets older than target for {{EMPTY}} consumer groups. However, due to a bug in {{MirrorCheckpointTask}} code, this prevention check is not enforced and it attempts to commit offsets for {{STABLE}} consumers. These calls can go through if consumers were momentarily disconnected moving the group state to {{{}EMPTY{}}}. This results in consumers' offsets getting reset to older values. If the offset is not available on the target broker (due to retention), the consumers can get reset to \"{{{}earliest\"{}}} or \"{{{}latest\"{}}}, thus reading duplicates or skipping messages.\u00a0\r\nh2. *Bug location*\r\n\r\n1. In [MirrorCheckpointTask|#L305], we only update the latest target cluster offsets ({{{}idleConsumerGroupsOffset{}}})\u00a0 if target consumer group state is {{{}EMPTY{}}}.\r\n\r\n2. When {{syncGroupOffset}} is called, we check if the target consumer group is present in\u00a0\u00a0\r\n\r\n{{{}idleConsumerGroupsOffset{}}}. The consumer group won't be present as it's an active group. We assume that this is a new group and start syncing consumer group offsets to target. These calls fail with {_}{{{{{}Unable to sync offsets for consumer group XYZ. This is likely caused by consumers currently using this group in the target cluster. (org.apache.kafka.connect.mirror.MirrorCheckpointTask{}}}}}{_}. When consumers have failed over, the logs typically contain a lot of these messages. These calls can succeed if consumer is momentarily disconnected due to restarts. The code should not assume the lack of consumer group in {{idleConsumerGroupsOffset}} map as a new consumer group.\r\n\r\n3. These erroneous behavior can also be triggered calls to {{describeConsumerGroups}} or {{listConsumerGroupOffsets}} fail in {{refreshIdleConsumerGroupOffset}} method due to transient timeouts.\r\nh2. *Fix*\r\n\r\nPotential fix would be to add an explicit check to only sync offsets for EMPTY consumer group. We should also skip offset syncing for consumer groups for which we couldn't refresh the offsets.\u00a0\r\n\r\n\u00a0\r\n{code:java}\r\n// Fixed code adds state checking:\r\nConsumerGroupState groupStateOnTarget = targetConsumerGroupStates.get(consumerGroupId);\r\nif (!isGroupPresentOnTarget || groupStateOnTarget == ConsumerGroupState.DEAD)\r\n{ \u00a0 \u00a0 // Safe to sync - new or dead group \u00a0 \u00a0 syncGroupOffset(consumerGroupId, convertedUpstreamOffset); }\r\nelse if (groupStateOnTarget == ConsumerGroupState.EMPTY)\r\n{ \u00a0 \u00a0 // Safe to sync - idle group \u00a0 \u00a0 // ... existing offset comparison logic }\r\nelse {\r\n\u00a0 \u00a0 // Skip active groups (STABLE, PREPARING_REBALANCE, COMPLETING_REBALANCE)\r\n\u00a0 \u00a0 log.info(\"Consumer group: {} with state: {} is being actively consumed on the target, skipping sync.\",\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0consumerGroupId, groupStateOnTarget);\r\n}\r\n{code}\r\n\u00a0\r\n\r\n\u00a0", "comments": [], "derived_tasks": {"summarization": "MirrorCheckpointTask causes consumers on target cluster to rewind offsets or skip messages due to erroneous offset commits", "classification": "feature", "qna": {"question": "What is the issue 'MirrorCheckpointTask causes consumers on target cluster to rewind offsets or skip messages due to erroneous offset commits' about?", "answer": "h2. *Description*\r\n\r\nThe MirrorCheckpointTask in Mirror Maker 2 commits offsets for active consumer groups on the target cluster, causing consumers to rewind their offsets or skip messages. Typically brokers prevent committing offsets for consumer groups that are not in `EMPTY` state by throwing {{{}UnknownMemberIdException{}}}. In addition, {{MirrorCheckpointTask}} has logic in place to prevent committing offsets older than target for {{EMPTY}} consumer groups. However, due to a bug in {{MirrorCheckpointTask}} code, this prevention check is not enforced and it attempts to commit offsets for {{STABLE}} consumers. These calls can go through if consumers were momentarily disconnected moving the group state to {{{}EMPTY{}}}. This results in consumers' offsets getting reset to older values. If the offset is not available on the target broker (due to retention), the consumers can get reset to \"{{{}earliest\"{}}} or \"{{{}latest\"{}}}, thus reading duplicates or skipping messages.\u00a0\r\nh2. *Bug location*\r\n\r\n1. In [MirrorCheckpointTask|#L305], we only update the latest target cluster offsets ({{{}idleConsumerGroupsOffset{}}})\u00a0 if target consumer group state is {{{}EMPTY{}}}.\r\n\r\n2. When {{syncGroupOffset}} is called, we check if the target consumer group is present in\u00a0\u00a0\r\n\r\n{{{}idleConsumerGroupsOffset{}}}. The consumer group won't be present as it's an active group. We assume that this is a new group and start syncing consumer group offsets to target. These calls fail with {_}{{{{{}Unable to sync offsets for consumer group XYZ. This is likely caused by consumers currently using this group in the target cluster. (org.apache.kafka.connect.mirror.MirrorCheckpointTask{}}}}}{_}. When consumers have failed over, the logs typically contain a lot of these messages. These calls can succeed if consumer is momentarily disconnected due to restarts. The code should not assume the lack of consumer group in {{idleConsumerGroupsOffset}} map as a new consumer group.\r\n\r\n3. These erroneous behavior can also be triggered calls to {{describeConsumerGroups}} or {{listConsumerGroupOffsets}} fail in {{refreshIdleConsumerGroupOffset}} method due to transient timeouts.\r\nh2. *Fix*\r\n\r\nPotential fix would be to add an explicit check to only sync offsets for EMPTY consumer group. We should also skip offset syncing for consumer groups for which we couldn't refresh the offsets.\u00a0\r\n\r\n\u00a0\r\n{code:java}\r\n// Fixed code adds state checking:\r\nConsumerGroupState groupStateOnTarget = targetConsumerGroupStates.get(consumerGroupId);\r\nif (!isGroupPresentOnTarget || groupStateOnTarget == ConsumerGroupState.DEAD)\r\n{ \u00a0 \u00a0 // Safe to sync - new or dead group \u00a0 \u00a0 syncGroupOffset(consumerGroupId, convertedUpstreamOffset); }\r\nelse if (groupStateOnTarget == ConsumerGroupState.EMPTY)\r\n{ \u00a0 \u00a0 // Safe to sync - idle group \u00a0 \u00a0 // ... existing offset comparison logic }\r\nelse {\r\n\u00a0 \u00a0 // Skip active groups (STABLE, PREPARING_REBALANCE, COMPLETING_REBALANCE)\r\n\u00a0 \u00a0 log.info(\"Consumer group: {} with state: {} is being actively consumed on the target, skipping sync.\",\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0consumerGroupId, groupStateOnTarget);\r\n}\r\n{code}\r\n\u00a0\r\n\r\n\u00a0"}}}
{"project": "KAFKA", "issue_key": "KAFKA-19793", "title": "Disable topic autocreation for streams consumers.", "status": "Patch Available", "reporter": "Nikita Shupletsov", "assignee": "Arpit Goyal", "priority": "Minor", "labels": [], "created": "2025-10-15T19:15:21.000+0000", "updated": "2025-10-31T17:07:22.000+0000", "description": "Currently we disable it only for [the main consumer|https://github.com/apache/kafka/blob/trunk/streams/src/main/java/org/apache/kafka/streams/StreamsConfig.java#L1793], but not for the [restore|https://github.com/apache/kafka/blob/trunk/streams/src/main/java/org/apache/kafka/streams/StreamsConfig.java#L1832] or [global|https://github.com/apache/kafka/blob/trunk/streams/src/main/java/org/apache/kafka/streams/StreamsConfig.java#L1865] consumers.", "comments": [], "derived_tasks": {"summarization": "Disable topic autocreation for streams consumers.", "classification": "feature", "qna": {"question": "What is the issue 'Disable topic autocreation for streams consumers.' about?", "answer": "Currently we disable it only for [the main consumer|https://github.com/apache/kafka/blob/trunk/streams/src/main/java/org/apache/kafka/streams/StreamsConfig.java#L1793], but not for the [restore|https://github.com/apache/kafka/blob/trunk/streams/src/main/java/org/apache/kafka/streams/StreamsConfig.java#L1832] or [global|https://github.com/apache/kafka/blob/trunk/streams/src/main/java/org/apache/kafka/streams/StreamsConfig.java#L1865] consumers."}}}
{"project": "KAFKA", "issue_key": "KAFKA-19792", "title": "Gradle build fails after Swagger patch version update", "status": "Resolved", "reporter": "Dejan Stojadinovi\u0107", "assignee": "Kuan Po Tseng", "priority": "Minor", "labels": ["Gradle", "build-failure", "build-problem", "gradle", "swagger"], "created": "2025-10-15T17:20:40.000+0000", "updated": "2025-10-18T15:43:02.000+0000", "description": "*How to reproduce:*\u00a0\r\n * checkout trunk (Swagger version: 2.2.25)\r\n * change `swaggerVersion` in `gradle.properties` to some more recent version (something in between of 2.2.27 and 2.2.39)\r\n * execute *./gradlew clean releaseTarGz*\r\n * build fails with an error _*Cannot set the value of task ':connect:runtime:genConnectOpenAPIDocs' property 'sortOutput' of type java.lang.Boolean using an instance of type java.lang.String.*_\r\n * see details below\r\n * (!) note: build works for swagger version 2.2.26 (although reporter had some issues once or twice with that particular version, so please keep that on your mind)\r\n\r\n{code:java}\r\ndejan@dejan-HP-ProBook-450-G7:~/kafka$ ./gradlew clean releaseTarGz \r\n\r\n> Configure project :\r\nStarting build with version 4.2.0-SNAPSHOT (commit id 34581dff) using Gradle 9.1.0, Java 17 and Scala 2.13.17\r\nBuild properties: ignoreFailures=false, maxParallelForks=8, maxScalacThreads=8, maxTestRetries=0\r\n\r\n[Incubating] Problems report is available at: file:///home/dejan/kafka/build/reports/problems/problems-report.html\r\n\r\nFAILURE: Build failed with an exception.\r\n\r\n* Where:\r\nBuild file '/home/dejan/kafka/build.gradle' line: 3728\r\n\r\n* What went wrong:\r\nA problem occurred evaluating root project 'kafka'.\r\n> Cannot set the value of task ':connect:runtime:genConnectOpenAPIDocs' property 'sortOutput' of type java.lang.Boolean using an instance of type java.lang.String.\r\n\r\n* Try:\r\n> Run with --stacktrace option to get the stack trace.\r\n> Run with --info or --debug option to get more log output.\r\n> Get more help at https://help.gradle.org.\r\n\r\nDeprecated Gradle features were used in this build, making it incompatible with Gradle 10.\r\n\r\nYou can use '--warning-mode all' to show the individual deprecation warnings and determine if they come from your own scripts or plugins.\r\n\r\nFor more on this, please refer to https://docs.gradle.org/9.1.0/userguide/command_line_interface.html#sec:command_line_warnings in the Gradle documentation.\r\n\r\nBUILD FAILED in 5s\r\ndejan@dejan-HP-ProBook-450-G7:~/kafka$\r\n{code}", "comments": [], "derived_tasks": {"summarization": "Gradle build fails after Swagger patch version update", "classification": "feature", "qna": {"question": "What is the issue 'Gradle build fails after Swagger patch version update' about?", "answer": "*How to reproduce:*\u00a0\r\n * checkout trunk (Swagger version: 2.2.25)\r\n * change `swaggerVersion` in `gradle.properties` to some more recent version (something in between of 2.2.27 and 2.2.39)\r\n * execute *./gradlew clean releaseTarGz*\r\n * build fails with an error _*Cannot set the value of task ':connect:runtime:genConnectOpenAPIDocs' property 'sortOutput' of type java.lang.Boolean using an instance of type java.lang.String.*_\r\n * see details below\r\n * (!) note: build works for swagger version 2.2.26 (although reporter had some issues once or twice with that particular version, so please keep that on your mind)\r\n\r\n{code:java}\r\ndejan@dejan-HP-ProBook-450-G7:~/kafka$ ./gradlew clean releaseTarGz \r\n\r\n> Configure project :\r\nStarting build with version 4.2.0-SNAPSHOT (commit id 34581dff) using Gradle 9.1.0, Java 17 and Scala 2.13.17\r\nBuild properties: ignoreFailures=false, maxParallelForks=8, maxScalacThreads=8, maxTestRetries=0\r\n\r\n[Incubating] Problems report is available at: file:///home/dejan/kafka/build/reports/problems/problems-report.html\r\n\r\nFAILURE: Build failed with an exception.\r\n\r\n* Where:\r\nBuild file '/home/dejan/kafka/build.gradle' line: 3728\r\n\r\n* What went wrong:\r\nA problem occurred evaluating root project 'kafka'.\r\n> Cannot set the value of task ':connect:runtime:genConnectOpenAPIDocs' property 'sortOutput' of type java.lang.Boolean using an instance of type java.lang.String.\r\n\r\n* Try:\r\n> Run with --stacktrace option to get the stack trace.\r\n> Run with --info or --debug option to get more log output.\r\n> Get more help at https://help.gradle.org.\r\n\r\nDeprecated Gradle features were used in this build, making it incompatible with Gradle 10.\r\n\r\nYou can use '--warning-mode all' to show the individual deprecation warnings and determine if they come from your own scripts or plugins.\r\n\r\nFor more on this, please refer to https://docs.gradle.org/9.1.0/userguide/command_line_interface.html#sec:command_line_warnings in the Gradle documentation.\r\n\r\nBUILD FAILED in 5s\r\ndejan@dejan-HP-ProBook-450-G7:~/kafka$\r\n{code}"}}}
{"project": "KAFKA", "issue_key": "KAFKA-19791", "title": "Add Idle Thread Ratio Metric to MetadataLoader", "status": "Resolved", "reporter": "Mahsa Seifikar", "assignee": "Mahsa Seifikar", "priority": "Major", "labels": [], "created": "2025-10-15T16:09:22.000+0000", "updated": "2025-10-31T00:39:08.000+0000", "description": "KIP-1229: [https://cwiki.apache.org/confluence/x/ZQteFw]\u00a0", "comments": [], "derived_tasks": {"summarization": "Add Idle Thread Ratio Metric to MetadataLoader", "classification": "feature", "qna": {"question": "What is the issue 'Add Idle Thread Ratio Metric to MetadataLoader' about?", "answer": "KIP-1229: [https://cwiki.apache.org/confluence/x/ZQteFw]\u00a0"}}}
{"project": "KAFKA", "issue_key": "KAFKA-19790", "title": "Parsing of the scope claim does not comply with RFC-8693", "status": "Open", "reporter": "Keith Wall", "assignee": null, "priority": "Minor", "labels": [], "created": "2025-10-15T10:45:36.000+0000", "updated": "2025-10-15T10:51:40.000+0000", "description": "I notice that the code in Kafka for handling of the scopes claim does not comply with the RFC.\r\n\r\n[https://datatracker.ietf.org/doc/html/rfc8693#name-scope-scopes-claim |https://datatracker.ietf.org/doc/html/rfc8693#name-scope-scopes-claim]says:\r\n{quote}The value of the\u00a0{{scope}}\u00a0claim is a JSON string containing a space-separated list of scopes associated with the token, in the format described in\u00a0[Section 3.3|https://www.rfc-editor.org/rfc/rfc6749#section-3.3]\u00a0of [[RFC6749|https://datatracker.ietf.org/doc/html/rfc6749]]\r\n{quote}\r\n\u00a0\r\n\r\nHowever the code in Kafka that parses the JWT payload does not permit a space separated list.\u00a0 It would treat a value like \"email phone address\" as a single scope \"email phone address\" rather than a three separate scopes of \"email\", \"phone\", \"address\".\r\n\r\nThe affected code is here:\r\n\r\n[https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/common/security/oauthbearer/BrokerJwtValidator.java#L166]\r\n\r\n[https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/common/security/oauthbearer/internals/unsecured/OAuthBearerUnsecuredJws.java#L343]\r\n\r\nImpact:\r\n\r\nLooking at the production code in Apache Kafka itself, I think the defect currently harmless.\u00a0 As far as I can tell, there's no production code that makes use of\u00a0 org.apache.kafka.common.security.oauthbearer.internals.secured.BasicOAuthBearerToken#scope.\r\n\r\nI think there would be a potential for impact for a user writing their own OAuthBearerValidatorCallbackHandler that uses Kafka's BrokerJwtValidator and made use of the scope value.\r\n\r\nFailing unit test:\r\n\r\n[https://github.com/apache/kafka/compare/trunk...k-wall:kafka:KAFKA-19790]\r\n\r\n\u00a0", "comments": [], "derived_tasks": {"summarization": "Parsing of the scope claim does not comply with RFC-8693", "classification": "feature", "qna": {"question": "What is the issue 'Parsing of the scope claim does not comply with RFC-8693' about?", "answer": "I notice that the code in Kafka for handling of the scopes claim does not comply with the RFC.\r\n\r\n[https://datatracker.ietf.org/doc/html/rfc8693#name-scope-scopes-claim |https://datatracker.ietf.org/doc/html/rfc8693#name-scope-scopes-claim]says:\r\n{quote}The value of the\u00a0{{scope}}\u00a0claim is a JSON string containing a space-separated list of scopes associated with the token, in the format described in\u00a0[Section 3.3|https://www.rfc-editor.org/rfc/rfc6749#section-3.3]\u00a0of [[RFC6749|https://datatracker.ietf.org/doc/html/rfc6749]]\r\n{quote}\r\n\u00a0\r\n\r\nHowever the code in Kafka that parses the JWT payload does not permit a space separated list.\u00a0 It would treat a value like \"email phone address\" as a single scope \"email phone address\" rather than a three separate scopes of \"email\", \"phone\", \"address\".\r\n\r\nThe affected code is here:\r\n\r\n[https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/common/security/oauthbearer/BrokerJwtValidator.java#L166]\r\n\r\n[https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/common/security/oauthbearer/internals/unsecured/OAuthBearerUnsecuredJws.java#L343]\r\n\r\nImpact:\r\n\r\nLooking at the production code in Apache Kafka itself, I think the defect currently harmless.\u00a0 As far as I can tell, there's no production code that makes use of\u00a0 org.apache.kafka.common.security.oauthbearer.internals.secured.BasicOAuthBearerToken#scope.\r\n\r\nI think there would be a potential for impact for a user writing their own OAuthBearerValidatorCallbackHandler that uses Kafka's BrokerJwtValidator and made use of the scope value.\r\n\r\nFailing unit test:\r\n\r\n[https://github.com/apache/kafka/compare/trunk...k-wall:kafka:KAFKA-19790]\r\n\r\n\u00a0"}}}
{"project": "KAFKA", "issue_key": "KAFKA-19789", "title": "Handle situations where broker responses appear logically incorrect", "status": "Resolved", "reporter": "Andrew Schofield", "assignee": "Shivsundar R", "priority": "Major", "labels": [], "created": "2025-10-14T10:01:53.000+0000", "updated": "2025-10-29T00:45:03.000+0000", "description": "We have seen a situation in which the ShareFetch responses from the broker appeared logically incorrect. The consumer should be more defensive, and at least log the problem.", "comments": [], "derived_tasks": {"summarization": "Handle situations where broker responses appear logically incorrect", "classification": "feature", "qna": {"question": "What is the issue 'Handle situations where broker responses appear logically incorrect' about?", "answer": "We have seen a situation in which the ShareFetch responses from the broker appeared logically incorrect. The consumer should be more defensive, and at least log the problem."}}}
{"project": "KAFKA", "issue_key": "KAFKA-19788", "title": "kafka server ClusterAuthorization don't support acks=-1 for kafka-client version>3.1.0", "status": "Resolved", "reporter": "zhuming", "assignee": null, "priority": "Major", "labels": [], "created": "2025-10-14T08:22:48.000+0000", "updated": "2025-10-15T06:45:14.000+0000", "description": "* \u00a0kafka server version is 2.5.1\r\n * \u00a0kafka-client version bigger than 3.1.1\u00a0\r\n\r\n\u00a0\r\n{code:java}\r\nimport org.apache.kafka.clients.producer.KafkaProducer;\r\nimport org.apache.kafka.clients.producer.Producer;\r\nimport org.apache.kafka.clients.producer.ProducerRecord;\r\n\r\nimport java.util.Properties;\r\n\r\n\r\npublic class KafkaSendTest {\r\n\r\n  public static void main(String[] args) {\r\n    Properties props = new Properties();\r\n    props.put(\"bootstrap.servers\", \"xxx:9092,xxxx:9092\");\r\n    props.put(\"key.serializer\", \"org.apache.kafka.common.serialization.ByteArraySerializer\");\r\n    props.put(\"value.serializer\", \"org.apache.kafka.common.serialization.ByteArraySerializer\");\r\n    props.put(\"transaction.timeout.ms\", \"300000\");\r\n    props.put(\"acks\", \"-1\"); // If acks=1 or acks=0 it will send successfully\r\n    props.put(\"compression.type\", \"lz4\");\r\n    props.put(\"security.protocol\", \"SASL_PLAINTEXT\");\r\n    props.put(\"sasl.mechanism\", \"SCRAM-SHA-256\");\r\n    props.put(\"sasl.jaas.config\", \"org.apache.kafka.common.security.scram.ScramLoginModule required username=\\\"xxx\\\" password=\\\"xxxx\\\";\");\r\n\r\n    Producer<byte[], byte[]> producer = new KafkaProducer<>(props);\r\n\r\n    try {\r\n      String topic = \"topic1\";\r\n      byte[] value = new byte[]{1,2}; // example\r\n      ProducerRecord<byte[], byte[]> record = new ProducerRecord<>(topic, null, value);\r\n      producer.send(record, (metadata, exception) -> {\r\n        if (exception == null) {\r\n          System.out.printf(\"Sent record(key=%s value=%s) meta(partition=%d, offset=%d)\\n\",\r\n                  record.key(), new String(record.value()), metadata.partition(), metadata.offset());\r\n        } else {\r\n          exception.printStackTrace();\r\n        }\r\n      });\r\n      producer.close();\r\n    } catch (Exception e) {\r\n      e.printStackTrace();\r\n    }\r\n  }\r\n} {code}\r\npom.xml config\r\n{code:java}\r\n<dependency>\r\n  <groupId>org.apache.kafka</groupId>\r\n  <artifactId>kafka-clients</artifactId>\r\n  <version>3.4.0</version>\r\n</dependency> {code}\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0When kafka producer acks=-1, It will throw exception.\r\n\r\n\u00a0\r\n{code:java}\r\norg.apache.kafka.common.KafkaException: Cannot execute transactional method because we are in an error state\tat org.apache.kafka.clients.producer.internals.TransactionManager.maybeFailWithError(TransactionManager.java:1010)\tat org.apache.kafka.clients.producer.internals.TransactionManager.maybeAddPartition(TransactionManager.java:328)\tat org.apache.kafka.clients.producer.KafkaProducer.doSend(KafkaProducer.java:1061)\tat org.apache.kafka.clients.producer.KafkaProducer.send(KafkaProducer.java:962)\tat com.mvad.realtime.show.converter.DataEntryToRTLogConverterTest.main(DataEntryToRTLogConverterTest.java:34)Caused by: org.apache.kafka.common.errors.ClusterAuthorizationException: Cluster authorization failed. {code}\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 If acks=1 or acks=0 it will send successfully\r\n{code:java}\r\nSent record(key=null ) meta(partition=6, offset=321496) {code}\r\n\u00a0 \u00a0 acks=-1 is just a param, How it effects ClusterAuthorization of kafka producer.\r\n\u00a0 \u00a0 Is this a bug or a mechanism in itself?\r\n\r\n\u00a0\r\n\r\nIf change kafka-client verison to 3.1.0. When kafka producer acks=-1, It will send successfully\r\n{code:java}\r\n<dependency>\r\n  <groupId>org.apache.kafka</groupId>\r\n  <artifactId>kafka-clients</artifactId>\r\n  <version>3.1.0</version>\r\n</dependency> {code}\r\n\u00a0", "comments": [], "derived_tasks": {"summarization": "kafka server ClusterAuthorization don't support acks=-1 for kafka-client version>3.1.0", "classification": "feature", "qna": {"question": "What is the issue 'kafka server ClusterAuthorization don't support acks=-1 for kafka-client version>3.1.0' about?", "answer": "* \u00a0kafka server version is 2.5.1\r\n * \u00a0kafka-client version bigger than 3.1.1\u00a0\r\n\r\n\u00a0\r\n{code:java}\r\nimport org.apache.kafka.clients.producer.KafkaProducer;\r\nimport org.apache.kafka.clients.producer.Producer;\r\nimport org.apache.kafka.clients.producer.ProducerRecord;\r\n\r\nimport java.util.Properties;\r\n\r\n\r\npublic class KafkaSendTest {\r\n\r\n  public static void main(String[] args) {\r\n    Properties props = new Properties();\r\n    props.put(\"bootstrap.servers\", \"xxx:9092,xxxx:9092\");\r\n    props.put(\"key.serializer\", \"org.apache.kafka.common.serialization.ByteArraySerializer\");\r\n    props.put(\"value.serializer\", \"org.apache.kafka.common.serialization.ByteArraySerializer\");\r\n    props.put(\"transaction.timeout.ms\", \"300000\");\r\n    props.put(\"acks\", \"-1\"); // If acks=1 or acks=0 it will send successfully\r\n    props.put(\"compression.type\", \"lz4\");\r\n    props.put(\"security.protocol\", \"SASL_PLAINTEXT\");\r\n    props.put(\"sasl.mechanism\", \"SCRAM-SHA-256\");\r\n    props.put(\"sasl.jaas.config\", \"org.apache.kafka.common.security.scram.ScramLoginModule required username=\\\"xxx\\\" password=\\\"xxxx\\\";\");\r\n\r\n    Producer<byte[], byte[]> producer = new KafkaProducer<>(props);\r\n\r\n    try {\r\n      String topic = \"topic1\";\r\n      byte[] value = new byte[]{1,2}; // example\r\n      ProducerRecord<byte[], byte[]> record = new ProducerRecord<>(topic, null, value);\r\n      producer.send(record, (metadata, exception) -> {\r\n        if (exception == null) {\r\n          System.out.printf(\"Sent record(key=%s value=%s) meta(partition=%d, offset=%d)\\n\",\r\n                  record.key(), new String(record.value()), metadata.partition(), metadata.offset());\r\n        } else {\r\n          exception.printStackTrace();\r\n        }\r\n      });\r\n      producer.close();\r\n    } catch (Exception e) {\r\n      e.printStackTrace();\r\n    }\r\n  }\r\n} {code}\r\npom.xml config\r\n{code:java}\r\n<dependency>\r\n  <groupId>org.apache.kafka</groupId>\r\n  <artifactId>kafka-clients</artifactId>\r\n  <version>3.4.0</version>\r\n</dependency> {code}\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0When kafka producer acks=-1, It will throw exception.\r\n\r\n\u00a0\r\n{code:java}\r\norg.apache.kafka.common.KafkaException: Cannot execute transactional method because we are in an error state\tat org.apache.kafka.clients.producer.internals.TransactionManager.maybeFailWithError(TransactionManager.java:1010)\tat org.apache.kafka.clients.producer.internals.TransactionManager.maybeAddPartition(TransactionManager.java:328)\tat org.apache.kafka.clients.producer.KafkaProducer.doSend(KafkaProducer.java:1061)\tat org.apache.kafka.clients.producer.KafkaProducer.send(KafkaProducer.java:962)\tat com.mvad.realtime.show.converter.DataEntryToRTLogConverterTest.main(DataEntryToRTLogConverterTest.java:34)Caused by: org.apache.kafka.common.errors.ClusterAuthorizationException: Cluster authorization failed. {code}\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 If acks=1 or acks=0 it will send successfully\r\n{code:java}\r\nSent record(key=null ) meta(partition=6, offset=321496) {code}\r\n\u00a0 \u00a0 acks=-1 is just a param, How it effects ClusterAuthorization of kafka producer.\r\n\u00a0 \u00a0 Is this a bug or a mechanism in itself?\r\n\r\n\u00a0\r\n\r\nIf change kafka-client verison to 3.1.0. When kafka producer acks=-1, It will send successfully\r\n{code:java}\r\n<dependency>\r\n  <groupId>org.apache.kafka</groupId>\r\n  <artifactId>kafka-clients</artifactId>\r\n  <version>3.1.0</version>\r\n</dependency> {code}\r\n\u00a0"}}}
{"project": "KAFKA", "issue_key": "KAFKA-19787", "title": "kafka server ClusterAuthorization don't support acks=-1 for kafka-client version>3.1.0", "status": "Closed", "reporter": "zhuming", "assignee": null, "priority": "Major", "labels": [], "created": "2025-10-14T03:09:11.000+0000", "updated": "2025-10-14T09:40:40.000+0000", "description": "* \u00a0kafka server version is 2.5.1\r\n * \u00a0kafka-client version bigger than 3.1.1\u00a0\r\n\r\n\u00a0\r\n{code:java}\r\nimport org.apache.kafka.clients.producer.KafkaProducer;\r\nimport org.apache.kafka.clients.producer.Producer;\r\nimport org.apache.kafka.clients.producer.ProducerRecord;\r\n\r\nimport java.util.Properties;\r\n\r\n\r\npublic class KafkaSendTest {\r\n\r\n  public static void main(String[] args) {\r\n    Properties props = new Properties();\r\n    props.put(\"bootstrap.servers\", \"xxx:9092,xxxx:9092\");\r\n    props.put(\"key.serializer\", \"org.apache.kafka.common.serialization.ByteArraySerializer\");\r\n    props.put(\"value.serializer\", \"org.apache.kafka.common.serialization.ByteArraySerializer\");\r\n    props.put(\"transaction.timeout.ms\", \"300000\");\r\n    props.put(\"acks\", \"-1\"); // If acks=1 or acks=0 it will send successfully\r\n    props.put(\"compression.type\", \"lz4\");\r\n    props.put(\"security.protocol\", \"SASL_PLAINTEXT\");\r\n    props.put(\"sasl.mechanism\", \"SCRAM-SHA-256\");\r\n    props.put(\"sasl.jaas.config\", \"org.apache.kafka.common.security.scram.ScramLoginModule required username=\\\"xxx\\\" password=\\\"xxxx\\\";\");\r\n\r\n    Producer<byte[], byte[]> producer = new KafkaProducer<>(props);\r\n\r\n    try {\r\n      String topic = \"topic1\";\r\n      byte[] value = new byte[]{1,2}; \r\n      ProducerRecord<byte[], byte[]> record = new ProducerRecord<>(topic, null, value);\r\n      producer.send(record, (metadata, exception) -> {\r\n        if (exception == null) {\r\n          System.out.printf(\"Sent record(key=%s value=%s) meta(partition=%d, offset=%d)\\n\",\r\n                  record.key(), new String(record.value()), metadata.partition(), metadata.offset());\r\n        } else {\r\n          exception.printStackTrace();\r\n        }\r\n      });\r\n      producer.close();\r\n    } catch (Exception e) {\r\n      e.printStackTrace();\r\n    }\r\n  }\r\n} {code}\r\npom.xml config\r\n\r\n\u00a0\r\n{code:java}\r\n<dependency>\r\n  <groupId>org.apache.kafka</groupId>\r\n  <artifactId>kafka-clients</artifactId>\r\n  <version>3.4.0</version>\r\n</dependency> {code}\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0When kafka producer acks=-1, It will throw exception.\r\n\r\n\u00a0\r\n{code:java}\r\norg.apache.kafka.common.KafkaException: Cannot execute transactional method because we are in an error state\tat org.apache.kafka.clients.producer.internals.TransactionManager.maybeFailWithError(TransactionManager.java:1010)\tat org.apache.kafka.clients.producer.internals.TransactionManager.maybeAddPartition(TransactionManager.java:328)\tat org.apache.kafka.clients.producer.KafkaProducer.doSend(KafkaProducer.java:1061)\tat org.apache.kafka.clients.producer.KafkaProducer.send(KafkaProducer.java:962)\tat com.mvad.realtime.show.converter.DataEntryToRTLogConverterTest.main(DataEntryToRTLogConverterTest.java:34)Caused by: org.apache.kafka.common.errors.ClusterAuthorizationException: Cluster authorization failed. {code}\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 If acks=1 or acks=0 it will send successfully\r\n{code:java}\r\nSent record(key=null ) meta(partition=6, offset=321496) {code}\r\n\u00a0 \u00a0 acks=-1 is just a param, How it effects ClusterAuthorization of kafka producer.\r\n\u00a0 \u00a0 Is this a bug or a mechanism in itself?\r\n\u00a0\r\n\u00a0", "comments": [], "derived_tasks": {"summarization": "kafka server ClusterAuthorization don't support acks=-1 for kafka-client version>3.1.0", "classification": "feature", "qna": {"question": "What is the issue 'kafka server ClusterAuthorization don't support acks=-1 for kafka-client version>3.1.0' about?", "answer": "* \u00a0kafka server version is 2.5.1\r\n * \u00a0kafka-client version bigger than 3.1.1\u00a0\r\n\r\n\u00a0\r\n{code:java}\r\nimport org.apache.kafka.clients.producer.KafkaProducer;\r\nimport org.apache.kafka.clients.producer.Producer;\r\nimport org.apache.kafka.clients.producer.ProducerRecord;\r\n\r\nimport java.util.Properties;\r\n\r\n\r\npublic class KafkaSendTest {\r\n\r\n  public static void main(String[] args) {\r\n    Properties props = new Properties();\r\n    props.put(\"bootstrap.servers\", \"xxx:9092,xxxx:9092\");\r\n    props.put(\"key.serializer\", \"org.apache.kafka.common.serialization.ByteArraySerializer\");\r\n    props.put(\"value.serializer\", \"org.apache.kafka.common.serialization.ByteArraySerializer\");\r\n    props.put(\"transaction.timeout.ms\", \"300000\");\r\n    props.put(\"acks\", \"-1\"); // If acks=1 or acks=0 it will send successfully\r\n    props.put(\"compression.type\", \"lz4\");\r\n    props.put(\"security.protocol\", \"SASL_PLAINTEXT\");\r\n    props.put(\"sasl.mechanism\", \"SCRAM-SHA-256\");\r\n    props.put(\"sasl.jaas.config\", \"org.apache.kafka.common.security.scram.ScramLoginModule required username=\\\"xxx\\\" password=\\\"xxxx\\\";\");\r\n\r\n    Producer<byte[], byte[]> producer = new KafkaProducer<>(props);\r\n\r\n    try {\r\n      String topic = \"topic1\";\r\n      byte[] value = new byte[]{1,2}; \r\n      ProducerRecord<byte[], byte[]> record = new ProducerRecord<>(topic, null, value);\r\n      producer.send(record, (metadata, exception) -> {\r\n        if (exception == null) {\r\n          System.out.printf(\"Sent record(key=%s value=%s) meta(partition=%d, offset=%d)\\n\",\r\n                  record.key(), new String(record.value()), metadata.partition(), metadata.offset());\r\n        } else {\r\n          exception.printStackTrace();\r\n        }\r\n      });\r\n      producer.close();\r\n    } catch (Exception e) {\r\n      e.printStackTrace();\r\n    }\r\n  }\r\n} {code}\r\npom.xml config\r\n\r\n\u00a0\r\n{code:java}\r\n<dependency>\r\n  <groupId>org.apache.kafka</groupId>\r\n  <artifactId>kafka-clients</artifactId>\r\n  <version>3.4.0</version>\r\n</dependency> {code}\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0When kafka producer acks=-1, It will throw exception.\r\n\r\n\u00a0\r\n{code:java}\r\norg.apache.kafka.common.KafkaException: Cannot execute transactional method because we are in an error state\tat org.apache.kafka.clients.producer.internals.TransactionManager.maybeFailWithError(TransactionManager.java:1010)\tat org.apache.kafka.clients.producer.internals.TransactionManager.maybeAddPartition(TransactionManager.java:328)\tat org.apache.kafka.clients.producer.KafkaProducer.doSend(KafkaProducer.java:1061)\tat org.apache.kafka.clients.producer.KafkaProducer.send(KafkaProducer.java:962)\tat com.mvad.realtime.show.converter.DataEntryToRTLogConverterTest.main(DataEntryToRTLogConverterTest.java:34)Caused by: org.apache.kafka.common.errors.ClusterAuthorizationException: Cluster authorization failed. {code}\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 If acks=1 or acks=0 it will send successfully\r\n{code:java}\r\nSent record(key=null ) meta(partition=6, offset=321496) {code}\r\n\u00a0 \u00a0 acks=-1 is just a param, How it effects ClusterAuthorization of kafka producer.\r\n\u00a0 \u00a0 Is this a bug or a mechanism in itself?\r\n\u00a0\r\n\u00a0"}}}
{"project": "KAFKA", "issue_key": "KAFKA-19786", "title": "Session mode compileScala reports missing classes java main directory", "status": "Open", "reporter": "HongYi Chen", "assignee": "HongYi Chen", "priority": "Minor", "labels": [], "created": "2025-10-13T04:58:26.000+0000", "updated": "2025-10-13T05:01:29.000+0000", "description": "*Problem*\r\nWhen running *{{:core:compileScala}}* under {*}{{keepAliveMode=SESSION}}{*}, joint compilation adds a non-existent directory ({_}{{.../core/build/classes/java/main}}{_}) to {{{}javac{}}}\u2019s classpath.\r\nThis triggers:\r\n{code:java}\r\nUnexpected javac output: warning: [path] bad path element \".../core/build/classes/java/main\": no such file or directory\r\nerror: warnings found and -Werror specified {code}\r\nThe same task succeeds under {{{}keepAliveMode=DAEMON{}}}.\r\n\r\n*Current Workaround*\r\nAdded a scoped compiler flag ({{{}-Xlint:-path{}}}) only under {{keepAliveMode=SESSION}}\u00a0to suppress the {{[path]}} warning and allow the build to succeed.\r\n\r\n*Steps to Reproduce*\r\n{code:java}\r\n./gradlew clean :core:compileScala -PkeepAliveMode=session \u00a0 # fails before workaround \u00a0\r\n./gradlew clean :core:compileScala -PkeepAliveMode=daemon \u00a0 \u00a0# succeeds \u00a0 {code}\r\n*Goal of This Ticket*\r\nIdentify why SESSION-mode joint compilation includes the missing {{classes/java/main}} entry in {{{}javac{}}}\u2019s classpath, align its behavior with DAEMON mode, and remove the temporary suppression once the classpath is corrected.", "comments": [], "derived_tasks": {"summarization": "Session mode compileScala reports missing classes java main directory", "classification": "feature", "qna": {"question": "What is the issue 'Session mode compileScala reports missing classes java main directory' about?", "answer": "*Problem*\r\nWhen running *{{:core:compileScala}}* under {*}{{keepAliveMode=SESSION}}{*}, joint compilation adds a non-existent directory ({_}{{.../core/build/classes/java/main}}{_}) to {{{}javac{}}}\u2019s classpath.\r\nThis triggers:\r\n{code:java}\r\nUnexpected javac output: warning: [path] bad path element \".../core/build/classes/java/main\": no such file or directory\r\nerror: warnings found and -Werror specified {code}\r\nThe same task succeeds under {{{}keepAliveMode=DAEMON{}}}.\r\n\r\n*Current Workaround*\r\nAdded a scoped compiler flag ({{{}-Xlint:-path{}}}) only under {{keepAliveMode=SESSION}}\u00a0to suppress the {{[path]}} warning and allow the build to succeed.\r\n\r\n*Steps to Reproduce*\r\n{code:java}\r\n./gradlew clean :core:compileScala -PkeepAliveMode=session \u00a0 # fails before workaround \u00a0\r\n./gradlew clean :core:compileScala -PkeepAliveMode=daemon \u00a0 \u00a0# succeeds \u00a0 {code}\r\n*Goal of This Ticket*\r\nIdentify why SESSION-mode joint compilation includes the missing {{classes/java/main}} entry in {{{}javac{}}}\u2019s classpath, align its behavior with DAEMON mode, and remove the temporary suppression once the classpath is corrected."}}}
{"project": "KAFKA", "issue_key": "KAFKA-19785", "title": "Two Kafka brokers were not active in 3 node cluster setup", "status": "Open", "reporter": "Sravani", "assignee": null, "priority": "Major", "labels": [], "created": "2025-10-13T04:25:53.000+0000", "updated": "2025-10-13T04:25:53.000+0000", "description": "Hi Team,\r\n\r\nWe were facing kafka issue where two of the kafka brokers were fenced and Kafka was not able to process messages. We are using Kafka 4.0.0 version. Below are the errors.\r\n\r\n\u00a0\r\n\r\nSep 22 09:41:42 s99ossv152 kafka[42245]: [2025-09-22 07:41:42,419] ERROR Encountered fatal fault: Unexpected error in raft IO thread (org.apache.kafka.server.fault.ProcessTerminatingFaultHandler)\r\nSep 22 09:41:42 s99ossv152 kafka[42245]: java.lang.IllegalStateException: Received request or response with leader OptionalInt[3] and epoch 55 *which is inconsistent with current leader* OptionalInt.empty and epoch 55\r\nSep 22 09:41:42 s99ossv152 kafka[42245]: #011at org.apache.kafka.raft.KafkaRaftClient.maybeTransition(KafkaRaftClient.java:2528) ~[kafka-raft-4.0.0.jar:?]\r\nSep 22 09:41:42 s99ossv152 kafka[42245]: #011at org.apache.kafka.raft.KafkaRaftClient.maybeHandleCommonResponse(KafkaRaftClient.java:2484) ~[kafka-raft-4.0.0.jar:?]\r\nSep 22 09:41:42 s99ossv152 kafka[42245]: #011at org.apache.kafka.raft.KafkaRaftClient.handleFetchResponse(KafkaRaftClient.java:1707) ~[kafka-raft-4.0.0.jar:?]\r\nSep 22 09:41:42 s99ossv152 kafka[42245]: #011at org.apache.kafka.raft.KafkaRaftClient.handleResponse(KafkaRaftClient.java:2568) ~[kafka-raft-4.0.0.jar:?]\r\nSep 22 09:41:42 s99ossv152 kafka[42245]: #011at org.apache.kafka.raft.KafkaRaftClient.handleInboundMessage(KafkaRaftClient.java:2724) ~[kafka-raft-4.0.0.jar:?]\r\nSep 22 09:41:42 s99ossv152 kafka[42245]: #011at org.apache.kafka.raft.KafkaRaftClient.poll(KafkaRaftClient.java:3460) ~[kafka-raft-4.0.0.jar:?]\r\nSep 22 09:41:42 s99ossv152 kafka[42245]: #011at org.apache.kafka.raft.KafkaRaftClientDriver.doWork(KafkaRaftClientDriver.java:64) [kafka-raft-4.0.0.jar:?]\r\nSep 22 09:41:42 s99ossv152 kafka[42245]: #011at org.apache.kafka.server.util.ShutdownableThread.run(ShutdownableThread.java:136) [kafka-server-common-4.0.0.jar:?]\r\n\r\nBelow metrics shows Fenceborker count as 2.0\r\n\r\nkafka_controller_KafkaController_Value\\{name=\"ActiveBrokerCount\",} 1.0\r\nkafka_controller_KafkaController_Value\\{name=\"GlobalTopicCount\",} 23.0\r\nkafka_controller_KafkaController_Value{name=\"{*}FencedBrokerCount{*}\",} 2.0\r\n\r\nPlease help us to resolve this issue.", "comments": [], "derived_tasks": {"summarization": "Two Kafka brokers were not active in 3 node cluster setup", "classification": "feature", "qna": {"question": "What is the issue 'Two Kafka brokers were not active in 3 node cluster setup' about?", "answer": "Hi Team,\r\n\r\nWe were facing kafka issue where two of the kafka brokers were fenced and Kafka was not able to process messages. We are using Kafka 4.0.0 version. Below are the errors.\r\n\r\n\u00a0\r\n\r\nSep 22 09:41:42 s99ossv152 kafka[42245]: [2025-09-22 07:41:42,419] ERROR Encountered fatal fault: Unexpected error in raft IO thread (org.apache.kafka.server.fault.ProcessTerminatingFaultHandler)\r\nSep 22 09:41:42 s99ossv152 kafka[42245]: java.lang.IllegalStateException: Received request or response with leader OptionalInt[3] and epoch 55 *which is inconsistent with current leader* OptionalInt.empty and epoch 55\r\nSep 22 09:41:42 s99ossv152 kafka[42245]: #011at org.apache.kafka.raft.KafkaRaftClient.maybeTransition(KafkaRaftClient.java:2528) ~[kafka-raft-4.0.0.jar:?]\r\nSep 22 09:41:42 s99ossv152 kafka[42245]: #011at org.apache.kafka.raft.KafkaRaftClient.maybeHandleCommonResponse(KafkaRaftClient.java:2484) ~[kafka-raft-4.0.0.jar:?]\r\nSep 22 09:41:42 s99ossv152 kafka[42245]: #011at org.apache.kafka.raft.KafkaRaftClient.handleFetchResponse(KafkaRaftClient.java:1707) ~[kafka-raft-4.0.0.jar:?]\r\nSep 22 09:41:42 s99ossv152 kafka[42245]: #011at org.apache.kafka.raft.KafkaRaftClient.handleResponse(KafkaRaftClient.java:2568) ~[kafka-raft-4.0.0.jar:?]\r\nSep 22 09:41:42 s99ossv152 kafka[42245]: #011at org.apache.kafka.raft.KafkaRaftClient.handleInboundMessage(KafkaRaftClient.java:2724) ~[kafka-raft-4.0.0.jar:?]\r\nSep 22 09:41:42 s99ossv152 kafka[42245]: #011at org.apache.kafka.raft.KafkaRaftClient.poll(KafkaRaftClient.java:3460) ~[kafka-raft-4.0.0.jar:?]\r\nSep 22 09:41:42 s99ossv152 kafka[42245]: #011at org.apache.kafka.raft.KafkaRaftClientDriver.doWork(KafkaRaftClientDriver.java:64) [kafka-raft-4.0.0.jar:?]\r\nSep 22 09:41:42 s99ossv152 kafka[42245]: #011at org.apache.kafka.server.util.ShutdownableThread.run(ShutdownableThread.java:136) [kafka-server-common-4.0.0.jar:?]\r\n\r\nBelow metrics shows Fenceborker count as 2.0\r\n\r\nkafka_controller_KafkaController_Value\\{name=\"ActiveBrokerCount\",} 1.0\r\nkafka_controller_KafkaController_Value\\{name=\"GlobalTopicCount\",} 23.0\r\nkafka_controller_KafkaController_Value{name=\"{*}FencedBrokerCount{*}\",} 2.0\r\n\r\nPlease help us to resolve this issue."}}}
{"project": "KAFKA", "issue_key": "KAFKA-19784", "title": "Expose Rack ID in MemberDescription", "status": "Resolved", "reporter": "fujian", "assignee": "fujian", "priority": "Major", "labels": ["needs-kip"], "created": "2025-10-12T09:34:16.000+0000", "updated": "2025-10-27T14:09:27.000+0000", "description": "Currently, the {{{}AdminClient{}}}\u2019s {{describeConsumerGroups}} API returns a {{MemberDescription}} that does *not* include rack information, even though the underlying {{ConsumerGroupDescribeResponse}} protocol already supports a {{rackId}} field.\r\nThis causes users to be unable to retrieve member rack information through the Admin API.\r\n\r\nRack information is crucial for:\r\n * Monitoring and visualization tools\r\n\r\n * Operational analysis of rack distribution\r\n\r\n * Diagnosing rack-aware assignment issues\r\n\r\nIn addition, StreamsGroupMemberDescription already includes the rackId, so adding it here would also make the behavior more consistent.\r\n\r\nThe PR: \u00a0[https://github.com/apache/kafka/pull/20691]\r\n\r\nThe KIP: https://cwiki.apache.org/confluence/display/KAFKA/KIP-1227%3A+Expose+Rack+ID+in+MemberDescription+and+ShareMemberDescription", "comments": [], "derived_tasks": {"summarization": "Expose Rack ID in MemberDescription", "classification": "feature", "qna": {"question": "What is the issue 'Expose Rack ID in MemberDescription' about?", "answer": "Currently, the {{{}AdminClient{}}}\u2019s {{describeConsumerGroups}} API returns a {{MemberDescription}} that does *not* include rack information, even though the underlying {{ConsumerGroupDescribeResponse}} protocol already supports a {{rackId}} field.\r\nThis causes users to be unable to retrieve member rack information through the Admin API.\r\n\r\nRack information is crucial for:\r\n * Monitoring and visualization tools\r\n\r\n * Operational analysis of rack distribution\r\n\r\n * Diagnosing rack-aware assignment issues\r\n\r\nIn addition, StreamsGroupMemberDescription already includes the rackId, so adding it here would also make the behavior more consistent.\r\n\r\nThe PR: \u00a0[https://github.com/apache/kafka/pull/20691]\r\n\r\nThe KIP: https://cwiki.apache.org/confluence/display/KAFKA/KIP-1227%3A+Expose+Rack+ID+in+MemberDescription+and+ShareMemberDescription"}}}
{"project": "KAFKA", "issue_key": "KAFKA-19783", "title": "Custom Gradle tasks (`unitTest` and `integrationTest`) are not executing tests in Gradle 9", "status": "Resolved", "reporter": "Dejan Stojadinovi\u0107", "assignee": "Dejan Stojadinovi\u0107", "priority": "Minor", "labels": ["Gradle", "build", "build-problem", "integration-tests", "regresion", "regression", "regresssion", "test", "unit-tests"], "created": "2025-10-12T09:06:37.000+0000", "updated": "2025-10-12T10:14:34.000+0000", "description": "Prologue: KAFKA-19174\r\n\r\n\r\n\r\n\r\nSee here for description: https://github.com/apache/kafka/pull/19513#issuecomment-3388760324", "comments": [], "derived_tasks": {"summarization": "Custom Gradle tasks (`unitTest` and `integrationTest`) are not executing tests in Gradle 9", "classification": "feature", "qna": {"question": "What is the issue 'Custom Gradle tasks (`unitTest` and `integrationTest`) are not executing tests in Gradle 9' about?", "answer": "Prologue: KAFKA-19174\r\n\r\n\r\n\r\n\r\nSee here for description: https://github.com/apache/kafka/pull/19513#issuecomment-3388760324"}}}
{"project": "KAFKA", "issue_key": "KAFKA-19782", "title": "Improve perfomance Authorizer.authorizeByResourceType by using Prefix Trie", "status": "Open", "reporter": "Evgeny Kuvardin", "assignee": null, "priority": "Minor", "labels": [], "created": "2025-10-11T16:15:42.000+0000", "updated": "2025-10-16T10:12:08.000+0000", "description": "Class Authorizer.authorizeByResourceType under the hood uses HashMap for \r\ncheck if the caller is authorized to perform the\u00a0given ACL operation on at least one resource of the given type.\r\n\r\nIt check each character in allowPatterns and pass to deny patterns\r\n\r\n{code:java}\r\n// For any literal allowed, if there's no dominant literal and prefix denied, return allow.\r\n        // For any prefix allowed, if there's no dominant prefix denied, return allow.\r\n        for (Map.Entry<PatternType, Set<String>> entry : allowPatterns.entrySet()) {\r\n            for (String allowStr : entry.getValue()) {\r\n                if (entry.getKey() == PatternType.LITERAL\r\n                        && denyPatterns.get(PatternType.LITERAL).contains(allowStr))\r\n                    continue;\r\n                StringBuilder sb = new StringBuilder();\r\n                boolean hasDominatedDeny = false;\r\n                for (char ch : allowStr.toCharArray()) {\r\n                    sb.append(ch);\r\n                    if (denyPatterns.get(PatternType.PREFIXED).contains(sb.toString())) {\r\n                        hasDominatedDeny = true;\r\n                        break;\r\n                    }\r\n                }\r\n                if (!hasDominatedDeny)\r\n                    return AuthorizationResult.ALLOWED;\r\n            }\r\n        }\r\n{code}\r\n\r\nTo improve performance better use Prefix Tree\r\n", "comments": [], "derived_tasks": {"summarization": "Improve perfomance Authorizer.authorizeByResourceType by using Prefix Trie", "classification": "feature", "qna": {"question": "What is the issue 'Improve perfomance Authorizer.authorizeByResourceType by using Prefix Trie' about?", "answer": "Class Authorizer.authorizeByResourceType under the hood uses HashMap for \r\ncheck if the caller is authorized to perform the\u00a0given ACL operation on at least one resource of the given type.\r\n\r\nIt check each character in allowPatterns and pass to deny patterns\r\n\r\n{code:java}\r\n// For any literal allowed, if there's no dominant literal and prefix denied, return allow.\r\n        // For any prefix allowed, if there's no dominant prefix denied, return allow.\r\n        for (Map.Entry<PatternType, Set<String>> entry : allowPatterns.entrySet()) {\r\n            for (String allowStr : entry.getValue()) {\r\n                if (entry.getKey() == PatternType.LITERAL\r\n                        && denyPatterns.get(PatternType.LITERAL).contains(allowStr))\r\n                    continue;\r\n                StringBuilder sb = new StringBuilder();\r\n                boolean hasDominatedDeny = false;\r\n                for (char ch : allowStr.toCharArray()) {\r\n                    sb.append(ch);\r\n                    if (denyPatterns.get(PatternType.PREFIXED).contains(sb.toString())) {\r\n                        hasDominatedDeny = true;\r\n                        break;\r\n                    }\r\n                }\r\n                if (!hasDominatedDeny)\r\n                    return AuthorizationResult.ALLOWED;\r\n            }\r\n        }\r\n{code}\r\n\r\nTo improve performance better use Prefix Tree\r\n"}}}
{"project": "KAFKA", "issue_key": "KAFKA-19781", "title": "Consumer NoOffsetForPartitionException for partitions being revoked", "status": "Open", "reporter": "Lianet Magrans", "assignee": "Lianet Magrans", "priority": "Major", "labels": [], "created": "2025-10-10T15:04:28.000+0000", "updated": "2025-10-14T16:20:59.000+0000", "description": "Currently, the consumer attempts to update positions for all partitions it owns that don't have a position, even when the partition may be already being revoked. This could lead to NoOffsetForPartitionException for such partitions (if there is no reset strategy defined). There are 2 main issues around this:\r\n * is probably unneeded work to fetch offsets and update positions for partitions being revoked. At that point we're actually not allowing fetching from there anymore\r\n * throwing NoOffsetForPartitionException may lead applications to take action to set positions themselves, which will most probably fail (the partition will most probably not owned by the consumer anymore since it was already being revoked when the NoOffsetForPartitionException was generated). We've seen this on Kafka Streams, that handled NoOffsetForPartitionException by calling seek to set positions.\u00a0 \u00a0\r\n\r\nThis task is to review if there are no other implications I may be missing? Then fix to ensure we don't update positions for partitions being revoked (aligned with how we don't allow fetching from them)\r\n\r\n\u00a0", "comments": [], "derived_tasks": {"summarization": "Consumer NoOffsetForPartitionException for partitions being revoked", "classification": "feature", "qna": {"question": "What is the issue 'Consumer NoOffsetForPartitionException for partitions being revoked' about?", "answer": "Currently, the consumer attempts to update positions for all partitions it owns that don't have a position, even when the partition may be already being revoked. This could lead to NoOffsetForPartitionException for such partitions (if there is no reset strategy defined). There are 2 main issues around this:\r\n * is probably unneeded work to fetch offsets and update positions for partitions being revoked. At that point we're actually not allowing fetching from there anymore\r\n * throwing NoOffsetForPartitionException may lead applications to take action to set positions themselves, which will most probably fail (the partition will most probably not owned by the consumer anymore since it was already being revoked when the NoOffsetForPartitionException was generated). We've seen this on Kafka Streams, that handled NoOffsetForPartitionException by calling seek to set positions.\u00a0 \u00a0\r\n\r\nThis task is to review if there are no other implications I may be missing? Then fix to ensure we don't update positions for partitions being revoked (aligned with how we don't allow fetching from them)\r\n\r\n\u00a0"}}}
{"project": "KAFKA", "issue_key": "KAFKA-19780", "title": "Create a CI to verify all gradle tasks", "status": "Open", "reporter": "Chia-Ping Tsai", "assignee": "Chia-Ping Tsai", "priority": "Minor", "labels": [], "created": "2025-10-10T13:27:53.000+0000", "updated": "2025-10-10T13:27:53.000+0000", "description": "see https://github.com/apache/kafka/pull/19513#issuecomment-3390047209", "comments": [], "derived_tasks": {"summarization": "Create a CI to verify all gradle tasks", "classification": "feature", "qna": {"question": "What is the issue 'Create a CI to verify all gradle tasks' about?", "answer": "see https://github.com/apache/kafka/pull/19513#issuecomment-3390047209"}}}
{"project": "KAFKA", "issue_key": "KAFKA-19779", "title": "Relax offset commit validation to allow member epochs since assignment", "status": "Patch Available", "reporter": "Lucas Brutschy", "assignee": "Lucas Brutschy", "priority": "Major", "labels": [], "created": "2025-10-10T12:30:03.000+0000", "updated": "2025-10-23T15:10:11.000+0000", "description": "h2. Fencing offset commits\r\n\r\nIn the Kafka protocol, when a consumer commits offsets or a producer tries to add offsets to a transaction, it includes its epoch/generation of the consumer group. The point of this is for the group coordinator to fence against zombie commit requests, that is, commit requests that include an offset for a partition that was since reassigned to a different member. If such a guard was not in place, a zombie offset commit may overwrite offsets of the new owner, or its offsets may be committed to the consumer offset topic but not be included in the result of the new owners offset fetch request.\r\n\r\nIn KIP-848, when receiving an offset commit request that includes the client-side member epoch and a member ID, the group coordinator performs the check\r\n\r\n{{Client-Side Member Epoch == Broker-Side Member Epoch}}\r\n\r\nand, if the check fails, returns a {{STALE_MEMBER_EPOCH}} error for regular offset commits and a {{ILLEGAL_GENERATION}} for transactional offset commits. If the member epoch sent in the request is the current broker-side member epoch, KIP-848 guarantees that the partition cannot also be owned by a different member at the same or a larger epoch. Therefore, this is sufficient for fencing zombie commits. Note that we assume zombie commits will always contain offsets for partitions that were owned by the member at the member epoch sent in the request. Commit requests that commit offsets for partitions that are _not_ owned by the member in that epoch, are not possible in a correct client-side implementation of the protocol.\r\n\r\nNote that the broker-side member epoch is not the group epoch or the target assignment epoch. For details, see KIP-848. Note also that commits can also be fenced because a member falls out of the group (e.g. because it does not revoke partitions within the rebalance timeout). At this point, its commits will be fenced solely based on the member ID (which is not part of the group anymore). We therefore ignore this case in this document, and only consider zombie commits from members that are still part of the group.\r\nh2. Downsides of the current approach\r\n\r\nThis fencing is, however, unnecessarily strict. Assume, for example, a member owns P1 at epoch 1. The broker-side member epoch is bumped to 2, but the member still has P1 assigned at epoch 2. The member may not learn about the new broker-side member epoch in time, and submit an offset commit commit for P1 with epoch 1. This is not a zombie commit request as define above (because P1 was not reassigned to a different member), but it will still be rejected by a KIP-848 group coordinator.\r\n\r\nThe trouble with this fencing mechanism is that it is very difficult to avoid the broker-side member epoch being bumped concurrently with an offset commit. Seen from the client-side, the broker-side member epoch may be bumped at any time while a heartbeat to the group coordinator is in-flight. To make sure the member epoch sent in an offset commit request is up-to-date would require making sure that no consumer group or streams group heartbeat request is in-flight at the same time.\r\nh2. Why a broker-side fix is warranted\r\n\r\nThis problem is particularly challenging to solve on the client side for transactional offset commits, since they are performed by the producer, not the consumer, and the producer has no way of knowing when a consumer group heartbeat or streams group heartbeat is in-flight. The member epoch is passed from the Java consumer to the Java producer using the {{ConsumerGroupMetadata}} object, which is passed into {{{}sendOffsetsToTransaction{}}}. By the time the transactional offset commit is sent, the member epoch may be stale, the broker will return an {{ILLEGAL_GENERATION}} exception. This will force the Java producer into an abortable error state, surfacing the error as a {{CommitFailedException}} to the user, the user has no other way to recover from this to abort the transaction.\r\n\r\nThis may hurt in any Kafka client application, since aborting transactions means throwing away work and restarting from an earlier point. But it is a particularly big problem in Kafka Streams with exactly-once semantics, where aborting a transaction usually means wiping and restoring the state store, so each aborted transaction means some downtime for apps using state stores of non-negligible size. Furthermore, since Kafka Streams commits every 100ms by default in EOS, this is likely to happen fairly often.\r\nh1. Conceptual Design\r\n\r\nn this design document, we therefore propose to relax the condition for offset commit fencing.\r\nh2. Identifying zombies using the last epoch a partition was assigned to the member\r\n\r\nTo derive a more relaxed check, we need to identify an epoch which separates zombie commits from commits of the current owner. As mentioned above, zombie commit requests are commit requests that include a partition, member ID and member epoch combination, so that the member owned the partition at that epoch. However, the partition has since been reassigned to a different member.\r\n\r\nMost precisely, on the level of a single partition, a relaxed offset commit check can be defined using a *assignment epoch* for each assigned partition and each member, which is the epoch at which the partition was assigned to that member. To fence from zombie commit requests, we can reject all offset commit requests from a member that either does not have the partition assigned, or that includes any member epoch that is smaller or equal than the assignment epoch for that member and that partition.\r\n\r\nAssignment Epoch <= Client-Side Member Epoch <= Broker-Side Member Epoch\r\n\r\nThe correctness of this is obvious: all commits of the current partition owner will be accepted, since the Client-Side Member Epoch of the current owner must always have an epoch that is larger or equal than the assignment epoch (a partition that is revoked in one epoch is never reassigned in the same epoch). It will also correctly reject any zombie commits from that member, because if a partition was owned by the member A at Client-Side Member Epoch (which we assume for zombie commits), but it was reassigned to member B since, we have two possible cases:\r\n # Member A currently does not have the partition assigned\r\n\r\n # Member A does currently have the partition assigned, but then it must have been reassigned to member A after being assigned to member B. By KIP-848 this cannot all happen in the same epoch, so we must have Assignment Epoch > Client-Side Member Epoch.\r\n\r\nh3. Differences to the design above\r\n\r\nThis design does not need to disable commits on the client-side. The need to disable commits came from the fact that we are tracking epochs \u201cimprecisely\u201d, on the member-level and not on the partition-level. So in the RevocationEpoch design, when we have just revoked P2 on the client, we may attempt to commit a partition P1, triggering the race condition because the broker can concurrently bump the revocation epoch for that member because of the revocation of P2. We prevent this by disabling commits while a partition is revoked, and by \u201cwhile a partition is revoked\u201d I mean the timeframe from executing the revocation on the client, and seeing the following epoch bump on the client. In the partition-level AssignmentEpoch design, if we are committing P1, we are still convinced that we own P1, so we must also still own it on the broker. At the same time, we may remove the assignment epoch for P2 on the broker, but it doesn\u2019t matter since this doesn\u2019t impact whether we can commit P1, and we are not going to try to commit P2 after having revoked it on the client side.\r\nh1. Proposed Changes\r\nh3. Introducing Per-Member and Per-Partition Assignment Epoch\r\n\r\nWe extend the model of a consumer group / streams group member with one integer per assigned partition for each member of a group. This includes both partitions directly assigned to the member, and partitions pending revocation. The assignment epoch is set to the epoch in which the partition was assigned to the member, and we have the invariant Assignment Epoch <= MemberEpoch <= TargetAssignmentEpoch <= GroupEpoch.\r\n\r\nThe AssignmentEpoch is added as a field to TopicPartitions in ConsumerGroupCurrentMemberAssignmentValue, so that it can be stored and replayed from the committed offsets topic.\r\n\r\nFor streams groups, we will use the same logic but add assignment epochs only for active tasks in StreamsGroupCurrentMemberAssignmentValue, since only active tasks commit offsets in Kafka Streams.\r\nh3. Relaxing the offset commit validation\r\n\r\nWe replace the current check offset commit validation check\r\n\r\nClient-Side Member Epoch == Broker-Side Member Epoch\r\n\r\nby\r\n\r\nAssignment Epoch <= Client-Side Member Epoch <= Broker-Side Member Epoch\r\n\r\nwhere, for simplicity, we can assume the assignment epoch of a partition that is not assigned to that member to be Integer.maxValue.", "comments": [], "derived_tasks": {"summarization": "Relax offset commit validation to allow member epochs since assignment", "classification": "feature", "qna": {"question": "What is the issue 'Relax offset commit validation to allow member epochs since assignment' about?", "answer": "h2. Fencing offset commits\r\n\r\nIn the Kafka protocol, when a consumer commits offsets or a producer tries to add offsets to a transaction, it includes its epoch/generation of the consumer group. The point of this is for the group coordinator to fence against zombie commit requests, that is, commit requests that include an offset for a partition that was since reassigned to a different member. If such a guard was not in place, a zombie offset commit may overwrite offsets of the new owner, or its offsets may be committed to the consumer offset topic but not be included in the result of the new owners offset fetch request.\r\n\r\nIn KIP-848, when receiving an offset commit request that includes the client-side member epoch and a member ID, the group coordinator performs the check\r\n\r\n{{Client-Side Member Epoch == Broker-Side Member Epoch}}\r\n\r\nand, if the check fails, returns a {{STALE_MEMBER_EPOCH}} error for regular offset commits and a {{ILLEGAL_GENERATION}} for transactional offset commits. If the member epoch sent in the request is the current broker-side member epoch, KIP-848 guarantees that the partition cannot also be owned by a different member at the same or a larger epoch. Therefore, this is sufficient for fencing zombie commits. Note that we assume zombie commits will always contain offsets for partitions that were owned by the member at the member epoch sent in the request. Commit requests that commit offsets for partitions that are _not_ owned by the member in that epoch, are not possible in a correct client-side implementation of the protocol.\r\n\r\nNote that the broker-side member epoch is not the group epoch or the target assignment epoch. For details, see KIP-848. Note also that commits can also be fenced because a member falls out of the group (e.g. because it does not revoke partitions within the rebalance timeout). At this point, its commits will be fenced solely based on the member ID (which is not part of the group anymore). We therefore ignore this case in this document, and only consider zombie commits from members that are still part of the group.\r\nh2. Downsides of the current approach\r\n\r\nThis fencing is, however, unnecessarily strict. Assume, for example, a member owns P1 at epoch 1. The broker-side member epoch is bumped to 2, but the member still has P1 assigned at epoch 2. The member may not learn about the new broker-side member epoch in time, and submit an offset commit commit for P1 with epoch 1. This is not a zombie commit request as define above (because P1 was not reassigned to a different member), but it will still be rejected by a KIP-848 group coordinator.\r\n\r\nThe trouble with this fencing mechanism is that it is very difficult to avoid the broker-side member epoch being bumped concurrently with an offset commit. Seen from the client-side, the broker-side member epoch may be bumped at any time while a heartbeat to the group coordinator is in-flight. To make sure the member epoch sent in an offset commit request is up-to-date would require making sure that no consumer group or streams group heartbeat request is in-flight at the same time.\r\nh2. Why a broker-side fix is warranted\r\n\r\nThis problem is particularly challenging to solve on the client side for transactional offset commits, since they are performed by the producer, not the consumer, and the producer has no way of knowing when a consumer group heartbeat or streams group heartbeat is in-flight. The member epoch is passed from the Java consumer to the Java producer using the {{ConsumerGroupMetadata}} object, which is passed into {{{}sendOffsetsToTransaction{}}}. By the time the transactional offset commit is sent, the member epoch may be stale, the broker will return an {{ILLEGAL_GENERATION}} exception. This will force the Java producer into an abortable error state, surfacing the error as a {{CommitFailedException}} to the user, the user has no other way to recover from this to abort the transaction.\r\n\r\nThis may hurt in any Kafka client application, since aborting transactions means throwing away work and restarting from an earlier point. But it is a particularly big problem in Kafka Streams with exactly-once semantics, where aborting a transaction usually means wiping and restoring the state store, so each aborted transaction means some downtime for apps using state stores of non-negligible size. Furthermore, since Kafka Streams commits every 100ms by default in EOS, this is likely to happen fairly often.\r\nh1. Conceptual Design\r\n\r\nn this design document, we therefore propose to relax the condition for offset commit fencing.\r\nh2. Identifying zombies using the last epoch a partition was assigned to the member\r\n\r\nTo derive a more relaxed check, we need to identify an epoch which separates zombie commits from commits of the current owner. As mentioned above, zombie commit requests are commit requests that include a partition, member ID and member epoch combination, so that the member owned the partition at that epoch. However, the partition has since been reassigned to a different member.\r\n\r\nMost precisely, on the level of a single partition, a relaxed offset commit check can be defined using a *assignment epoch* for each assigned partition and each member, which is the epoch at which the partition was assigned to that member. To fence from zombie commit requests, we can reject all offset commit requests from a member that either does not have the partition assigned, or that includes any member epoch that is smaller or equal than the assignment epoch for that member and that partition.\r\n\r\nAssignment Epoch <= Client-Side Member Epoch <= Broker-Side Member Epoch\r\n\r\nThe correctness of this is obvious: all commits of the current partition owner will be accepted, since the Client-Side Member Epoch of the current owner must always have an epoch that is larger or equal than the assignment epoch (a partition that is revoked in one epoch is never reassigned in the same epoch). It will also correctly reject any zombie commits from that member, because if a partition was owned by the member A at Client-Side Member Epoch (which we assume for zombie commits), but it was reassigned to member B since, we have two possible cases:\r\n # Member A currently does not have the partition assigned\r\n\r\n # Member A does currently have the partition assigned, but then it must have been reassigned to member A after being assigned to member B. By KIP-848 this cannot all happen in the same epoch, so we must have Assignment Epoch > Client-Side Member Epoch.\r\n\r\nh3. Differences to the design above\r\n\r\nThis design does not need to disable commits on the client-side. The need to disable commits came from the fact that we are tracking epochs \u201cimprecisely\u201d, on the member-level and not on the partition-level. So in the RevocationEpoch design, when we have just revoked P2 on the client, we may attempt to commit a partition P1, triggering the race condition because the broker can concurrently bump the revocation epoch for that member because of the revocation of P2. We prevent this by disabling commits while a partition is revoked, and by \u201cwhile a partition is revoked\u201d I mean the timeframe from executing the revocation on the client, and seeing the following epoch bump on the client. In the partition-level AssignmentEpoch design, if we are committing P1, we are still convinced that we own P1, so we must also still own it on the broker. At the same time, we may remove the assignment epoch for P2 on the broker, but it doesn\u2019t matter since this doesn\u2019t impact whether we can commit P1, and we are not going to try to commit P2 after having revoked it on the client side.\r\nh1. Proposed Changes\r\nh3. Introducing Per-Member and Per-Partition Assignment Epoch\r\n\r\nWe extend the model of a consumer group / streams group member with one integer per assigned partition for each member of a group. This includes both partitions directly assigned to the member, and partitions pending revocation. The assignment epoch is set to the epoch in which the partition was assigned to the member, and we have the invariant Assignment Epoch <= MemberEpoch <= TargetAssignmentEpoch <= GroupEpoch.\r\n\r\nThe AssignmentEpoch is added as a field to TopicPartitions in ConsumerGroupCurrentMemberAssignmentValue, so that it can be stored and replayed from the committed offsets topic.\r\n\r\nFor streams groups, we will use the same logic but add assignment epochs only for active tasks in StreamsGroupCurrentMemberAssignmentValue, since only active tasks commit offsets in Kafka Streams.\r\nh3. Relaxing the offset commit validation\r\n\r\nWe replace the current check offset commit validation check\r\n\r\nClient-Side Member Epoch == Broker-Side Member Epoch\r\n\r\nby\r\n\r\nAssignment Epoch <= Client-Side Member Epoch <= Broker-Side Member Epoch\r\n\r\nwhere, for simplicity, we can assume the assignment epoch of a partition that is not assigned to that member to be Integer.maxValue."}}}
{"project": "KAFKA", "issue_key": "KAFKA-19778", "title": "Share Partition Lag Persistence and Retrieval", "status": "Open", "reporter": "Chirag Wadhwa", "assignee": "Chirag Wadhwa", "priority": "Minor", "labels": [], "created": "2025-10-10T07:59:04.000+0000", "updated": "2025-10-19T19:31:16.000+0000", "description": "Ticket for KIP-1226. The KIP proposes introducing the concept of lag for share partitions, which will be persisted in the __share_group_state topic and can be retrieved using admin.listShareGroupOffsets", "comments": [], "derived_tasks": {"summarization": "Share Partition Lag Persistence and Retrieval", "classification": "feature", "qna": {"question": "What is the issue 'Share Partition Lag Persistence and Retrieval' about?", "answer": "Ticket for KIP-1226. The KIP proposes introducing the concept of lag for share partitions, which will be persisted in the __share_group_state topic and can be retrieved using admin.listShareGroupOffsets"}}}
{"project": "KAFKA", "issue_key": "KAFKA-19777", "title": "Generator | Fix order of arguments to assertEquals in unit tests", "status": "Open", "reporter": "Ksolves India Limited", "assignee": "Ksolves India Limited", "priority": "Trivial", "labels": [], "created": "2025-10-10T07:27:28.000+0000", "updated": "2025-10-26T15:59:06.000+0000", "description": "This sub-task is intended to fix the order of arguments passed in assertions in test cases within the generator package.", "comments": [], "derived_tasks": {"summarization": "Generator | Fix order of arguments to assertEquals in unit tests", "classification": "feature", "qna": {"question": "What is the issue 'Generator | Fix order of arguments to assertEquals in unit tests' about?", "answer": "This sub-task is intended to fix the order of arguments passed in assertions in test cases within the generator package."}}}
{"project": "KAFKA", "issue_key": "KAFKA-19776", "title": "Incorrect leaderId and leaderEpoch logged in Partition#makeFollower", "status": "Resolved", "reporter": "Gaurav Narula", "assignee": "Gaurav Narula", "priority": "Major", "labels": [], "created": "2025-10-09T10:17:50.000+0000", "updated": "2025-10-10T17:15:53.000+0000", "description": "[Partition::makeFollower|https://github.com/apache/kafka/blob/be3e40c45ac9a70833ce0d5735ad141a5bd24627/core/src/main/scala/kafka/cluster/Partition.scala#L877] logs the existing leader-id and existing leader epoch if the partition registration has a leader epoch > the existing leader epoch.\r\n\r\nThere's a bug in the logging as we update the existing leaderId and leaderEpoch with the latest values from partition registration prior to logging them, thereby never logging the existing values.\r\n\r\n", "comments": [], "derived_tasks": {"summarization": "Incorrect leaderId and leaderEpoch logged in Partition#makeFollower", "classification": "feature", "qna": {"question": "What is the issue 'Incorrect leaderId and leaderEpoch logged in Partition#makeFollower' about?", "answer": "[Partition::makeFollower|https://github.com/apache/kafka/blob/be3e40c45ac9a70833ce0d5735ad141a5bd24627/core/src/main/scala/kafka/cluster/Partition.scala#L877] logs the existing leader-id and existing leader epoch if the partition registration has a leader epoch > the existing leader epoch.\r\n\r\nThere's a bug in the logging as we update the existing leaderId and leaderEpoch with the latest values from partition registration prior to logging them, thereby never logging the existing values.\r\n\r\n"}}}
{"project": "KAFKA", "issue_key": "KAFKA-19775", "title": "Error if an empty topic is created when there is a regex source KS", "status": "Resolved", "reporter": "Nikita Shupletsov", "assignee": "Nikita Shupletsov", "priority": "Major", "labels": [], "created": "2025-10-09T01:01:52.000+0000", "updated": "2025-10-16T04:42:22.000+0000", "description": "If there is a KS application that uses a regex source, and we create a new topic that matches that regex, but produce no messages, the application will get into an {{ERROR}} state.\r\n\r\n\u00a0\r\n\r\nif we take\u00a0{}[RegexSourceIntegrationTest#testRegexRecordsAreProcessedAfterNewTopicCreatedWithMultipleSubtopologies|#L206{}}}], but without producing any messages to {{TEST-TOPIC-2}} the problem will reproduce:\r\n{quote}{{org.apache.kafka.streams.errors.StreamsException: java.lang.IllegalStateException: Stream task 0_0 does not know the partition: TEST-TOPIC-2-0}}\r\n{{\u00a0 \u00a0 at org.apache.kafka.streams.processor.internals.StreamThread.runLoop(StreamThread.java:981)}}\r\n{{\u00a0 \u00a0 at org.apache.kafka.streams.processor.internals.StreamThread.run(StreamThread.java:898)}}\r\n{{Caused by: java.lang.IllegalStateException: Stream task 0_0 does not know the partition: TEST-TOPIC-2-0}}\r\n{{\u00a0 \u00a0 at org.apache.kafka.streams.processor.internals.StreamTask.findOffsetAndMetadata(StreamTask.java:480)}}\r\n{{\u00a0 \u00a0 at org.apache.kafka.streams.processor.internals.StreamTask.committableOffsetsAndMetadata(StreamTask.java:511)}}\r\n{{\u00a0 \u00a0 at org.apache.kafka.streams.processor.internals.StreamTask.prepareCommit(StreamTask.java:454)}}\r\n{{\u00a0 \u00a0 at org.apache.kafka.streams.processor.internals.TaskExecutor.commitTasksAndMaybeUpdateCommittableOffsets(TaskExecutor.java:145)}}\r\n{{\u00a0 \u00a0 at org.apache.kafka.streams.processor.internals.TaskManager.commitTasksAndMaybeUpdateCommittableOffsets(TaskManager.java:2025)}}\r\n{{\u00a0 \u00a0 at org.apache.kafka.streams.processor.internals.TaskManager.commit(TaskManager.java:1992)}}\r\n{{\u00a0 \u00a0 at org.apache.kafka.streams.processor.internals.StreamThread.maybeCommit(StreamThread.java:1836)}}\r\n{{\u00a0 \u00a0 at org.apache.kafka.streams.processor.internals.StreamThread.runOnceWithoutProcessingThreads(StreamThread.java:1288)}}\r\n{{\u00a0 \u00a0 at org.apache.kafka.streams.processor.internals.StreamThread.runLoop(StreamThread.java:938)}}\r\n{{\u00a0 \u00a0 ... 1 more}}\r\n{quote}\r\n\u00a0", "comments": [], "derived_tasks": {"summarization": "Error if an empty topic is created when there is a regex source KS", "classification": "feature", "qna": {"question": "What is the issue 'Error if an empty topic is created when there is a regex source KS' about?", "answer": "If there is a KS application that uses a regex source, and we create a new topic that matches that regex, but produce no messages, the application will get into an {{ERROR}} state.\r\n\r\n\u00a0\r\n\r\nif we take\u00a0{}[RegexSourceIntegrationTest#testRegexRecordsAreProcessedAfterNewTopicCreatedWithMultipleSubtopologies|#L206{}}}], but without producing any messages to {{TEST-TOPIC-2}} the problem will reproduce:\r\n{quote}{{org.apache.kafka.streams.errors.StreamsException: java.lang.IllegalStateException: Stream task 0_0 does not know the partition: TEST-TOPIC-2-0}}\r\n{{\u00a0 \u00a0 at org.apache.kafka.streams.processor.internals.StreamThread.runLoop(StreamThread.java:981)}}\r\n{{\u00a0 \u00a0 at org.apache.kafka.streams.processor.internals.StreamThread.run(StreamThread.java:898)}}\r\n{{Caused by: java.lang.IllegalStateException: Stream task 0_0 does not know the partition: TEST-TOPIC-2-0}}\r\n{{\u00a0 \u00a0 at org.apache.kafka.streams.processor.internals.StreamTask.findOffsetAndMetadata(StreamTask.java:480)}}\r\n{{\u00a0 \u00a0 at org.apache.kafka.streams.processor.internals.StreamTask.committableOffsetsAndMetadata(StreamTask.java:511)}}\r\n{{\u00a0 \u00a0 at org.apache.kafka.streams.processor.internals.StreamTask.prepareCommit(StreamTask.java:454)}}\r\n{{\u00a0 \u00a0 at org.apache.kafka.streams.processor.internals.TaskExecutor.commitTasksAndMaybeUpdateCommittableOffsets(TaskExecutor.java:145)}}\r\n{{\u00a0 \u00a0 at org.apache.kafka.streams.processor.internals.TaskManager.commitTasksAndMaybeUpdateCommittableOffsets(TaskManager.java:2025)}}\r\n{{\u00a0 \u00a0 at org.apache.kafka.streams.processor.internals.TaskManager.commit(TaskManager.java:1992)}}\r\n{{\u00a0 \u00a0 at org.apache.kafka.streams.processor.internals.StreamThread.maybeCommit(StreamThread.java:1836)}}\r\n{{\u00a0 \u00a0 at org.apache.kafka.streams.processor.internals.StreamThread.runOnceWithoutProcessingThreads(StreamThread.java:1288)}}\r\n{{\u00a0 \u00a0 at org.apache.kafka.streams.processor.internals.StreamThread.runLoop(StreamThread.java:938)}}\r\n{{\u00a0 \u00a0 ... 1 more}}\r\n{quote}\r\n\u00a0"}}}
{"project": "KAFKA", "issue_key": "KAFKA-19774", "title": "Mechanism to cordon brokers and log directories", "status": "Open", "reporter": "Mickael Maison", "assignee": "Mickael Maison", "priority": "Major", "labels": [], "created": "2025-10-08T16:18:56.000+0000", "updated": "2025-10-08T16:28:31.000+0000", "description": "Jira for KIP-1066: https://cwiki.apache.org/confluence/display/KAFKA/KIP-1066%3A+Mechanism+to+cordon+brokers+and+log+directories", "comments": [], "derived_tasks": {"summarization": "Mechanism to cordon brokers and log directories", "classification": "feature", "qna": {"question": "What is the issue 'Mechanism to cordon brokers and log directories' about?", "answer": "Jira for KIP-1066: https://cwiki.apache.org/confluence/display/KAFKA/KIP-1066%3A+Mechanism+to+cordon+brokers+and+log+directories"}}}
{"project": "KAFKA", "issue_key": "KAFKA-19773", "title": "Include push interval in ClientTelemetryReceiver context", "status": "Open", "reporter": "Mickael Maison", "assignee": "Maros Orsak", "priority": "Major", "labels": [], "created": "2025-10-08T15:55:54.000+0000", "updated": "2025-10-31T09:23:02.000+0000", "description": "Jira for KIP-1217\r\nhttps://cwiki.apache.org/confluence/display/KAFKA/KIP-1217%3A+Include+push+interval+in+ClientTelemetryReceiver+context", "comments": [], "derived_tasks": {"summarization": "Include push interval in ClientTelemetryReceiver context", "classification": "feature", "qna": {"question": "What is the issue 'Include push interval in ClientTelemetryReceiver context' about?", "answer": "Jira for KIP-1217\r\nhttps://cwiki.apache.org/confluence/display/KAFKA/KIP-1217%3A+Include+push+interval+in+ClientTelemetryReceiver+context"}}}
{"project": "KAFKA", "issue_key": "KAFKA-19772", "title": "enhance the documentation for `Node#isFanced`", "status": "Resolved", "reporter": "Chia-Ping Tsai", "assignee": "HongYi Chen", "priority": "Minor", "labels": [], "created": "2025-10-08T15:10:56.000+0000", "updated": "2025-10-12T15:54:53.000+0000", "description": "The `isFenced` value is only valid for broker nodes. It is always `false` for quorum controller nodes.", "comments": [], "derived_tasks": {"summarization": "enhance the documentation for `Node#isFanced`", "classification": "feature", "qna": {"question": "What is the issue 'enhance the documentation for `Node#isFanced`' about?", "answer": "The `isFenced` value is only valid for broker nodes. It is always `false` for quorum controller nodes."}}}
{"project": "KAFKA", "issue_key": "KAFKA-19771", "title": "Update SpotBugs version and enable Spotbugs Gradle tasks on Java 25", "status": "Resolved", "reporter": "Dejan Stojadinovi\u0107", "assignee": "Chia-Ping Tsai", "priority": "Major", "labels": ["Java25", "build", "spotbugs", "update", "upgrade"], "created": "2025-10-08T09:06:12.000+0000", "updated": "2025-10-19T11:36:40.000+0000", "description": "*Prologue:* KAFKA-19664\u00a0\r\n\r\n*In more details:* Apache commons bcel Java 25 compatible version will be created soon (and SpotBugs version will follow immediately): https://issues.apache.org/jira/browse/BCEL-377?focusedCommentId=18028106&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-18028106\r\n\r\n*Action points:*\r\n - upgrade SpotBugs version (use Java 25 compatible version)\r\n - enable SpotBugs checks for Java 25 Github actions build\r\n\r\n*Related links:\u00a0*\r\n- [https://issues.apache.org/jira/projects/BCEL/versions/12354966] \r\n- [https://github.com/spotbugs/spotbugs/issues/3564]", "comments": [], "derived_tasks": {"summarization": "Update SpotBugs version and enable Spotbugs Gradle tasks on Java 25", "classification": "feature", "qna": {"question": "What is the issue 'Update SpotBugs version and enable Spotbugs Gradle tasks on Java 25' about?", "answer": "*Prologue:* KAFKA-19664\u00a0\r\n\r\n*In more details:* Apache commons bcel Java 25 compatible version will be created soon (and SpotBugs version will follow immediately): https://issues.apache.org/jira/browse/BCEL-377?focusedCommentId=18028106&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-18028106\r\n\r\n*Action points:*\r\n - upgrade SpotBugs version (use Java 25 compatible version)\r\n - enable SpotBugs checks for Java 25 Github actions build\r\n\r\n*Related links:\u00a0*\r\n- [https://issues.apache.org/jira/projects/BCEL/versions/12354966] \r\n- [https://github.com/spotbugs/spotbugs/issues/3564]"}}}
{"project": "KAFKA", "issue_key": "KAFKA-19770", "title": "Remove the `gradlew` workaround for running the old `gradle-wrapper.jar`", "status": "Open", "reporter": "Chia-Ping Tsai", "assignee": "Chia-Ping Tsai", "priority": "Minor", "labels": [], "created": "2025-10-08T07:45:41.000+0000", "updated": "2025-10-08T07:45:41.000+0000", "description": "see https://github.com/apache/kafka/pull/20658", "comments": [], "derived_tasks": {"summarization": "Remove the `gradlew` workaround for running the old `gradle-wrapper.jar`", "classification": "feature", "qna": {"question": "What is the issue 'Remove the `gradlew` workaround for running the old `gradle-wrapper.jar`' about?", "answer": "see https://github.com/apache/kafka/pull/20658"}}}
{"project": "KAFKA", "issue_key": "KAFKA-19769", "title": "[Java 25] few tests are failling (class: clients/org.apache.kafka.common.network.SslTransportLayerTest)", "status": "Resolved", "reporter": "Dejan Stojadinovi\u0107", "assignee": "Gaurav Narula", "priority": "Major", "labels": ["Java25", "help-wanted", "test-fail", "test-failure", "test-failure-fresh", "up-for-grabs", "upgrade"], "created": "2025-10-07T17:58:00.000+0000", "updated": "2025-10-08T08:18:53.000+0000", "description": "{panel:title=Notes:|borderStyle=dashed|borderColor=#cccccc|titleBGColor=#f7d6c1|bgColor=#ffffce}\r\n * (!) this ticket blocks KAFKA-19664 *_Support building with Java 25 (LTS release)_*\r\n * (on) looks similar to: KAFKA-15117 *_SslTransportLayerTest.testValidEndpointIdentificationCN fails with Java 20 & 21_*{panel}\r\n*Reproducer:*\r\n * use this branch: [https://github.com/dejan2609/kafka/tree/KAFKA-19664] (i.e. this PR: [https://github.com/apache/kafka/pull/20561])\r\n * execute: *./gradlew :clients:test --tests \"org.apache.kafka.common.network.SslTransportLayerTest\" -i* and change Java versions\r\n * test results:\r\n ** {*}Java 17{*}: (/)\r\n ** J{*}ava 24{*}: (/)\r\n ** {*}Java 25{*}: 159 tests completed, *8 failed* (x)\r\n\r\n*Test results on Github CI:*\r\n!Screenshot from 2025-10-07 19-59-32.png!\r\n\r\n*Test results locally:*\r\n{code:java}\r\ndejan@dejan-HP-ProBook-450-G7:~/kafka$ git log -3 --oneline\u00a0\r\na37616a1eb (HEAD -> KAFKA-19664, origin/KAFKA-19664) KAFKA-19664 Support building with Java 25 (LTS release)\r\nc6bbbbe24d KAFKA-19174 Gradle version upgrade 8 -->> 9 (#19513)\r\nf5a87b3703 KAFKA-19748: Add a note in docs about memory leak in Kafka Streams 4.1.0 \u00a0(#20639)\r\ndejan@dejan-HP-ProBook-450-G7:~/kafka$ java -version\u00a0\r\nopenjdk version \"17.0.16\" 2025-07-15\r\nOpenJDK Runtime Environment Temurin-17.0.16+8 (build 17.0.16+8)\r\nOpenJDK 64-Bit Server VM Temurin-17.0.16+8 (build 17.0.16+8, mixed mode, sharing)\r\ndejan@dejan-HP-ProBook-450-G7:~/kafka$ ./gradlew :clients:test --tests \"org.apache.kafka.common.network.SslTransportLayerTest\" -q\r\nStarting build with version 4.2.0-SNAPSHOT (commit id a37616a1) using Gradle 9.1.0, Java 17 and Scala 2.13.17\r\nBuild properties: ignoreFailures=false, maxParallelForks=8, maxScalacThreads=8, maxTestRetries=0\r\nMessageGenerator: processed 197 Kafka message JSON file(s).\r\nMessageGenerator: processed 4 Kafka message JSON file(s).\r\nNote: Some input files use or override a deprecated API.\r\nNote: Recompile with -Xlint:deprecation for details.\r\nOpenJDK 64-Bit Server VM warning: Sharing is only supported for boot loader classes because bootstrap classpath has been appended\r\ndejan@dejan-HP-ProBook-450-G7:~/kafka$ sdk use java 24.0.2-tem\r\n\r\nUsing java version 24.0.2-tem in this shell.\r\ndejan@dejan-HP-ProBook-450-G7:~/kafka$ java -version\r\nopenjdk version \"24.0.2\" 2025-07-15\r\nOpenJDK Runtime Environment Temurin-24.0.2+12 (build 24.0.2+12)\r\nOpenJDK 64-Bit Server VM Temurin-24.0.2+12 (build 24.0.2+12, mixed mode, sharing)\r\ndejan@dejan-HP-ProBook-450-G7:~/kafka$ ./gradlew :clients:test --tests \"org.apache.kafka.common.network.SslTransportLayerTest\" -q\r\nStarting build with version 4.2.0-SNAPSHOT (commit id a37616a1) using Gradle 9.1.0, Java 24 and Scala 2.13.17\r\nBuild properties: ignoreFailures=false, maxParallelForks=8, maxScalacThreads=8, maxTestRetries=0\r\nMessageGenerator: processed 197 Kafka message JSON file(s).\r\nMessageGenerator: processed 4 Kafka message JSON file(s).\r\nNote: Some input files use or override a deprecated API.\r\nNote: Recompile with -Xlint:deprecation for details.\r\nOpenJDK 64-Bit Server VM warning: Sharing is only supported for boot loader classes because bootstrap classpath has been appended\r\ndejan@dejan-HP-ProBook-450-G7:~/kafka$ sdk use java 25-tem\r\n\r\nUsing java version 25-tem in this shell.\r\ndejan@dejan-HP-ProBook-450-G7:~/kafka$ java -version\r\nopenjdk version \"25\" 2025-09-16 LTS\r\nOpenJDK Runtime Environment Temurin-25+36 (build 25+36-LTS)\r\nOpenJDK 64-Bit Server VM Temurin-25+36 (build 25+36-LTS, mixed mode, sharing)\r\ndejan@dejan-HP-ProBook-450-G7:~/kafka$ ./gradlew :clients:test --tests \"org.apache.kafka.common.network.SslTransportLayerTest\" -q\r\nStarting build with version 4.2.0-SNAPSHOT (commit id a37616a1) using Gradle 9.1.0, Java 25 and Scala 2.13.17\r\nBuild properties: ignoreFailures=false, maxParallelForks=8, maxScalacThreads=8, maxTestRetries=0\r\nMessageGenerator: processed 197 Kafka message JSON file(s).\r\nMessageGenerator: processed 4 Kafka message JSON file(s).\r\nNote: Some input files use or override a deprecated API.\r\nNote: Recompile with -Xlint:deprecation for details.\r\norg.apache.kafka.common.network.SslTransportLayerTest.testListenerConfigOverride(Args)[1] failed, log available in /home/dejan/kafka/clients/build/reports/testOutput/org.apache.kafka.common.network.SslTransportLayerTest.testListenerConfigOverride(Args)[1].test.stdout\r\nOpenJDK 64-Bit Server VM warning: Sharing is only supported for boot loader classes because bootstrap classpath has been appended\r\norg.apache.kafka.common.network.SslTransportLayerTest.testInterBrokerSslConfigValidation(Args)[1] failed, log available in /home/dejan/kafka/clients/build/reports/testOutput/org.apache.kafka.common.network.SslTransportLayerTest.testInterBrokerSslConfigValidation(Args)[1].test.stdout\r\norg.apache.kafka.common.network.SslTransportLayerTest.testServerTruststoreDynamicUpdate(Args)[1] failed, log available in /home/dejan/kafka/clients/build/reports/testOutput/org.apache.kafka.common.network.SslTransportLayerTest.testServerTruststoreDynamicUpdate(Args)[1].test.stdout\r\norg.apache.kafka.common.network.SslTransportLayerTest.testECKeyPair(Args)[1] failed, log available in /home/dejan/kafka/clients/build/reports/testOutput/org.apache.kafka.common.network.SslTransportLayerTest.testECKeyPair(Args)[1].test.stdout\r\norg.apache.kafka.common.network.SslTransportLayerTest.testPemFiles(Args)[1] failed, log available in /home/dejan/kafka/clients/build/reports/testOutput/org.apache.kafka.common.network.SslTransportLayerTest.testPemFiles(Args)[1].test.stdout\r\norg.apache.kafka.common.network.SslTransportLayerTest.testClientAuthenticationRequiredValidProvided(Args)[1] failed, log available in /home/dejan/kafka/clients/build/reports/testOutput/org.apache.kafka.common.network.SslTransportLayerTest.testClientAuthenticationRequiredValidProvided(Args)[1].test.stdout\r\norg.apache.kafka.common.network.SslTransportLayerTest.testDsaKeyPair(Args)[1] failed, log available in /home/dejan/kafka/clients/build/reports/testOutput/org.apache.kafka.common.network.SslTransportLayerTest.testDsaKeyPair(Args)[1].test.stdout\r\norg.apache.kafka.common.network.SslTransportLayerTest.testPemFilesWithoutClientKeyPassword(Args)[1] failed, log available in /home/dejan/kafka/clients/build/reports/testOutput/org.apache.kafka.common.network.SslTransportLayerTest.testPemFilesWithoutClientKeyPassword(Args)[1].test.stdout\r\n\r\n159 tests completed, 8 failed\r\n\r\nFAILURE: Build failed with an exception.\r\n\r\n* What went wrong:\r\nExecution failed for task ':clients:test'.\r\n> There were failing tests. See the report at: file:///home/dejan/kafka/clients/build/reports/tests/test/index.html\r\n\r\nBUILD FAILED in 8m 22s\r\ndejan@dejan-HP-ProBook-450-G7:~/kafka$\r\n{code}", "comments": [], "derived_tasks": {"summarization": "[Java 25] few tests are failling (class: clients/org.apache.kafka.common.network.SslTransportLayerTest)", "classification": "feature", "qna": {"question": "What is the issue '[Java 25] few tests are failling (class: clients/org.apache.kafka.common.network.SslTransportLayerTest)' about?", "answer": "{panel:title=Notes:|borderStyle=dashed|borderColor=#cccccc|titleBGColor=#f7d6c1|bgColor=#ffffce}\r\n * (!) this ticket blocks KAFKA-19664 *_Support building with Java 25 (LTS release)_*\r\n * (on) looks similar to: KAFKA-15117 *_SslTransportLayerTest.testValidEndpointIdentificationCN fails with Java 20 & 21_*{panel}\r\n*Reproducer:*\r\n * use this branch: [https://github.com/dejan2609/kafka/tree/KAFKA-19664] (i.e. this PR: [https://github.com/apache/kafka/pull/20561])\r\n * execute: *./gradlew :clients:test --tests \"org.apache.kafka.common.network.SslTransportLayerTest\" -i* and change Java versions\r\n * test results:\r\n ** {*}Java 17{*}: (/)\r\n ** J{*}ava 24{*}: (/)\r\n ** {*}Java 25{*}: 159 tests completed, *8 failed* (x)\r\n\r\n*Test results on Github CI:*\r\n!Screenshot from 2025-10-07 19-59-32.png!\r\n\r\n*Test results locally:*\r\n{code:java}\r\ndejan@dejan-HP-ProBook-450-G7:~/kafka$ git log -3 --oneline\u00a0\r\na37616a1eb (HEAD -> KAFKA-19664, origin/KAFKA-19664) KAFKA-19664 Support building with Java 25 (LTS release)\r\nc6bbbbe24d KAFKA-19174 Gradle version upgrade 8 -->> 9 (#19513)\r\nf5a87b3703 KAFKA-19748: Add a note in docs about memory leak in Kafka Streams 4.1.0 \u00a0(#20639)\r\ndejan@dejan-HP-ProBook-450-G7:~/kafka$ java -version\u00a0\r\nopenjdk version \"17.0.16\" 2025-07-15\r\nOpenJDK Runtime Environment Temurin-17.0.16+8 (build 17.0.16+8)\r\nOpenJDK 64-Bit Server VM Temurin-17.0.16+8 (build 17.0.16+8, mixed mode, sharing)\r\ndejan@dejan-HP-ProBook-450-G7:~/kafka$ ./gradlew :clients:test --tests \"org.apache.kafka.common.network.SslTransportLayerTest\" -q\r\nStarting build with version 4.2.0-SNAPSHOT (commit id a37616a1) using Gradle 9.1.0, Java 17 and Scala 2.13.17\r\nBuild properties: ignoreFailures=false, maxParallelForks=8, maxScalacThreads=8, maxTestRetries=0\r\nMessageGenerator: processed 197 Kafka message JSON file(s).\r\nMessageGenerator: processed 4 Kafka message JSON file(s).\r\nNote: Some input files use or override a deprecated API.\r\nNote: Recompile with -Xlint:deprecation for details.\r\nOpenJDK 64-Bit Server VM warning: Sharing is only supported for boot loader classes because bootstrap classpath has been appended\r\ndejan@dejan-HP-ProBook-450-G7:~/kafka$ sdk use java 24.0.2-tem\r\n\r\nUsing java version 24.0.2-tem in this shell.\r\ndejan@dejan-HP-ProBook-450-G7:~/kafka$ java -version\r\nopenjdk version \"24.0.2\" 2025-07-15\r\nOpenJDK Runtime Environment Temurin-24.0.2+12 (build 24.0.2+12)\r\nOpenJDK 64-Bit Server VM Temurin-24.0.2+12 (build 24.0.2+12, mixed mode, sharing)\r\ndejan@dejan-HP-ProBook-450-G7:~/kafka$ ./gradlew :clients:test --tests \"org.apache.kafka.common.network.SslTransportLayerTest\" -q\r\nStarting build with version 4.2.0-SNAPSHOT (commit id a37616a1) using Gradle 9.1.0, Java 24 and Scala 2.13.17\r\nBuild properties: ignoreFailures=false, maxParallelForks=8, maxScalacThreads=8, maxTestRetries=0\r\nMessageGenerator: processed 197 Kafka message JSON file(s).\r\nMessageGenerator: processed 4 Kafka message JSON file(s).\r\nNote: Some input files use or override a deprecated API.\r\nNote: Recompile with -Xlint:deprecation for details.\r\nOpenJDK 64-Bit Server VM warning: Sharing is only supported for boot loader classes because bootstrap classpath has been appended\r\ndejan@dejan-HP-ProBook-450-G7:~/kafka$ sdk use java 25-tem\r\n\r\nUsing java version 25-tem in this shell.\r\ndejan@dejan-HP-ProBook-450-G7:~/kafka$ java -version\r\nopenjdk version \"25\" 2025-09-16 LTS\r\nOpenJDK Runtime Environment Temurin-25+36 (build 25+36-LTS)\r\nOpenJDK 64-Bit Server VM Temurin-25+36 (build 25+36-LTS, mixed mode, sharing)\r\ndejan@dejan-HP-ProBook-450-G7:~/kafka$ ./gradlew :clients:test --tests \"org.apache.kafka.common.network.SslTransportLayerTest\" -q\r\nStarting build with version 4.2.0-SNAPSHOT (commit id a37616a1) using Gradle 9.1.0, Java 25 and Scala 2.13.17\r\nBuild properties: ignoreFailures=false, maxParallelForks=8, maxScalacThreads=8, maxTestRetries=0\r\nMessageGenerator: processed 197 Kafka message JSON file(s).\r\nMessageGenerator: processed 4 Kafka message JSON file(s).\r\nNote: Some input files use or override a deprecated API.\r\nNote: Recompile with -Xlint:deprecation for details.\r\norg.apache.kafka.common.network.SslTransportLayerTest.testListenerConfigOverride(Args)[1] failed, log available in /home/dejan/kafka/clients/build/reports/testOutput/org.apache.kafka.common.network.SslTransportLayerTest.testListenerConfigOverride(Args)[1].test.stdout\r\nOpenJDK 64-Bit Server VM warning: Sharing is only supported for boot loader classes because bootstrap classpath has been appended\r\norg.apache.kafka.common.network.SslTransportLayerTest.testInterBrokerSslConfigValidation(Args)[1] failed, log available in /home/dejan/kafka/clients/build/reports/testOutput/org.apache.kafka.common.network.SslTransportLayerTest.testInterBrokerSslConfigValidation(Args)[1].test.stdout\r\norg.apache.kafka.common.network.SslTransportLayerTest.testServerTruststoreDynamicUpdate(Args)[1] failed, log available in /home/dejan/kafka/clients/build/reports/testOutput/org.apache.kafka.common.network.SslTransportLayerTest.testServerTruststoreDynamicUpdate(Args)[1].test.stdout\r\norg.apache.kafka.common.network.SslTransportLayerTest.testECKeyPair(Args)[1] failed, log available in /home/dejan/kafka/clients/build/reports/testOutput/org.apache.kafka.common.network.SslTransportLayerTest.testECKeyPair(Args)[1].test.stdout\r\norg.apache.kafka.common.network.SslTransportLayerTest.testPemFiles(Args)[1] failed, log available in /home/dejan/kafka/clients/build/reports/testOutput/org.apache.kafka.common.network.SslTransportLayerTest.testPemFiles(Args)[1].test.stdout\r\norg.apache.kafka.common.network.SslTransportLayerTest.testClientAuthenticationRequiredValidProvided(Args)[1] failed, log available in /home/dejan/kafka/clients/build/reports/testOutput/org.apache.kafka.common.network.SslTransportLayerTest.testClientAuthenticationRequiredValidProvided(Args)[1].test.stdout\r\norg.apache.kafka.common.network.SslTransportLayerTest.testDsaKeyPair(Args)[1] failed, log available in /home/dejan/kafka/clients/build/reports/testOutput/org.apache.kafka.common.network.SslTransportLayerTest.testDsaKeyPair(Args)[1].test.stdout\r\norg.apache.kafka.common.network.SslTransportLayerTest.testPemFilesWithoutClientKeyPassword(Args)[1] failed, log available in /home/dejan/kafka/clients/build/reports/testOutput/org.apache.kafka.common.network.SslTransportLayerTest.testPemFilesWithoutClientKeyPassword(Args)[1].test.stdout\r\n\r\n159 tests completed, 8 failed\r\n\r\nFAILURE: Build failed with an exception.\r\n\r\n* What went wrong:\r\nExecution failed for task ':clients:test'.\r\n> There were failing tests. See the report at: file:///home/dejan/kafka/clients/build/reports/tests/test/index.html\r\n\r\nBUILD FAILED in 8m 22s\r\ndejan@dejan-HP-ProBook-450-G7:~/kafka$\r\n{code}"}}}
{"project": "KAFKA", "issue_key": "KAFKA-19768", "title": "`ci-complete` needs to work with active branches after the JDK is updated", "status": "Open", "reporter": "Chia-Ping Tsai", "assignee": "Ming-Yen Chung", "priority": "Minor", "labels": ["build", "github-actions"], "created": "2025-10-07T16:31:26.000+0000", "updated": "2025-10-31T09:02:11.000+0000", "description": "The JDK version used by `ci-complete` is inconsistent with other active branches, which is causing the build report task to fail", "comments": [], "derived_tasks": {"summarization": "`ci-complete` needs to work with active branches after the JDK is updated", "classification": "feature", "qna": {"question": "What is the issue '`ci-complete` needs to work with active branches after the JDK is updated' about?", "answer": "The JDK version used by `ci-complete` is inconsistent with other active branches, which is causing the build report task to fail"}}}
{"project": "KAFKA", "issue_key": "KAFKA-19767", "title": "Improve handling of long processing times", "status": "In Progress", "reporter": "Andrew Schofield", "assignee": "Andrew Schofield", "priority": "Major", "labels": [], "created": "2025-10-07T16:15:01.000+0000", "updated": "2025-10-23T17:36:17.000+0000", "description": "If the share consumer application takes a long time to process records, the share consume request manager can do a better job of time-outs and liveness.", "comments": [], "derived_tasks": {"summarization": "Improve handling of long processing times", "classification": "feature", "qna": {"question": "What is the issue 'Improve handling of long processing times' about?", "answer": "If the share consumer application takes a long time to process records, the share consume request manager can do a better job of time-outs and liveness."}}}
{"project": "KAFKA", "issue_key": "KAFKA-19766", "title": "Upgrade Kafka repo to use JUnit6", "status": "Open", "reporter": "Maros Orsak", "assignee": null, "priority": "Minor", "labels": [], "created": "2025-10-07T16:11:54.000+0000", "updated": "2025-10-08T12:03:09.000+0000", "description": "As JUnit6 was released\u00a0[https://docs.junit.org/6.0.0/release-notes], it would be great to upgrade repo with such version. Currently {*}5.13.1{*}.\r\n\r\n\u00a0", "comments": [], "derived_tasks": {"summarization": "Upgrade Kafka repo to use JUnit6", "classification": "feature", "qna": {"question": "What is the issue 'Upgrade Kafka repo to use JUnit6' about?", "answer": "As JUnit6 was released\u00a0[https://docs.junit.org/6.0.0/release-notes], it would be great to upgrade repo with such version. Currently {*}5.13.1{*}.\r\n\r\n\u00a0"}}}
{"project": "KAFKA", "issue_key": "KAFKA-19765", "title": "Store the last used assignment configuration in the group metadata", "status": "Resolved", "reporter": "Lucas Brutschy", "assignee": "Lucy Liu", "priority": "Major", "labels": [], "created": "2025-10-07T14:50:58.000+0000", "updated": "2025-10-24T16:00:35.000+0000", "description": "We should store the last used assignment configuration in the group metadata and bump the group epoch if the assignment configuration is changed.", "comments": [], "derived_tasks": {"summarization": "Store the last used assignment configuration in the group metadata", "classification": "feature", "qna": {"question": "What is the issue 'Store the last used assignment configuration in the group metadata' about?", "answer": "We should store the last used assignment configuration in the group metadata and bump the group epoch if the assignment configuration is changed."}}}
{"project": "KAFKA", "issue_key": "KAFKA-19764", "title": "KIP-1224: Adaptive append.linger.ms for the group coordinator and share coordinator", "status": "Open", "reporter": "Sean Quah", "assignee": "Sean Quah", "priority": "Minor", "labels": [], "created": "2025-10-07T14:20:27.000+0000", "updated": "2025-10-22T13:49:09.000+0000", "description": "Ticket for KIP-1224.\r\n\r\n* Add a new allowed value for {{group.coordinator.append.linger.ms}} and {{share.coordinator.append.linger.ms}} of -1.\r\n* When {{append.linger.ms}} is set to -1, use the flush strategy outlined in the KIP.\r\n* Add batch-linger-time metrics.\r\n* Add batch-flush-time metrics.", "comments": [], "derived_tasks": {"summarization": "KIP-1224: Adaptive append.linger.ms for the group coordinator and share coordinator", "classification": "feature", "qna": {"question": "What is the issue 'KIP-1224: Adaptive append.linger.ms for the group coordinator and share coordinator' about?", "answer": "Ticket for KIP-1224.\r\n\r\n* Add a new allowed value for {{group.coordinator.append.linger.ms}} and {{share.coordinator.append.linger.ms}} of -1.\r\n* When {{append.linger.ms}} is set to -1, use the flush strategy outlined in the KIP.\r\n* Add batch-linger-time metrics.\r\n* Add batch-flush-time metrics."}}}
{"project": "KAFKA", "issue_key": "KAFKA-19763", "title": "Parallel remote reads causes memory leak in broker", "status": "Resolved", "reporter": "Kamal Chandraprakash", "assignee": "Kamal Chandraprakash", "priority": "Blocker", "labels": [], "created": "2025-10-07T13:53:55.000+0000", "updated": "2025-10-20T00:15:56.000+0000", "description": "This issue is caused with changes from https://issues.apache.org/jira/browse/KAFKA-14915\r\n\r\nBroker heap memory gets filled up and throws OOM error when remote reads are triggered for multiple partitions within a FETCH request. \r\n\r\nSteps to reproduce: \r\n\r\n1. Start a one node broker and configure LocalTieredStorage as remote storage. \r\n2. Create a topic with 5 partitions. \r\n3. Produce message and ensure that few segments are uploaded to remote.\r\n4. Start a consumer to read from those 5 partitions. Seek the offset to beginning for 4 partitions and to end for 1 partition. This is to simulate that the FETCH request read from both remote-log and local-log.\r\n5. The broker crashes with the OOM error.\r\n6. The DelayedRemoteFetch / RemoteLogReadResult references are being held by the purgatory, so the broker crashes.\r\n\r\ncc [~showuon] [~satish.duggana]\r\n", "comments": [], "derived_tasks": {"summarization": "Parallel remote reads causes memory leak in broker", "classification": "feature", "qna": {"question": "What is the issue 'Parallel remote reads causes memory leak in broker' about?", "answer": "This issue is caused with changes from https://issues.apache.org/jira/browse/KAFKA-14915\r\n\r\nBroker heap memory gets filled up and throws OOM error when remote reads are triggered for multiple partitions within a FETCH request. \r\n\r\nSteps to reproduce: \r\n\r\n1. Start a one node broker and configure LocalTieredStorage as remote storage. \r\n2. Create a topic with 5 partitions. \r\n3. Produce message and ensure that few segments are uploaded to remote.\r\n4. Start a consumer to read from those 5 partitions. Seek the offset to beginning for 4 partitions and to end for 1 partition. This is to simulate that the FETCH request read from both remote-log and local-log.\r\n5. The broker crashes with the OOM error.\r\n6. The DelayedRemoteFetch / RemoteLogReadResult references are being held by the purgatory, so the broker crashes.\r\n\r\ncc [~showuon] [~satish.duggana]\r\n"}}}
{"project": "KAFKA", "issue_key": "KAFKA-19762", "title": "Turn on Gradle reproducible builds feature", "status": "Open", "reporter": "Dejan Stojadinovi\u0107", "assignee": null, "priority": "Minor", "labels": ["Gradle", "build", "gradle"], "created": "2025-10-07T11:23:45.000+0000", "updated": "2025-10-27T08:51:59.000+0000", "description": "*Prologue:* [https://github.com/apache/kafka/pull/19513#discussion_r2405757923]\u00a0\r\n\r\n*Note:* during the Gradle version upgrade from 8 to 9 (KAFKA-19174), the reproducible build feature was turned off (but it should be switched on at some point in the future)\r\n\r\n*Related Gradle issues and links:*\r\n * \u00a0[https://github.com/gradle/gradle/issues/34643]\r\n * [https://github.com/gradle/gradle/issues/30871] \u00a0\r\n * [https://docs.gradle.org/9.1.0/userguide/working_with_files.html#sec:reproducible_archives]\r\n * [https://docs.gradle.org/9.1.0/userguide/upgrading_major_version_9.html#reproducible_archives_by_default]\u00a0\r\n * [https://docs.gradle.org/9.1.0/dsl/org.gradle.api.tasks.bundling.Tar.html#org.gradle.api.tasks.bundling.Tar:preserveFileTimestamps] \u00a0\r\n\r\n\u00a0\r\n\r\n*Definition of done (at the minimum):*\r\n * *./gradlew releaseTarGz* works as expected\r\n * produced archive works as expected (Kafka cluster starts, messages can be produced/consumed, etc) : [https://kafka.apache.org/quickstart]\u00a0\r\n\r\n\u00a0", "comments": [], "derived_tasks": {"summarization": "Turn on Gradle reproducible builds feature", "classification": "feature", "qna": {"question": "What is the issue 'Turn on Gradle reproducible builds feature' about?", "answer": "*Prologue:* [https://github.com/apache/kafka/pull/19513#discussion_r2405757923]\u00a0\r\n\r\n*Note:* during the Gradle version upgrade from 8 to 9 (KAFKA-19174), the reproducible build feature was turned off (but it should be switched on at some point in the future)\r\n\r\n*Related Gradle issues and links:*\r\n * \u00a0[https://github.com/gradle/gradle/issues/34643]\r\n * [https://github.com/gradle/gradle/issues/30871] \u00a0\r\n * [https://docs.gradle.org/9.1.0/userguide/working_with_files.html#sec:reproducible_archives]\r\n * [https://docs.gradle.org/9.1.0/userguide/upgrading_major_version_9.html#reproducible_archives_by_default]\u00a0\r\n * [https://docs.gradle.org/9.1.0/dsl/org.gradle.api.tasks.bundling.Tar.html#org.gradle.api.tasks.bundling.Tar:preserveFileTimestamps] \u00a0\r\n\r\n\u00a0\r\n\r\n*Definition of done (at the minimum):*\r\n * *./gradlew releaseTarGz* works as expected\r\n * produced archive works as expected (Kafka cluster starts, messages can be produced/consumed, etc) : [https://kafka.apache.org/quickstart]\u00a0\r\n\r\n\u00a0"}}}
{"project": "KAFKA", "issue_key": "KAFKA-19761", "title": "Reorder Gradle tasks (in order to bump Shadow plugin version)", "status": "Open", "reporter": "Dejan Stojadinovi\u0107", "assignee": "Nishanth", "priority": "Minor", "labels": ["Gradle", "build", "gradle"], "created": "2025-10-07T11:13:51.000+0000", "updated": "2025-10-27T09:08:36.000+0000", "description": "*Prologue:*\r\n * JIRA ticket: KAFKA-19174\r\n * GitHub PR comment: [https://github.com/apache/kafka/pull/19513#discussion_r2365678027]\r\n\r\n*Scenario:*\r\n * checkout Kafka trunk and bump Gradle Shadow plugin version to 9+\r\n * execute: *_./gradlew :jmh-benchmarks:shadowJar_* - build will fail (x)\r\n\r\n*Action points (what needs to be done):*\r\n * use `com.gradleup.shadow` recent version (9+)\r\n * reorder Gradle tasks so that Gradle command mentioned above can work\r\n\r\n*Definition of done (at the minimum):*\r\n * Gradle command mentioned above works as expected\r\n * also: *./gradlew releaseTarGz* works as expected\r\n * produced archive works as expected (Kafka cluster starts, messages can be produced/consumed, etc): [https://kafka.apache.org/quickstart]\u00a0\r\n\r\n\u00a0", "comments": [], "derived_tasks": {"summarization": "Reorder Gradle tasks (in order to bump Shadow plugin version)", "classification": "feature", "qna": {"question": "What is the issue 'Reorder Gradle tasks (in order to bump Shadow plugin version)' about?", "answer": "*Prologue:*\r\n * JIRA ticket: KAFKA-19174\r\n * GitHub PR comment: [https://github.com/apache/kafka/pull/19513#discussion_r2365678027]\r\n\r\n*Scenario:*\r\n * checkout Kafka trunk and bump Gradle Shadow plugin version to 9+\r\n * execute: *_./gradlew :jmh-benchmarks:shadowJar_* - build will fail (x)\r\n\r\n*Action points (what needs to be done):*\r\n * use `com.gradleup.shadow` recent version (9+)\r\n * reorder Gradle tasks so that Gradle command mentioned above can work\r\n\r\n*Definition of done (at the minimum):*\r\n * Gradle command mentioned above works as expected\r\n * also: *./gradlew releaseTarGz* works as expected\r\n * produced archive works as expected (Kafka cluster starts, messages can be produced/consumed, etc): [https://kafka.apache.org/quickstart]\u00a0\r\n\r\n\u00a0"}}}
{"project": "KAFKA", "issue_key": "KAFKA-19760", "title": "RecordTooLargeExceptions in group coordinator when offsets.topic.compression.codec is used", "status": "Resolved", "reporter": "Sean Quah", "assignee": "Izzy Harker", "priority": "Blocker", "labels": [], "created": "2025-10-07T07:27:55.000+0000", "updated": "2025-10-21T16:55:35.000+0000", "description": "The coordinator runtime can be overly optimistic about compression ratios when filling up batches and can often overflow `max.message.bytes` of the topic. When this happens, all the group coordinator requests that wrote to the batch fail with an UNKNOWN_SERVER_ERROR. This error is non-retriable and disruptive to client applications.", "comments": [], "derived_tasks": {"summarization": "RecordTooLargeExceptions in group coordinator when offsets.topic.compression.codec is used", "classification": "feature", "qna": {"question": "What is the issue 'RecordTooLargeExceptions in group coordinator when offsets.topic.compression.codec is used' about?", "answer": "The coordinator runtime can be overly optimistic about compression ratios when filling up batches and can often overflow `max.message.bytes` of the topic. When this happens, all the group coordinator requests that wrote to the batch fail with an UNKNOWN_SERVER_ERROR. This error is non-retriable and disruptive to client applications."}}}
{"project": "KAFKA", "issue_key": "KAFKA-19759", "title": "Add built-in TTL (Time-to-Live) support for Kafka Streams State Stores", "status": "Open", "reporter": "Ankur Sinha", "assignee": null, "priority": "Minor", "labels": ["needs-kip"], "created": "2025-10-06T17:12:35.000+0000", "updated": "2025-10-06T22:44:46.000+0000", "description": "In business cases Kafka Streams users frequently need *per-key Time-To-Live (TTL)* behavior for state stores such as keyValueStore or kTables , typically to model cache-like or deduplication scenarios.\r\n\r\nToday, achieving this requires *manual handling* using:\r\n * Custom timestamp tracking per key,\r\n\r\n * Punctuators to periodically scan and remove expired entries, and\r\n\r\n * Manual emission of tombstones to maintain changelog consistency.\r\n\r\nThese workarounds are:\r\n * {*}Inconsistent across applications{*}, and\r\n\r\n * {*}Operationally costly{*}, as each developer must reimplement the same logic.\r\n\r\n*Proposal* is to introduce a *built-in TTL mechanism* for Kafka Streams state stores, allowing automatic expiration of records after a configured duration.\r\n\r\nIntroduction to new Api's like :\u00a0\r\n\r\nStoreBuilder<T> withTTL(Duration ttl);\r\nMaterialized<K, V, S> withTtl(Duration ttl);\r\n\r\nWhen configured:\r\n * Each record\u2019s timestamp (from event-time or processing-time) is tracked.\r\n\r\n * Expired keys are automatically evicted by a background task (via ProcessorContext.Schedule()).\r\n\r\n * Corresponding tombstones are flushed to changelog.\r\n\r\nThis feature can provide a *TTL abstraction* that simplifies common use cases as:\r\n * Maintaining cache-like state (e.g., last-seen values with limited lifespan)\r\n\r\n * Automatically purging inactive or stale keys without manual cleanup.\r\n\r\nPoints of Risk and Benifits i considered it can bring :\u00a0\r\n * Consistency as automatic changelog tombstones will preserve correctness across rebalances and restores.\r\n\r\n * Will help to avoid boilerplate punctuator code for manual expiration.\r\n\r\n * TTL is optional and opt-in; existing stores remain unaffected so backward compatibility would be maintaoined.\r\n\r\nExample to StateStore/ kTable inferface :\u00a0\r\n\r\nKTable<String, UserSession> sessions = builder\r\n\u00a0 \u00a0 .table(\"sessions\", Materialized.<String, UserSession, KeyValueStore<Bytes, byte[]>>as(\"session-store\")\r\n\u00a0 \u00a0 \u00a0 \u00a0 .withTtl(Duration.ofHours(1))\r\n\u00a0 \u00a0 \u00a0 \u00a0 .withValueSerde(userSessionSerde));\r\n\r\nHere, session entries older than 1 hour will be automatically expired and deleted from the local RocksDB store and hence a flush ~ tombstone to changelog topic.", "comments": [], "derived_tasks": {"summarization": "Add built-in TTL (Time-to-Live) support for Kafka Streams State Stores", "classification": "feature", "qna": {"question": "What is the issue 'Add built-in TTL (Time-to-Live) support for Kafka Streams State Stores' about?", "answer": "In business cases Kafka Streams users frequently need *per-key Time-To-Live (TTL)* behavior for state stores such as keyValueStore or kTables , typically to model cache-like or deduplication scenarios.\r\n\r\nToday, achieving this requires *manual handling* using:\r\n * Custom timestamp tracking per key,\r\n\r\n * Punctuators to periodically scan and remove expired entries, and\r\n\r\n * Manual emission of tombstones to maintain changelog consistency.\r\n\r\nThese workarounds are:\r\n * {*}Inconsistent across applications{*}, and\r\n\r\n * {*}Operationally costly{*}, as each developer must reimplement the same logic.\r\n\r\n*Proposal* is to introduce a *built-in TTL mechanism* for Kafka Streams state stores, allowing automatic expiration of records after a configured duration.\r\n\r\nIntroduction to new Api's like :\u00a0\r\n\r\nStoreBuilder<T> withTTL(Duration ttl);\r\nMaterialized<K, V, S> withTtl(Duration ttl);\r\n\r\nWhen configured:\r\n * Each record\u2019s timestamp (from event-time or processing-time) is tracked.\r\n\r\n * Expired keys are automatically evicted by a background task (via ProcessorContext.Schedule()).\r\n\r\n * Corresponding tombstones are flushed to changelog.\r\n\r\nThis feature can provide a *TTL abstraction* that simplifies common use cases as:\r\n * Maintaining cache-like state (e.g., last-seen values with limited lifespan)\r\n\r\n * Automatically purging inactive or stale keys without manual cleanup.\r\n\r\nPoints of Risk and Benifits i considered it can bring :\u00a0\r\n * Consistency as automatic changelog tombstones will preserve correctness across rebalances and restores.\r\n\r\n * Will help to avoid boilerplate punctuator code for manual expiration.\r\n\r\n * TTL is optional and opt-in; existing stores remain unaffected so backward compatibility would be maintaoined.\r\n\r\nExample to StateStore/ kTable inferface :\u00a0\r\n\r\nKTable<String, UserSession> sessions = builder\r\n\u00a0 \u00a0 .table(\"sessions\", Materialized.<String, UserSession, KeyValueStore<Bytes, byte[]>>as(\"session-store\")\r\n\u00a0 \u00a0 \u00a0 \u00a0 .withTtl(Duration.ofHours(1))\r\n\u00a0 \u00a0 \u00a0 \u00a0 .withValueSerde(userSessionSerde));\r\n\r\nHere, session entries older than 1 hour will be automatically expired and deleted from the local RocksDB store and hence a flush ~ tombstone to changelog topic."}}}
{"project": "KAFKA", "issue_key": "KAFKA-19758", "title": "Weird behavior on Kafka Connect 4.1 class loading", "status": "Resolved", "reporter": "Mario Fiore Vitale", "assignee": "Mickael Maison", "priority": "Blocker", "labels": [], "created": "2025-10-06T13:26:46.000+0000", "updated": "2025-10-23T05:40:37.000+0000", "description": "I have the [DebeziumOpenLineageEmitter|https://github.com/debezium/debezium/blob/main/debezium-openlineage/debezium-openlineage-api/src/main/java/io/debezium/openlineage/DebeziumOpenLineageEmitter.java] class in the *debezium-openlineage-api* that internally has a static map to maintain the registered emitter, the key of this map is \"connectoLogicalName-taskid\"\r\nThen there is the [OpenLineage SMT|https://github.com/debezium/debezium/blob/main/debezium-core/src/main/java/io/debezium/transforms/openlineage/OpenLineage.java], which is part of the *debezium-core.* In this SMT, I simply pass the same context to instantiate the same emitter via the connector.\r\n\r\nNow I'm running the following image\r\n{code:java}\r\nFROM quay.io/debezium/connect:3.3.0.Final\r\nENV MAVEN_REPO=\"https://repo1.maven.org/maven2\"\r\nENV GROUP_ID=\"io/debezium\"\r\nENV DEBEZIUM_VERSION=\"3.3.0.Final\"\r\nENV ARTIFACT_ID=\"debezium-openlineage-core\"\r\nENV CLASSIFIER=\"-libs\"\r\nCOPY log4j.properties /kafka/config/log4j.properties\r\n\r\nAdd OpenLineage\r\nRUN mkdir -p /tmp/openlineage-libs && \\\r\n\u00a0 \u00a0 curl \"$MAVEN_REPO/$GROUP_ID/$ARTIFACT_ID/$DEBEZIUM_VERSION/$ARTIFACT_ID-${DEBEZIUM_VERSION}${CLASSIFIER}.tar.gz\" -o /tmp/debezium-openlineage-core-libs.tar.gz && \\\r\n\u00a0 \u00a0 tar -xzvf /tmp/debezium-openlineage-core-libs.tar.gz -C /tmp/openlineage-libs --strip-components=1\r\n\r\nRUN cp -r /tmp/openlineage-libs/* /kafka/connect/debezium-connector-postgres/\r\nRUN cp -r /tmp/openlineage-libs/* /kafka/connect/debezium-connector-mongodb/\r\nADD openlineage.yml /kafka/ {code}\r\nSo is practically debezium connect image with just openlineage jars copied into postgres and mongodb connector folders.\r\n\r\nWhen I register the PostgreSQL connector\r\n{code:java}\r\n{\r\n  \"name\": \"inventory-connector-postgres\",\r\n  \"config\": {\r\n    \"connector.class\": \"io.debezium.connector.postgresql.PostgresConnector\",\r\n    \"tasks.max\": \"1\",\r\n    \"database.hostname\": \"postgres\",\r\n    \"database.port\": \"5432\",\r\n    \"database.user\": \"postgres\",\r\n    \"database.password\": \"postgres\",\r\n    \"database.server.id\": \"184054\",\r\n    \"database.dbname\": \"postgres\",\r\n    \"topic.prefix\": \"inventory\",\r\n    \"snapshot.mode\": \"initial\",\r\n    \"schema.history.internal.kafka.bootstrap.servers\": \"kafka:9092\",\r\n    \"schema.history.internal.kafka.topic\": \"schema-changes.inventory\",\r\n    \"slot.name\": \"postgres\",\r\n    \"openlineage.integration.enabled\": \"true\",\r\n    \"openlineage.integration.config.file.path\": \"/kafka/openlineage.yml\",\r\n    \"openlineage.integration.job.description\": \"This connector does cdc for products\",\r\n    \"openlineage.integration.tags\": \"env=prod,team=cdc\",\r\n    \"openlineage.integration.owners\": \"Mario=maintainer,John Doe=Data scientist,IronMan=superero\",\r\n    \"transforms\": \"openlineage\",\r\n    \"transforms.openlineage.type\": \"io.debezium.transforms.openlineage.OpenLineage\"\r\n  }\r\n} {code}\r\n\u00a0\r\nI get the following error\r\n{code:java}\r\n2025-10-03T14:22:09,761 ERROR \u00a0|| \u00a0WorkerSourceTask{id=inventory-connector-postgres-0} Task threw an uncaught and unrecoverable exception. Task is being killed and will not recover until manually restarted \u00a0 [org.apache.kafka.connect.runtime.WorkerTask]\r\norg.apache.kafka.connect.errors.ConnectException: Tolerance exceeded in error handler\r\n\u00a0 \u00a0 at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execAndHandleError(RetryWithToleranceOperator.java:260) ~[connect-runtime-4.1.0.jar:?]\r\n\u00a0 \u00a0 at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execute(RetryWithToleranceOperator.java:180) ~[connect-runtime-4.1.0.jar:?]\r\n\u00a0 \u00a0 at org.apache.kafka.connect.runtime.TransformationChain.apply(TransformationChain.java:58) ~[connect-runtime-4.1.0.jar:?]\r\n\u00a0 \u00a0 at org.apache.kafka.connect.runtime.AbstractWorkerSourceTask.sendRecords(AbstractWorkerSourceTask.java:415) ~[connect-runtime-4.1.0.jar:?]\r\n\u00a0 \u00a0 at org.apache.kafka.connect.runtime.AbstractWorkerSourceTask.execute(AbstractWorkerSourceTask.java:376) ~[connect-runtime-4.1.0.jar:?]\r\n\u00a0 \u00a0 at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:243) ~[connect-runtime-4.1.0.jar:?]\r\n\u00a0 \u00a0 at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:298) ~[connect-runtime-4.1.0.jar:?]\r\n\u00a0 \u00a0 at org.apache.kafka.connect.runtime.AbstractWorkerSourceTask.run(AbstractWorkerSourceTask.java:83) ~[connect-runtime-4.1.0.jar:?]\r\n\u00a0 \u00a0 at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:254) ~[connect-runtime-4.1.0.jar:?]\r\n\u00a0 \u00a0 at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572) ~[?:?]\r\n\u00a0 \u00a0 at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317) ~[?:?]\r\n\u00a0 \u00a0 at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144) ~[?:?]\r\n\u00a0 \u00a0 at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642) ~[?:?]\r\n\u00a0 \u00a0 at java.base/java.lang.Thread.run(Thread.java:1583) [?:?]\r\nCaused by: java.lang.IllegalStateException: DebeziumOpenLineageEmitter not initialized for connector ConnectorContext[connectorLogicalName=inventory, connectorName=postgresql, taskId=0, version=null, config=null]. Call init() first.\r\n\u00a0 \u00a0 at io.debezium.openlineage.DebeziumOpenLineageEmitter.getEmitter(DebeziumOpenLineageEmitter.java:176) ~[debezium-openlineage-api-3.3.0.Final.jar:3.3.0.Final]\r\n\u00a0 \u00a0 at io.debezium.openlineage.DebeziumOpenLineageEmitter.emit(DebeziumOpenLineageEmitter.java:153) ~[debezium-openlineage-api-3.3.0.Final.jar:3.3.0.Final]\r\n\u00a0 \u00a0 at io.debezium.transforms.openlineage.OpenLineage.apply(OpenLineage.java:74) ~[debezium-core-3.3.0.Final.jar:3.3.0.Final]\r\n\u00a0 \u00a0 at org.apache.kafka.connect.runtime.TransformationStage.apply(TransformationStage.java:95) ~[connect-runtime-4.1.0.jar:?]\r\n\u00a0 \u00a0 at org.apache.kafka.connect.runtime.TransformationChain.lambda$apply$0(TransformationChain.java:58) ~[connect-runtime-4.1.0.jar:?]\r\n\u00a0 \u00a0 at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execAndRetry(RetryWithToleranceOperator.java:208) ~[connect-runtime-4.1.0.jar:?]\r\n\u00a0 \u00a0 at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execAndHandleError(RetryWithToleranceOperator.java:244) ~[connect-runtime-4.1.0.jar:?]\r\n\u00a0 \u00a0 ... 13 more {code}\r\nFull logs [^connect-service.log]\r\n\r\n\u00a0\r\n\r\nThis is evidence that the emitters map is not shared between the connector and the SMT.\r\n\r\nThe situation becomes weirder if I remove all connectors from the image except PostgreSQL and MongoDB.\r\nIn that case, the PostgreSQL connector works perfectly.\r\n\r\nThe plugins are in the folder */kafka/connect* (that is, the only `plugin.path` configured folder), each under a dedicated folder with their dependencies.\u00a0\r\n\r\nI then started to add more connectors, and it continued to work until I added the SQL Server\u00a0connector.\r\nTo summarize, the problem arises when I put one or all of [sqlserver, spanner,vitess].\r\n\u00a0\r\nThe commonality for these connectors seems to be that they support multi-task. The others don't.\u00a0\r\n\r\nAm I correct that Kafka Connect guarantees that each connector is loaded with an isolated class loader with its dependencies so that the static emitters should be shared between the Connector and the SMT?\r\n\r\nTo add more, if I run the image from 3.2.0.Final (so Kafka 4.0.0) with all connectors, it works fine.\r\n\r\nI did other tests, and things are more and more weird. All tests were done with *{{plugin.path=/kafka/connect}}*\u00a0and *KC 4.1*\r\n\r\nMy original tests were with this directory structure\r\n\r\n\u00a0\r\n{code:java}\r\n/kafka/connect\r\n|___ debezium-connector-postgres\r\n|___ debezium-connector-mongodb\r\n|___ debezium-connector-sqlserver{code}\r\n\u00a0\r\n\r\nIn this case, each connector should be isolated from each others (having a dedicated class loader). In that case, the sharing between the connector and SMT does not work for KC 4.0\r\n\r\nThen I tried with\r\n\r\n\u00a0\r\n{code:java}\r\n/kafka/connect\r\n|___ debezium-connectors\r\n     |___ debezium-connector-postgres\r\n     |___ debezium-connector-mongodb\r\n     |___ debezium-connector-sqlserver{code}\r\n\u00a0\r\n\r\nSo all connectors are not isolated and share the same class loader. In this case, no issue. And I'll say that this is expected.\r\n\r\nThen I tried with\r\n\r\n\u00a0\r\n{code:java}\r\n/kafka/connect\r\n|___ debezium-connectors\r\n|    |___ debezium-connector-postgres\r\n|    |___ debezium-connector-mongodb\r\n|___ debezium-connector-sqlserver{code}\r\n\u00a0\r\n\r\nwhere\u00a0*{{postgres}}*\u00a0and\u00a0*{{mongodb}}*\u00a0are not isolated (same classloader) and\u00a0*{{sqlserver}}* is isolated (different classloader), and in this case, it still works. I expected this to fail as with the first setup.\r\n\r\nThe SMT is in the *debezium-core* jar that and each connector has its own copy\r\nSo in each connector folder, there are:\r\n{code:java}\r\ndebezium-api-3.3.0.Final.jar\r\ndebezium-common-3.3.0.Final.jar\r\ndebezium-connector-[connectorName]-3.3.0.Final.jar\r\ndebezium-core-3.3.0.Final.jar\r\ndebezium-openlineage-api-3.3.0.Final.jar{code}", "comments": [], "derived_tasks": {"summarization": "Weird behavior on Kafka Connect 4.1 class loading", "classification": "feature", "qna": {"question": "What is the issue 'Weird behavior on Kafka Connect 4.1 class loading' about?", "answer": "I have the [DebeziumOpenLineageEmitter|https://github.com/debezium/debezium/blob/main/debezium-openlineage/debezium-openlineage-api/src/main/java/io/debezium/openlineage/DebeziumOpenLineageEmitter.java] class in the *debezium-openlineage-api* that internally has a static map to maintain the registered emitter, the key of this map is \"connectoLogicalName-taskid\"\r\nThen there is the [OpenLineage SMT|https://github.com/debezium/debezium/blob/main/debezium-core/src/main/java/io/debezium/transforms/openlineage/OpenLineage.java], which is part of the *debezium-core.* In this SMT, I simply pass the same context to instantiate the same emitter via the connector.\r\n\r\nNow I'm running the following image\r\n{code:java}\r\nFROM quay.io/debezium/connect:3.3.0.Final\r\nENV MAVEN_REPO=\"https://repo1.maven.org/maven2\"\r\nENV GROUP_ID=\"io/debezium\"\r\nENV DEBEZIUM_VERSION=\"3.3.0.Final\"\r\nENV ARTIFACT_ID=\"debezium-openlineage-core\"\r\nENV CLASSIFIER=\"-libs\"\r\nCOPY log4j.properties /kafka/config/log4j.properties\r\n\r\nAdd OpenLineage\r\nRUN mkdir -p /tmp/openlineage-libs && \\\r\n\u00a0 \u00a0 curl \"$MAVEN_REPO/$GROUP_ID/$ARTIFACT_ID/$DEBEZIUM_VERSION/$ARTIFACT_ID-${DEBEZIUM_VERSION}${CLASSIFIER}.tar.gz\" -o /tmp/debezium-openlineage-core-libs.tar.gz && \\\r\n\u00a0 \u00a0 tar -xzvf /tmp/debezium-openlineage-core-libs.tar.gz -C /tmp/openlineage-libs --strip-components=1\r\n\r\nRUN cp -r /tmp/openlineage-libs/* /kafka/connect/debezium-connector-postgres/\r\nRUN cp -r /tmp/openlineage-libs/* /kafka/connect/debezium-connector-mongodb/\r\nADD openlineage.yml /kafka/ {code}\r\nSo is practically debezium connect image with just openlineage jars copied into postgres and mongodb connector folders.\r\n\r\nWhen I register the PostgreSQL connector\r\n{code:java}\r\n{\r\n  \"name\": \"inventory-connector-postgres\",\r\n  \"config\": {\r\n    \"connector.class\": \"io.debezium.connector.postgresql.PostgresConnector\",\r\n    \"tasks.max\": \"1\",\r\n    \"database.hostname\": \"postgres\",\r\n    \"database.port\": \"5432\",\r\n    \"database.user\": \"postgres\",\r\n    \"database.password\": \"postgres\",\r\n    \"database.server.id\": \"184054\",\r\n    \"database.dbname\": \"postgres\",\r\n    \"topic.prefix\": \"inventory\",\r\n    \"snapshot.mode\": \"initial\",\r\n    \"schema.history.internal.kafka.bootstrap.servers\": \"kafka:9092\",\r\n    \"schema.history.internal.kafka.topic\": \"schema-changes.inventory\",\r\n    \"slot.name\": \"postgres\",\r\n    \"openlineage.integration.enabled\": \"true\",\r\n    \"openlineage.integration.config.file.path\": \"/kafka/openlineage.yml\",\r\n    \"openlineage.integration.job.description\": \"This connector does cdc for products\",\r\n    \"openlineage.integration.tags\": \"env=prod,team=cdc\",\r\n    \"openlineage.integration.owners\": \"Mario=maintainer,John Doe=Data scientist,IronMan=superero\",\r\n    \"transforms\": \"openlineage\",\r\n    \"transforms.openlineage.type\": \"io.debezium.transforms.openlineage.OpenLineage\"\r\n  }\r\n} {code}\r\n\u00a0\r\nI get the following error\r\n{code:java}\r\n2025-10-03T14:22:09,761 ERROR \u00a0|| \u00a0WorkerSourceTask{id=inventory-connector-postgres-0} Task threw an uncaught and unrecoverable exception. Task is being killed and will not recover until manually restarted \u00a0 [org.apache.kafka.connect.runtime.WorkerTask]\r\norg.apache.kafka.connect.errors.ConnectException: Tolerance exceeded in error handler\r\n\u00a0 \u00a0 at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execAndHandleError(RetryWithToleranceOperator.java:260) ~[connect-runtime-4.1.0.jar:?]\r\n\u00a0 \u00a0 at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execute(RetryWithToleranceOperator.java:180) ~[connect-runtime-4.1.0.jar:?]\r\n\u00a0 \u00a0 at org.apache.kafka.connect.runtime.TransformationChain.apply(TransformationChain.java:58) ~[connect-runtime-4.1.0.jar:?]\r\n\u00a0 \u00a0 at org.apache.kafka.connect.runtime.AbstractWorkerSourceTask.sendRecords(AbstractWorkerSourceTask.java:415) ~[connect-runtime-4.1.0.jar:?]\r\n\u00a0 \u00a0 at org.apache.kafka.connect.runtime.AbstractWorkerSourceTask.execute(AbstractWorkerSourceTask.java:376) ~[connect-runtime-4.1.0.jar:?]\r\n\u00a0 \u00a0 at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:243) ~[connect-runtime-4.1.0.jar:?]\r\n\u00a0 \u00a0 at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:298) ~[connect-runtime-4.1.0.jar:?]\r\n\u00a0 \u00a0 at org.apache.kafka.connect.runtime.AbstractWorkerSourceTask.run(AbstractWorkerSourceTask.java:83) ~[connect-runtime-4.1.0.jar:?]\r\n\u00a0 \u00a0 at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:254) ~[connect-runtime-4.1.0.jar:?]\r\n\u00a0 \u00a0 at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572) ~[?:?]\r\n\u00a0 \u00a0 at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317) ~[?:?]\r\n\u00a0 \u00a0 at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144) ~[?:?]\r\n\u00a0 \u00a0 at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642) ~[?:?]\r\n\u00a0 \u00a0 at java.base/java.lang.Thread.run(Thread.java:1583) [?:?]\r\nCaused by: java.lang.IllegalStateException: DebeziumOpenLineageEmitter not initialized for connector ConnectorContext[connectorLogicalName=inventory, connectorName=postgresql, taskId=0, version=null, config=null]. Call init() first.\r\n\u00a0 \u00a0 at io.debezium.openlineage.DebeziumOpenLineageEmitter.getEmitter(DebeziumOpenLineageEmitter.java:176) ~[debezium-openlineage-api-3.3.0.Final.jar:3.3.0.Final]\r\n\u00a0 \u00a0 at io.debezium.openlineage.DebeziumOpenLineageEmitter.emit(DebeziumOpenLineageEmitter.java:153) ~[debezium-openlineage-api-3.3.0.Final.jar:3.3.0.Final]\r\n\u00a0 \u00a0 at io.debezium.transforms.openlineage.OpenLineage.apply(OpenLineage.java:74) ~[debezium-core-3.3.0.Final.jar:3.3.0.Final]\r\n\u00a0 \u00a0 at org.apache.kafka.connect.runtime.TransformationStage.apply(TransformationStage.java:95) ~[connect-runtime-4.1.0.jar:?]\r\n\u00a0 \u00a0 at org.apache.kafka.connect.runtime.TransformationChain.lambda$apply$0(TransformationChain.java:58) ~[connect-runtime-4.1.0.jar:?]\r\n\u00a0 \u00a0 at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execAndRetry(RetryWithToleranceOperator.java:208) ~[connect-runtime-4.1.0.jar:?]\r\n\u00a0 \u00a0 at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execAndHandleError(RetryWithToleranceOperator.java:244) ~[connect-runtime-4.1.0.jar:?]\r\n\u00a0 \u00a0 ... 13 more {code}\r\nFull logs [^connect-service.log]\r\n\r\n\u00a0\r\n\r\nThis is evidence that the emitters map is not shared between the connector and the SMT.\r\n\r\nThe situation becomes weirder if I remove all connectors from the image except PostgreSQL and MongoDB.\r\nIn that case, the PostgreSQL connector works perfectly.\r\n\r\nThe plugins are in the folder */kafka/connect* (that is, the only `plugin.path` configured folder), each under a dedicated folder with their dependencies.\u00a0\r\n\r\nI then started to add more connectors, and it continued to work until I added the SQL Server\u00a0connector.\r\nTo summarize, the problem arises when I put one or all of [sqlserver, spanner,vitess].\r\n\u00a0\r\nThe commonality for these connectors seems to be that they support multi-task. The others don't.\u00a0\r\n\r\nAm I correct that Kafka Connect guarantees that each connector is loaded with an isolated class loader with its dependencies so that the static emitters should be shared between the Connector and the SMT?\r\n\r\nTo add more, if I run the image from 3.2.0.Final (so Kafka 4.0.0) with all connectors, it works fine.\r\n\r\nI did other tests, and things are more and more weird. All tests were done with *{{plugin.path=/kafka/connect}}*\u00a0and *KC 4.1*\r\n\r\nMy original tests were with this directory structure\r\n\r\n\u00a0\r\n{code:java}\r\n/kafka/connect\r\n|___ debezium-connector-postgres\r\n|___ debezium-connector-mongodb\r\n|___ debezium-connector-sqlserver{code}\r\n\u00a0\r\n\r\nIn this case, each connector should be isolated from each others (having a dedicated class loader). In that case, the sharing between the connector and SMT does not work for KC 4.0\r\n\r\nThen I tried with\r\n\r\n\u00a0\r\n{code:java}\r\n/kafka/connect\r\n|___ debezium-connectors\r\n     |___ debezium-connector-postgres\r\n     |___ debezium-connector-mongodb\r\n     |___ debezium-connector-sqlserver{code}\r\n\u00a0\r\n\r\nSo all connectors are not isolated and share the same class loader. In this case, no issue. And I'll say that this is expected.\r\n\r\nThen I tried with\r\n\r\n\u00a0\r\n{code:java}\r\n/kafka/connect\r\n|___ debezium-connectors\r\n|    |___ debezium-connector-postgres\r\n|    |___ debezium-connector-mongodb\r\n|___ debezium-connector-sqlserver{code}\r\n\u00a0\r\n\r\nwhere\u00a0*{{postgres}}*\u00a0and\u00a0*{{mongodb}}*\u00a0are not isolated (same classloader) and\u00a0*{{sqlserver}}* is isolated (different classloader), and in this case, it still works. I expected this to fail as with the first setup.\r\n\r\nThe SMT is in the *debezium-core* jar that and each connector has its own copy\r\nSo in each connector folder, there are:\r\n{code:java}\r\ndebezium-api-3.3.0.Final.jar\r\ndebezium-common-3.3.0.Final.jar\r\ndebezium-connector-[connectorName]-3.3.0.Final.jar\r\ndebezium-core-3.3.0.Final.jar\r\ndebezium-openlineage-api-3.3.0.Final.jar{code}"}}}
{"project": "KAFKA", "issue_key": "KAFKA-19757", "title": "Mark KIP-932 interfaces as stable for GA release", "status": "Open", "reporter": "Andrew Schofield", "assignee": "Andrew Schofield", "priority": "Major", "labels": [], "created": "2025-10-06T12:48:28.000+0000", "updated": "2025-10-06T12:48:28.000+0000", "description": null, "comments": [], "derived_tasks": {"summarization": "Mark KIP-932 interfaces as stable for GA release", "classification": "feature", "qna": {"question": "What is the issue 'Mark KIP-932 interfaces as stable for GA release' about?", "answer": null}}}
{"project": "KAFKA", "issue_key": "KAFKA-19756", "title": "Write down the steps for upgrading Gradle", "status": "Open", "reporter": "Chia-Ping Tsai", "assignee": "Chih-Yuan Chien", "priority": "Minor", "labels": [], "created": "2025-10-06T09:59:20.000+0000", "updated": "2025-10-27T04:29:43.000+0000", "description": "Normally, we only update `gradle-wrapper.properties` and `dependencies.gradle`, but that process is incomplete. The correct steps are shown below.\r\n # upgrade gradle-wrapper.properties to the latest gradle\r\n # upgrade dependencies.gradle as well\r\n # use latest gradle to run command `gradle wrapper`to update gradlew\u00a0\r\n # update wrapper.gradle to ensure the generated \"download command\" works well\r\n\r\n\u00a0", "comments": [], "derived_tasks": {"summarization": "Write down the steps for upgrading Gradle", "classification": "feature", "qna": {"question": "What is the issue 'Write down the steps for upgrading Gradle' about?", "answer": "Normally, we only update `gradle-wrapper.properties` and `dependencies.gradle`, but that process is incomplete. The correct steps are shown below.\r\n # upgrade gradle-wrapper.properties to the latest gradle\r\n # upgrade dependencies.gradle as well\r\n # use latest gradle to run command `gradle wrapper`to update gradlew\u00a0\r\n # update wrapper.gradle to ensure the generated \"download command\" works well\r\n\r\n\u00a0"}}}
{"project": "KAFKA", "issue_key": "KAFKA-19755", "title": "Move `KRaftClusterTest` from core module to server module", "status": "Open", "reporter": "Chia-Ping Tsai", "assignee": "Lan Ding", "priority": "Minor", "labels": [], "created": "2025-10-04T14:33:51.000+0000", "updated": "2025-10-31T07:26:13.000+0000", "description": "It should include following tasks\r\n # rewrite by java\r\n # move to server module", "comments": [], "derived_tasks": {"summarization": "Move `KRaftClusterTest` from core module to server module", "classification": "feature", "qna": {"question": "What is the issue 'Move `KRaftClusterTest` from core module to server module' about?", "answer": "It should include following tasks\r\n # rewrite by java\r\n # move to server module"}}}
{"project": "KAFKA", "issue_key": "KAFKA-19754", "title": "Add RPC-level integration tests for StreamsGroupDescribe", "status": "Resolved", "reporter": "Lucy Liu", "assignee": "Lucy Liu", "priority": "Major", "labels": [], "created": "2025-10-02T16:44:45.000+0000", "updated": "2025-10-17T20:06:51.000+0000", "description": "Add integration tests similar to {{ShareGroupDescribeRequestTest}} and {{ConsumerGroupDescribeRequestTest for StreamsGroupDescribeRequest}}", "comments": [], "derived_tasks": {"summarization": "Add RPC-level integration tests for StreamsGroupDescribe", "classification": "feature", "qna": {"question": "What is the issue 'Add RPC-level integration tests for StreamsGroupDescribe' about?", "answer": "Add integration tests similar to {{ShareGroupDescribeRequestTest}} and {{ConsumerGroupDescribeRequestTest for StreamsGroupDescribeRequest}}"}}}
{"project": "SPARK", "issue_key": "SPARK-54129", "title": "Extract dropping table from catalog from DataFrameWriter in case of overwriting table during creation", "status": "Open", "reporter": "Tigran Manasyan", "assignee": null, "priority": "Minor", "labels": [], "created": "2025-11-01T09:55:16.000+0000", "updated": "2025-11-01T09:55:16.000+0000", "description": "Currently, if we call DataFrameWriter.saveAsTable(tableName) with the SaveMode equal to Overwrite and the DataSource is V1, the table is dropped from catalog directly in the DataFrameWriter and only then the CreateTable command is created.\u00a0\r\n\r\nIn case if we want to somehow change the default behavior using Spark extensions, for instance, postpone the table dropping to decrease the time interval, when the old table is dropped and the new one is not created, such hardcoded behavior in DataFrameWriter makes it almost impossible if we want to use the original saveAsTable method. \r\n\r\nFor instance, if we call df.write.mode(\"overwrite\").insertInto(table_name) then it's possible to control the table removal logic if we provide our own implementation of the spark.sql.sources.commitProtocolClass. In case of saveAsTable it makes no sense, because a table is unconditionally dropped before handling the CreateTable command.\r\n\r\nTherefore, is it possible to extract the table dropping logic from DataFrameWriter to the corresponding LogicalPlan commands (as it was done for V2 Datasources) in order to give more flexibility to Spark extensions?", "comments": [], "derived_tasks": {"summarization": "Extract dropping table from catalog from DataFrameWriter in case of overwriting table during creation", "classification": "feature", "qna": {"question": "What is the issue 'Extract dropping table from catalog from DataFrameWriter in case of overwriting table during creation' about?", "answer": "Currently, if we call DataFrameWriter.saveAsTable(tableName) with the SaveMode equal to Overwrite and the DataSource is V1, the table is dropped from catalog directly in the DataFrameWriter and only then the CreateTable command is created.\u00a0\r\n\r\nIn case if we want to somehow change the default behavior using Spark extensions, for instance, postpone the table dropping to decrease the time interval, when the old table is dropped and the new one is not created, such hardcoded behavior in DataFrameWriter makes it almost impossible if we want to use the original saveAsTable method. \r\n\r\nFor instance, if we call df.write.mode(\"overwrite\").insertInto(table_name) then it's possible to control the table removal logic if we provide our own implementation of the spark.sql.sources.commitProtocolClass. In case of saveAsTable it makes no sense, because a table is unconditionally dropped before handling the CreateTable command.\r\n\r\nTherefore, is it possible to extract the table dropping logic from DataFrameWriter to the corresponding LogicalPlan commands (as it was done for V2 Datasources) in order to give more flexibility to Spark extensions?"}}}
{"project": "SPARK", "issue_key": "SPARK-54128", "title": "Improve Error Handling for Spark Connect", "status": "Open", "reporter": "Martin Grund", "assignee": null, "priority": "Major", "labels": [], "created": "2025-11-01T08:47:45.000+0000", "updated": "2025-11-01T08:47:45.000+0000", "description": "This is an umbrella Jira to migrate exceptions in Spark Connect from plain runtime exceptions to proper Spark Exceptions with error class.", "comments": [], "derived_tasks": {"summarization": "Improve Error Handling for Spark Connect", "classification": "feature", "qna": {"question": "What is the issue 'Improve Error Handling for Spark Connect' about?", "answer": "This is an umbrella Jira to migrate exceptions in Spark Connect from plain runtime exceptions to proper Spark Exceptions with error class."}}}
{"project": "SPARK", "issue_key": "SPARK-54127", "title": "Fix sbt inconsistent shading package", "status": "Open", "reporter": "Cheng Pan", "assignee": null, "priority": "Major", "labels": ["pull-request-available"], "created": "2025-11-01T04:20:38.000+0000", "updated": "2025-11-01T04:29:06.000+0000", "description": null, "comments": [], "derived_tasks": {"summarization": "Fix sbt inconsistent shading package", "classification": "feature", "qna": {"question": "What is the issue 'Fix sbt inconsistent shading package' about?", "answer": null}}}
{"project": "SPARK", "issue_key": "SPARK-54126", "title": "SHOW TBLPROPERTIES AS JSON", "status": "Open", "reporter": "Amanda Liu", "assignee": null, "priority": "Major", "labels": [], "created": "2025-11-01T00:18:18.000+0000", "updated": "2025-11-01T00:20:36.000+0000", "description": null, "comments": [], "derived_tasks": {"summarization": "SHOW TBLPROPERTIES AS JSON", "classification": "feature", "qna": {"question": "What is the issue 'SHOW TBLPROPERTIES AS JSON' about?", "answer": null}}}
{"project": "SPARK", "issue_key": "SPARK-54125", "title": "SHOW DATABASES AS JSON", "status": "Open", "reporter": "Amanda Liu", "assignee": null, "priority": "Major", "labels": [], "created": "2025-11-01T00:14:14.000+0000", "updated": "2025-11-01T00:20:55.000+0000", "description": null, "comments": [], "derived_tasks": {"summarization": "SHOW DATABASES AS JSON", "classification": "feature", "qna": {"question": "What is the issue 'SHOW DATABASES AS JSON' about?", "answer": null}}}
{"project": "SPARK", "issue_key": "SPARK-54124", "title": "SHOW VIEWS AS JSON", "status": "Open", "reporter": "Amanda Liu", "assignee": null, "priority": "Major", "labels": [], "created": "2025-11-01T00:13:05.000+0000", "updated": "2025-11-01T00:21:04.000+0000", "description": null, "comments": [], "derived_tasks": {"summarization": "SHOW VIEWS AS JSON", "classification": "feature", "qna": {"question": "What is the issue 'SHOW VIEWS AS JSON' about?", "answer": null}}}
{"project": "SPARK", "issue_key": "SPARK-54123", "title": "Add timezone to make timestamp absolute time.", "status": "Open", "reporter": "Takuya Ueshin", "assignee": null, "priority": "Major", "labels": ["pull-request-available"], "created": "2025-11-01T00:05:48.000+0000", "updated": "2025-11-01T00:25:46.000+0000", "description": null, "comments": [], "derived_tasks": {"summarization": "Add timezone to make timestamp absolute time.", "classification": "feature", "qna": {"question": "What is the issue 'Add timezone to make timestamp absolute time.' about?", "answer": null}}}
{"project": "SPARK", "issue_key": "SPARK-54122", "title": "Improve the testing experience for TransformWithState", "status": "Open", "reporter": "Dmytro Fedoriaka", "assignee": null, "priority": "Minor", "labels": ["pull-request-available"], "created": "2025-11-01T00:03:04.000+0000", "updated": "2025-11-01T01:12:01.000+0000", "description": "Currently it's hard to write unit tests for user's implementation of StatefulProcessor used in TransformWithState. User either needs to test it by running actual streaming query, or they need to refactor business logic into a separate function (that will be called from handleInputRows), and write unit tests for that function.\r\n\u00a0\r\nI propose to add a simple testing framework for TransformWithState that will allow users easily test their business logic inside StatefulProcessor.\r\n\u00a0\r\nOn high-level, it's a class TwsTester that takes StatefulProcessor and allows to feed in input rows and immediately returns what rows would be produced by TWS. It also allows to set and inspect state. This can be used in unit tests without having to run streaming query and it won't need RocksDB (it will use in-memory state store). I will start with implementing this for Scala users, potentially making it available for Python users later.", "comments": [], "derived_tasks": {"summarization": "Improve the testing experience for TransformWithState", "classification": "feature", "qna": {"question": "What is the issue 'Improve the testing experience for TransformWithState' about?", "answer": "Currently it's hard to write unit tests for user's implementation of StatefulProcessor used in TransformWithState. User either needs to test it by running actual streaming query, or they need to refactor business logic into a separate function (that will be called from handleInputRows), and write unit tests for that function.\r\n\u00a0\r\nI propose to add a simple testing framework for TransformWithState that will allow users easily test their business logic inside StatefulProcessor.\r\n\u00a0\r\nOn high-level, it's a class TwsTester that takes StatefulProcessor and allows to feed in input rows and immediately returns what rows would be produced by TWS. It also allows to set and inspect state. This can be used in unit tests without having to run streaming query and it won't need RocksDB (it will use in-memory state store). I will start with implementing this for Scala users, potentially making it available for Python users later."}}}
{"project": "SPARK", "issue_key": "SPARK-54121", "title": "Automatic Snapshot Repair for State store", "status": "Open", "reporter": "B. Micheal Okutubo", "assignee": null, "priority": "Major", "labels": ["pull-request-available"], "created": "2025-10-31T22:49:00.000+0000", "updated": "2025-11-01T04:54:12.000+0000", "description": "Today, the engine currently treats both the changelog and snapshot files with the same importance, so when a state store needs to be loaded it reads the latest snapshot and applies the subsequent changes on it. If the snapshot is bad or corrupted, then the query will fail and be completely down and blocked, needing manual intervention. This leads to user clearing their query checkpoint and having to do full recomputation.\r\n\r\nThis shouldn\u2019t be the case. The changelog should be treated as the \u201csource of truth\u201d and the snapshot is just a disposable materialization of the log.\r\n\r\nIntroducing Automatic snapshot repair, which will automatically repair the checkpoint by skipping bad snapshots and rebuilding the current state from the last good snapshot (works even if there\u2019s none) and applying the changelogs on it. This eliminates the need for manual intervention and unblocks the pipeline to keep it running.\r\n\r\nAlso emit metrics about number of state stores that were auto repaired in a given batch, so that you can build alert and dashboard for it.", "comments": [], "derived_tasks": {"summarization": "Automatic Snapshot Repair for State store", "classification": "feature", "qna": {"question": "What is the issue 'Automatic Snapshot Repair for State store' about?", "answer": "Today, the engine currently treats both the changelog and snapshot files with the same importance, so when a state store needs to be loaded it reads the latest snapshot and applies the subsequent changes on it. If the snapshot is bad or corrupted, then the query will fail and be completely down and blocked, needing manual intervention. This leads to user clearing their query checkpoint and having to do full recomputation.\r\n\r\nThis shouldn\u2019t be the case. The changelog should be treated as the \u201csource of truth\u201d and the snapshot is just a disposable materialization of the log.\r\n\r\nIntroducing Automatic snapshot repair, which will automatically repair the checkpoint by skipping bad snapshots and rebuilding the current state from the last good snapshot (works even if there\u2019s none) and applying the changelogs on it. This eliminates the need for manual intervention and unblocks the pipeline to keep it running.\r\n\r\nAlso emit metrics about number of state stores that were auto repaired in a given batch, so that you can build alert and dashboard for it."}}}
{"project": "SPARK", "issue_key": "SPARK-54120", "title": "Update assertGeneratedCRDMatchesHelmChart to include diff", "status": "Resolved", "reporter": "Zhou JIANG", "assignee": "Zhou JIANG", "priority": "Major", "labels": ["pull-request-available"], "created": "2025-10-31T21:24:37.000+0000", "updated": "2025-11-01T09:44:53.000+0000", "description": null, "comments": [], "derived_tasks": {"summarization": "Update assertGeneratedCRDMatchesHelmChart to include diff", "classification": "feature", "qna": {"question": "What is the issue 'Update assertGeneratedCRDMatchesHelmChart to include diff' about?", "answer": null}}}
{"project": "SPARK", "issue_key": "SPARK-54119", "title": "Metrics & semantic modeling in Spark", "status": "Open", "reporter": "Linhong Liu", "assignee": null, "priority": "Major", "labels": [], "created": "2025-10-31T20:49:11.000+0000", "updated": "2025-10-31T21:33:37.000+0000", "description": "SPIP: https://docs.google.com/document/d/1xVTLijvDTJ90lZ_ujwzf9HvBJgWg0mY6cYM44Fcghl0/edit?tab=t.0#heading=h.4iogryr5qznc", "comments": [], "derived_tasks": {"summarization": "Metrics & semantic modeling in Spark", "classification": "feature", "qna": {"question": "What is the issue 'Metrics & semantic modeling in Spark' about?", "answer": "SPIP: https://docs.google.com/document/d/1xVTLijvDTJ90lZ_ujwzf9HvBJgWg0mY6cYM44Fcghl0/edit?tab=t.0#heading=h.4iogryr5qznc"}}}
{"project": "SPARK", "issue_key": "SPARK-54118", "title": "Improve the put/merge operation in ListState when t here are multiple values", "status": "Resolved", "reporter": "Huanli Wang", "assignee": "Huanli Wang", "priority": "Major", "labels": ["pull-request-available"], "created": "2025-10-31T20:24:12.000+0000", "updated": "2025-11-01T04:42:03.000+0000", "description": "In SS TWS, when we do the {{put(array)}}\u00a0operation in liststate, we put the first element and then merge the remaining elements one by one. so if we want to put an array with 100 elements, it means we need do 1 put + 99 merges. This can result in worse performance than a single put operation for the entire array.\r\n\r\n\u00a0\r\n\r\nSimilar, we have the same issue in {{merge(array)}}\u00a0\r\n\u00a0", "comments": [], "derived_tasks": {"summarization": "Improve the put/merge operation in ListState when t here are multiple values", "classification": "feature", "qna": {"question": "What is the issue 'Improve the put/merge operation in ListState when t here are multiple values' about?", "answer": "In SS TWS, when we do the {{put(array)}}\u00a0operation in liststate, we put the first element and then merge the remaining elements one by one. so if we want to put an array with 100 elements, it means we need do 1 put + 99 merges. This can result in worse performance than a single put operation for the entire array.\r\n\r\n\u00a0\r\n\r\nSimilar, we have the same issue in {{merge(array)}}\u00a0\r\n\u00a0"}}}
{"project": "SPARK", "issue_key": "SPARK-54117", "title": "Throw better error to indicate that TWS is only supported with RocksDB state store provider", "status": "Open", "reporter": "Dmytro Fedoriaka", "assignee": null, "priority": "Minor", "labels": ["pull-request-available"], "created": "2025-10-31T17:09:35.000+0000", "updated": "2025-10-31T23:11:36.000+0000", "description": "When user tries to use TWS with HDFS as state store, they should get an error explicitly requiring to use RocksDB state provider.", "comments": [], "derived_tasks": {"summarization": "Throw better error to indicate that TWS is only supported with RocksDB state store provider", "classification": "feature", "qna": {"question": "What is the issue 'Throw better error to indicate that TWS is only supported with RocksDB state store provider' about?", "answer": "When user tries to use TWS with HDFS as state store, they should get an error explicitly requiring to use RocksDB state provider."}}}
{"project": "SPARK", "issue_key": "SPARK-54116", "title": "Add off-heap mode support for LongHashedRelation", "status": "Open", "reporter": "Hongze Zhang", "assignee": null, "priority": "Major", "labels": ["pull-request-available"], "created": "2025-10-31T12:13:28.000+0000", "updated": "2025-10-31T12:24:50.000+0000", "description": "LongHashedRelation now only supports on-heap mode. The task is for adding off-heap mode support for the relation.", "comments": [], "derived_tasks": {"summarization": "Add off-heap mode support for LongHashedRelation", "classification": "feature", "qna": {"question": "What is the issue 'Add off-heap mode support for LongHashedRelation' about?", "answer": "LongHashedRelation now only supports on-heap mode. The task is for adding off-heap mode support for the relation."}}}
{"project": "SPARK", "issue_key": "SPARK-54115", "title": "Display connect server execution threads first in thread dump page", "status": "Open", "reporter": "Cheng Pan", "assignee": null, "priority": "Major", "labels": ["pull-request-available"], "created": "2025-10-31T09:50:13.000+0000", "updated": "2025-10-31T17:04:26.000+0000", "description": null, "comments": [], "derived_tasks": {"summarization": "Display connect server execution threads first in thread dump page", "classification": "feature", "qna": {"question": "What is the issue 'Display connect server execution threads first in thread dump page' about?", "answer": null}}}
{"project": "SPARK", "issue_key": "SPARK-54114", "title": "Support getColumns for SparkConnectDatabaseMetaData", "status": "Open", "reporter": "Cheng Pan", "assignee": null, "priority": "Major", "labels": [], "created": "2025-10-31T09:03:00.000+0000", "updated": "2025-10-31T09:03:00.000+0000", "description": null, "comments": [], "derived_tasks": {"summarization": "Support getColumns for SparkConnectDatabaseMetaData", "classification": "feature", "qna": {"question": "What is the issue 'Support getColumns for SparkConnectDatabaseMetaData' about?", "answer": null}}}
{"project": "SPARK", "issue_key": "SPARK-54113", "title": "Support getTables for SparkConnectDatabaseMetaData", "status": "Open", "reporter": "Cheng Pan", "assignee": null, "priority": "Major", "labels": [], "created": "2025-10-31T09:02:33.000+0000", "updated": "2025-10-31T09:02:33.000+0000", "description": null, "comments": [], "derived_tasks": {"summarization": "Support getTables for SparkConnectDatabaseMetaData", "classification": "feature", "qna": {"question": "What is the issue 'Support getTables for SparkConnectDatabaseMetaData' about?", "answer": null}}}
{"project": "SPARK", "issue_key": "SPARK-54112", "title": "Support getSchemas for SparkConnectDatabaseMetaData", "status": "Open", "reporter": "Cheng Pan", "assignee": null, "priority": "Major", "labels": ["pull-request-available"], "created": "2025-10-31T09:02:03.000+0000", "updated": "2025-10-31T17:46:19.000+0000", "description": null, "comments": [], "derived_tasks": {"summarization": "Support getSchemas for SparkConnectDatabaseMetaData", "classification": "feature", "qna": {"question": "What is the issue 'Support getSchemas for SparkConnectDatabaseMetaData' about?", "answer": null}}}
{"project": "SPARK", "issue_key": "SPARK-54111", "title": "Support getCatalogs for SparkConnectDatabaseMetaData", "status": "Open", "reporter": "Cheng Pan", "assignee": null, "priority": "Major", "labels": ["pull-request-available"], "created": "2025-10-31T09:01:36.000+0000", "updated": "2025-10-31T09:34:27.000+0000", "description": null, "comments": [], "derived_tasks": {"summarization": "Support getCatalogs for SparkConnectDatabaseMetaData", "classification": "feature", "qna": {"question": "What is the issue 'Support getCatalogs for SparkConnectDatabaseMetaData' about?", "answer": null}}}
{"project": "SPARK", "issue_key": "SPARK-54110", "title": "Introduce type encoders for Geography and Geometry types", "status": "Open", "reporter": "Uro\u0161 Bojani\u0107", "assignee": null, "priority": "Major", "labels": ["pull-request-available"], "created": "2025-10-31T08:32:57.000+0000", "updated": "2025-11-01T09:22:54.000+0000", "description": null, "comments": [], "derived_tasks": {"summarization": "Introduce type encoders for Geography and Geometry types", "classification": "feature", "qna": {"question": "What is the issue 'Introduce type encoders for Geography and Geometry types' about?", "answer": null}}}
{"project": "SPARK", "issue_key": "SPARK-54109", "title": "Avoid function conflicts in test_pandas_grouped_map", "status": "Resolved", "reporter": "Ruifeng Zheng", "assignee": "Ruifeng Zheng", "priority": "Major", "labels": ["pull-request-available"], "created": "2025-10-31T07:47:22.000+0000", "updated": "2025-10-31T09:38:25.000+0000", "description": null, "comments": [], "derived_tasks": {"summarization": "Avoid function conflicts in test_pandas_grouped_map", "classification": "feature", "qna": {"question": "What is the issue 'Avoid function conflicts in test_pandas_grouped_map' about?", "answer": null}}}
{"project": "SPARK", "issue_key": "SPARK-54108", "title": "Revise execute methods of SparkConnectStatement", "status": "Open", "reporter": "Cheng Pan", "assignee": null, "priority": "Major", "labels": ["pull-request-available"], "created": "2025-10-31T06:24:19.000+0000", "updated": "2025-10-31T07:22:06.000+0000", "description": null, "comments": [], "derived_tasks": {"summarization": "Revise execute methods of SparkConnectStatement", "classification": "feature", "qna": {"question": "What is the issue 'Revise execute methods of SparkConnectStatement' about?", "answer": null}}}
{"project": "SPARK", "issue_key": "SPARK-54107", "title": "Use `4.1.0-preview3-java21-scala` image for preview examples ", "status": "Resolved", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "priority": "Major", "labels": ["pull-request-available"], "created": "2025-10-31T05:41:26.000+0000", "updated": "2025-10-31T06:10:40.000+0000", "description": null, "comments": [], "derived_tasks": {"summarization": "Use `4.1.0-preview3-java21-scala` image for preview examples ", "classification": "feature", "qna": {"question": "What is the issue 'Use `4.1.0-preview3-java21-scala` image for preview examples ' about?", "answer": null}}}
{"project": "SPARK", "issue_key": "SPARK-54106", "title": "State Store Row Checksum implementation", "status": "Resolved", "reporter": "B. Micheal Okutubo", "assignee": "B. Micheal Okutubo", "priority": "Major", "labels": ["pull-request-available"], "created": "2025-10-31T05:23:24.000+0000", "updated": "2025-11-01T00:17:55.000+0000", "description": "Introduce row checksum creation and verification for state store, both {*}HDFS and RocksDB state store{*}. This will help detect corruption at the row level and help prevent corruption.", "comments": [], "derived_tasks": {"summarization": "State Store Row Checksum implementation", "classification": "feature", "qna": {"question": "What is the issue 'State Store Row Checksum implementation' about?", "answer": "Introduce row checksum creation and verification for state store, both {*}HDFS and RocksDB state store{*}. This will help detect corruption at the row level and help prevent corruption."}}}
{"project": "SPARK", "issue_key": "SPARK-54105", "title": "Update `integration-test-ubuntu-spark41` to use Spark `4.1.0-preview3-java21`", "status": "Resolved", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "priority": "Major", "labels": ["pull-request-available"], "created": "2025-10-31T05:17:51.000+0000", "updated": "2025-10-31T05:38:38.000+0000", "description": null, "comments": [], "derived_tasks": {"summarization": "Update `integration-test-ubuntu-spark41` to use Spark `4.1.0-preview3-java21`", "classification": "feature", "qna": {"question": "What is the issue 'Update `integration-test-ubuntu-spark41` to use Spark `4.1.0-preview3-java21`' about?", "answer": null}}}
{"project": "SPARK", "issue_key": "SPARK-54104", "title": "Disallow casting geospatial types to/from other data types", "status": "Resolved", "reporter": "Uro\u0161 Bojani\u0107", "assignee": "Uro\u0161 Bojani\u0107", "priority": "Major", "labels": ["pull-request-available"], "created": "2025-10-31T02:25:41.000+0000", "updated": "2025-10-31T15:32:09.000+0000", "description": null, "comments": [], "derived_tasks": {"summarization": "Disallow casting geospatial types to/from other data types", "classification": "feature", "qna": {"question": "What is the issue 'Disallow casting geospatial types to/from other data types' about?", "answer": null}}}
{"project": "SPARK", "issue_key": "SPARK-54103", "title": "Introduce client-side Geography and Geometry classes", "status": "Resolved", "reporter": "Uro\u0161 Bojani\u0107", "assignee": "Uro\u0161 Bojani\u0107", "priority": "Major", "labels": ["pull-request-available"], "created": "2025-10-31T00:00:31.000+0000", "updated": "2025-10-31T06:51:19.000+0000", "description": null, "comments": [], "derived_tasks": {"summarization": "Introduce client-side Geography and Geometry classes", "classification": "feature", "qna": {"question": "What is the issue 'Introduce client-side Geography and Geometry classes' about?", "answer": null}}}
{"project": "SPARK", "issue_key": "SPARK-54102", "title": "Spark 4.0.1 still throws \"String length (20054016) exceeds the maximum length (20000000)\" and \"from_json\" fails on a very large JSON with a jackson_core parse error", "status": "Open", "reporter": "Yuan Yuan", "assignee": null, "priority": "Major", "labels": [], "created": "2025-10-30T23:41:04.000+0000", "updated": "2025-10-30T23:41:04.000+0000", "description": "Based on JIRA *SPARK-49872* and the implementation in [{{JsonProtocol.scala}}|https://github.com/apache/spark/blob/29434ea766b0fc3c3bf6eaadb43a8f931133649e/core/src/main/scala/org/apache/spark/util/JsonProtocol.scala#L71], the relevant limit was removed in {*}4.0.1{*}. However, in our environment we can still reliably reproduce:\r\n # When generating/processing a very large string:\r\n\r\n{code:java}\r\nCaused by: com.fasterxml.jackson.core.exc.StreamConstraintsException: String value length (20040525) exceeds the maximum allowed (20000000, from `StreamReadConstraints.getMaxStringLength()`){code}\r\n # When using {{from_json}} on a _valid_ very large single-line JSON (no missing comma), Jackson throws at around column {*}20,271,838{*}:\r\n\r\n{code:java}\r\nCaused by: com.fasterxml.jackson.core.JsonParseException: Unexpected character ('1' (code 49)): was expecting comma to separate Object entries\r\n at [Source: UNKNOWN; line: 1, column: 20271838]{code}\r\nI'm sure this is not a formatting issue. If I truncate the JSON to below column {*}20,271,838{*}, it parses successfully.\r\nHere is my parsing code:\r\n\r\n{code:java}\r\nraw_df.withColumn(\"parsed_item\", f.from_json(f.col(\"item\"), my_schema){code}", "comments": [], "derived_tasks": {"summarization": "Spark 4.0.1 still throws \"String length (20054016) exceeds the maximum length (20000000)\" and \"from_json\" fails on a very large JSON with a jackson_core parse error", "classification": "feature", "qna": {"question": "What is the issue 'Spark 4.0.1 still throws \"String length (20054016) exceeds the maximum length (20000000)\" and \"from_json\" fails on a very large JSON with a jackson_core parse error' about?", "answer": "Based on JIRA *SPARK-49872* and the implementation in [{{JsonProtocol.scala}}|https://github.com/apache/spark/blob/29434ea766b0fc3c3bf6eaadb43a8f931133649e/core/src/main/scala/org/apache/spark/util/JsonProtocol.scala#L71], the relevant limit was removed in {*}4.0.1{*}. However, in our environment we can still reliably reproduce:\r\n # When generating/processing a very large string:\r\n\r\n{code:java}\r\nCaused by: com.fasterxml.jackson.core.exc.StreamConstraintsException: String value length (20040525) exceeds the maximum allowed (20000000, from `StreamReadConstraints.getMaxStringLength()`){code}\r\n # When using {{from_json}} on a _valid_ very large single-line JSON (no missing comma), Jackson throws at around column {*}20,271,838{*}:\r\n\r\n{code:java}\r\nCaused by: com.fasterxml.jackson.core.JsonParseException: Unexpected character ('1' (code 49)): was expecting comma to separate Object entries\r\n at [Source: UNKNOWN; line: 1, column: 20271838]{code}\r\nI'm sure this is not a formatting issue. If I truncate the JSON to below column {*}20,271,838{*}, it parses successfully.\r\nHere is my parsing code:\r\n\r\n{code:java}\r\nraw_df.withColumn(\"parsed_item\", f.from_json(f.col(\"item\"), my_schema){code}"}}}
{"project": "SPARK", "issue_key": "SPARK-54101", "title": "Introduce the framework for adding ST functions in Scala", "status": "Resolved", "reporter": "Uro\u0161 Bojani\u0107", "assignee": "Uro\u0161 Bojani\u0107", "priority": "Major", "labels": ["pull-request-available"], "created": "2025-10-30T22:53:14.000+0000", "updated": "2025-10-31T09:20:51.000+0000", "description": null, "comments": [], "derived_tasks": {"summarization": "Introduce the framework for adding ST functions in Scala", "classification": "feature", "qna": {"question": "What is the issue 'Introduce the framework for adding ST functions in Scala' about?", "answer": null}}}
{"project": "SPARK", "issue_key": "SPARK-54100", "title": "Remove `ignore.symbol.file` Javac option", "status": "Resolved", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "priority": "Major", "labels": ["pull-request-available"], "created": "2025-10-30T22:09:49.000+0000", "updated": "2025-10-31T03:55:13.000+0000", "description": null, "comments": [], "derived_tasks": {"summarization": "Remove `ignore.symbol.file` Javac option", "classification": "feature", "qna": {"question": "What is the issue 'Remove `ignore.symbol.file` Javac option' about?", "answer": null}}}
{"project": "SPARK", "issue_key": "SPARK-54099", "title": "XML to Variant conversion throws ArithmeticException on decimals with extreme exponents", "status": "Open", "reporter": "Xiaonan Yang", "assignee": null, "priority": "Major", "labels": ["pull-request-available"], "created": "2025-10-30T21:30:34.000+0000", "updated": "2025-10-31T00:30:02.000+0000", "description": "When parsing XML data with `parse_xml` that contains decimal numbers with very large\u00a0\r\nexponents (e.g., \"1E+2147483647\"), the conversion to Variant type fails with:\r\n\r\n```\r\njava.lang.ArithmeticException: BigInteger would overflow supported range\r\n\u00a0 \u00a0 at java.base/java.math.BigDecimal.setScale(BigDecimal.java:3000)\r\n\u00a0 \u00a0 at org.apache.spark.sql.catalyst.xml.StaxXmlParser$.org$apache$spark$sql$catalyst$xml$StaxXmlParser$$appendXMLCharacterToVariant(StaxXmlParser.scala:1285)\r\n```", "comments": [], "derived_tasks": {"summarization": "XML to Variant conversion throws ArithmeticException on decimals with extreme exponents", "classification": "feature", "qna": {"question": "What is the issue 'XML to Variant conversion throws ArithmeticException on decimals with extreme exponents' about?", "answer": "When parsing XML data with `parse_xml` that contains decimal numbers with very large\u00a0\r\nexponents (e.g., \"1E+2147483647\"), the conversion to Variant type fails with:\r\n\r\n```\r\njava.lang.ArithmeticException: BigInteger would overflow supported range\r\n\u00a0 \u00a0 at java.base/java.math.BigDecimal.setScale(BigDecimal.java:3000)\r\n\u00a0 \u00a0 at org.apache.spark.sql.catalyst.xml.StaxXmlParser$.org$apache$spark$sql$catalyst$xml$StaxXmlParser$$appendXMLCharacterToVariant(StaxXmlParser.scala:1285)\r\n```"}}}
{"project": "SPARK", "issue_key": "SPARK-54098", "title": "Set `--sun-misc-unsafe-memory-access=allow` for `JavaExec` Gradle tasks", "status": "Open", "reporter": "Dongjoon Hyun", "assignee": null, "priority": "Major", "labels": ["pull-request-available"], "created": "2025-10-30T21:25:47.000+0000", "updated": "2025-10-30T21:45:58.000+0000", "description": null, "comments": [], "derived_tasks": {"summarization": "Set `--sun-misc-unsafe-memory-access=allow` for `JavaExec` Gradle tasks", "classification": "feature", "qna": {"question": "What is the issue 'Set `--sun-misc-unsafe-memory-access=allow` for `JavaExec` Gradle tasks' about?", "answer": null}}}
{"project": "SPARK", "issue_key": "SPARK-54097", "title": "Upgrade `Gradle` to 9.2.0", "status": "Resolved", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "priority": "Major", "labels": ["pull-request-available"], "created": "2025-10-30T21:10:46.000+0000", "updated": "2025-10-30T21:23:53.000+0000", "description": null, "comments": [], "derived_tasks": {"summarization": "Upgrade `Gradle` to 9.2.0", "classification": "feature", "qna": {"question": "What is the issue 'Upgrade `Gradle` to 9.2.0' about?", "answer": null}}}
{"project": "SPARK", "issue_key": "SPARK-54096", "title": "Support Spatial Reference System mapping in PySpark", "status": "Resolved", "reporter": "Uro\u0161 Bojani\u0107", "assignee": "Uro\u0161 Bojani\u0107", "priority": "Major", "labels": ["pull-request-available"], "created": "2025-10-30T19:29:49.000+0000", "updated": "2025-10-31T07:54:59.000+0000", "description": null, "comments": [], "derived_tasks": {"summarization": "Support Spatial Reference System mapping in PySpark", "classification": "feature", "qna": {"question": "What is the issue 'Support Spatial Reference System mapping in PySpark' about?", "answer": null}}}
{"project": "SPARK", "issue_key": "SPARK-54095", "title": "Release Spark Connect Swift Client 0.6.0", "status": "Open", "reporter": "Dongjoon Hyun", "assignee": null, "priority": "Major", "labels": [], "created": "2025-10-30T17:49:49.000+0000", "updated": "2025-10-30T17:50:32.000+0000", "description": null, "comments": [], "derived_tasks": {"summarization": "Release Spark Connect Swift Client 0.6.0", "classification": "feature", "qna": {"question": "What is the issue 'Release Spark Connect Swift Client 0.6.0' about?", "answer": null}}}
{"project": "SPARK", "issue_key": "SPARK-54094", "title": "Extract common methods to KafkaOffsetReaderBase", "status": "Resolved", "reporter": "L. C. Hsieh", "assignee": "L. C. Hsieh", "priority": "Minor", "labels": ["pull-request-available"], "created": "2025-10-30T17:45:28.000+0000", "updated": "2025-10-30T22:08:46.000+0000", "description": "When reviewing https://github.com/apache/spark/pull/52729, I found that KafkaOffsetReaderConsumer and KafkaOffsetReaderAdmin have the exactly same getOffsetRangesFromResolvedOffsets method. The method is actually long so seems good to extract them to common one.", "comments": [], "derived_tasks": {"summarization": "Extract common methods to KafkaOffsetReaderBase", "classification": "feature", "qna": {"question": "What is the issue 'Extract common methods to KafkaOffsetReaderBase' about?", "answer": "When reviewing https://github.com/apache/spark/pull/52729, I found that KafkaOffsetReaderConsumer and KafkaOffsetReaderAdmin have the exactly same getOffsetRangesFromResolvedOffsets method. The method is actually long so seems good to extract them to common one."}}}
{"project": "SPARK", "issue_key": "SPARK-54093", "title": "Release Spark Kubernetes Operator 0.7.0", "status": "Open", "reporter": "Dongjoon Hyun", "assignee": null, "priority": "Major", "labels": [], "created": "2025-10-30T15:49:36.000+0000", "updated": "2025-10-30T15:49:54.000+0000", "description": null, "comments": [], "derived_tasks": {"summarization": "Release Spark Kubernetes Operator 0.7.0", "classification": "feature", "qna": {"question": "What is the issue 'Release Spark Kubernetes Operator 0.7.0' about?", "answer": null}}}
{"project": "SPARK", "issue_key": "SPARK-54092", "title": "Use Java-friendly `KubernetesClientUtils` APIs", "status": "Resolved", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "priority": "Major", "labels": ["pull-request-available"], "created": "2025-10-30T15:24:09.000+0000", "updated": "2025-10-30T17:27:48.000+0000", "description": null, "comments": [], "derived_tasks": {"summarization": "Use Java-friendly `KubernetesClientUtils` APIs", "classification": "feature", "qna": {"question": "What is the issue 'Use Java-friendly `KubernetesClientUtils` APIs' about?", "answer": null}}}
{"project": "SPARK", "issue_key": "SPARK-54091", "title": "Implement the ST_Srid expression in SQL", "status": "Resolved", "reporter": "Uro\u0161 Bojani\u0107", "assignee": "Uro\u0161 Bojani\u0107", "priority": "Major", "labels": ["pull-request-available"], "created": "2025-10-30T13:20:13.000+0000", "updated": "2025-10-31T04:42:39.000+0000", "description": null, "comments": [], "derived_tasks": {"summarization": "Implement the ST_Srid expression in SQL", "classification": "feature", "qna": {"question": "What is the issue 'Implement the ST_Srid expression in SQL' about?", "answer": null}}}
{"project": "SPARK", "issue_key": "SPARK-54090", "title": "AssertDataframeEqual carries rows when showing differences", "status": "Open", "reporter": "Aimilios Tsouvelekakis", "assignee": null, "priority": "Major", "labels": [], "created": "2025-10-30T10:44:10.000+0000", "updated": "2025-10-30T13:09:58.000+0000", "description": "When we try to do assertDataFrameEqual, on two dataframes that have not the same amount of rows, the output gets cascading from the difference till the end. Why this is happening:\r\n\r\n[https://github.com/apache/spark/blob/067969ff946712eeabf47040415f25000837cd87/python/pyspark/testing/utils.py#L1036]\r\n{code:java}\r\n\u00a0 \u00a0 def assert_rows_equal(\r\n\u00a0 \u00a0 \u00a0 \u00a0 rows1: List[Row], rows2: List[Row], maxErrors: int = None, showOnlyDiff: bool = False\r\n\u00a0 \u00a0 ):\r\n\u00a0 \u00a0 \u00a0 \u00a0 __tracebackhide__ = True\r\n\u00a0 \u00a0 \u00a0 \u00a0 zipped = list(zip_longest(rows1, rows2))\r\n\u00a0 \u00a0 \u00a0 \u00a0 diff_rows_cnt = 0\r\n\u00a0 \u00a0 \u00a0 \u00a0 diff_rows = []\r\n\u00a0 \u00a0 \u00a0 \u00a0 has_diff_rows = False\u00a0 \u00a0 \u00a0 \u00a0 \r\n        rows_str1 = \"\"\r\n\u00a0 \u00a0 \u00a0 \u00a0 rows_str2 = \"\"\u00a0 \u00a0 \u00a0 \u00a0 \r\n        \r\n        # count different rows\r\n\u00a0 \u00a0 \u00a0 \u00a0 for r1, r2 in zipped:\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 if not compare_rows(r1, r2):\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 diff_rows_cnt += 1\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 has_diff_rows = True\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 if includeDiffRows:\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 diff_rows.append((r1, r2))\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 rows_str1 += str(r1) + \"\\n\"\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 rows_str2 += str(r2) + \"\\n\"\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 if maxErrors is not None and diff_rows_cnt >= maxErrors:\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 break\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 elif not showOnlyDiff:\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 rows_str1 += str(r1) + \"\\n\"\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 rows_str2 += str(r2) + \"\\n\"\u00a0 \u00a0 \u00a0 \u00a0 \r\n        generated_diff = _context_diff(\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 actual=rows_str1.splitlines(), expected=rows_str2.splitlines(), n=len(zipped)\r\n\u00a0 \u00a0 \u00a0 \u00a0 )\u00a0 \u00a0 \u00a0 \u00a0 \r\n        if has_diff_rows:\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 error_msg = \"Results do not match: \"\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 percent_diff = (diff_rows_cnt / len(zipped)) * 100\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 error_msg += \"( %.5f %% )\" % percent_diff\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 error_msg += \"\\n\" + \"\\n\".join(generated_diff)\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 data = diff_rows if includeDiffRows else None\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 raise PySparkAssertionError(\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 errorClass=\"DIFFERENT_ROWS\", messageParameters={\"error_msg\": error_msg}, data=data\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ){code}\r\nThe problem lies in the way that we zip the lines\r\n{code:java}\r\nzipped = list(zip_longest(rows1, rows2)){code}\r\nWith zip longest we assume that the rows are in order and we do position by position comparison but it does not work well with checkRowOrder which defaults to False.\r\n\r\nIf I have 1 line difference in 100 line dataframe the result percentage won't be 1% but the amount of rows that cascade towards on from that difference.\u00a0\r\n\r\nThe best solution here would be to have a set based comparison and return the percentage and the rows over that.\r\n\r\nA sample of what zip_longest is doing:\r\n{code:java}\r\nfrom itertools import zip_longest\r\nrows1 = [    'A',    'B',    'C',    'D',    'E']\r\nrows2 = [    'A',    'C',    'D',]\r\nzipped = list(zip_longest(rows1, rows2))zipped {code}\r\nResult:\r\n{code:java}\r\n[('A', 'A'), ('B', 'C'), ('C', 'D'), ('D', None), ('E', None)]{code}\r\nSo in this case we would have 80% rows failure.\r\n\r\nThis comes directly with the implementation of CheckRowOrder, when it is True we do not sort and it makes sense to use the zip_longest, when it is False it makes sense to use set based comparison since we have already sorted.\r\n\r\n\u00a0", "comments": [], "derived_tasks": {"summarization": "AssertDataframeEqual carries rows when showing differences", "classification": "feature", "qna": {"question": "What is the issue 'AssertDataframeEqual carries rows when showing differences' about?", "answer": "When we try to do assertDataFrameEqual, on two dataframes that have not the same amount of rows, the output gets cascading from the difference till the end. Why this is happening:\r\n\r\n[https://github.com/apache/spark/blob/067969ff946712eeabf47040415f25000837cd87/python/pyspark/testing/utils.py#L1036]\r\n{code:java}\r\n\u00a0 \u00a0 def assert_rows_equal(\r\n\u00a0 \u00a0 \u00a0 \u00a0 rows1: List[Row], rows2: List[Row], maxErrors: int = None, showOnlyDiff: bool = False\r\n\u00a0 \u00a0 ):\r\n\u00a0 \u00a0 \u00a0 \u00a0 __tracebackhide__ = True\r\n\u00a0 \u00a0 \u00a0 \u00a0 zipped = list(zip_longest(rows1, rows2))\r\n\u00a0 \u00a0 \u00a0 \u00a0 diff_rows_cnt = 0\r\n\u00a0 \u00a0 \u00a0 \u00a0 diff_rows = []\r\n\u00a0 \u00a0 \u00a0 \u00a0 has_diff_rows = False\u00a0 \u00a0 \u00a0 \u00a0 \r\n        rows_str1 = \"\"\r\n\u00a0 \u00a0 \u00a0 \u00a0 rows_str2 = \"\"\u00a0 \u00a0 \u00a0 \u00a0 \r\n        \r\n        # count different rows\r\n\u00a0 \u00a0 \u00a0 \u00a0 for r1, r2 in zipped:\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 if not compare_rows(r1, r2):\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 diff_rows_cnt += 1\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 has_diff_rows = True\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 if includeDiffRows:\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 diff_rows.append((r1, r2))\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 rows_str1 += str(r1) + \"\\n\"\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 rows_str2 += str(r2) + \"\\n\"\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 if maxErrors is not None and diff_rows_cnt >= maxErrors:\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 break\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 elif not showOnlyDiff:\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 rows_str1 += str(r1) + \"\\n\"\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 rows_str2 += str(r2) + \"\\n\"\u00a0 \u00a0 \u00a0 \u00a0 \r\n        generated_diff = _context_diff(\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 actual=rows_str1.splitlines(), expected=rows_str2.splitlines(), n=len(zipped)\r\n\u00a0 \u00a0 \u00a0 \u00a0 )\u00a0 \u00a0 \u00a0 \u00a0 \r\n        if has_diff_rows:\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 error_msg = \"Results do not match: \"\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 percent_diff = (diff_rows_cnt / len(zipped)) * 100\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 error_msg += \"( %.5f %% )\" % percent_diff\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 error_msg += \"\\n\" + \"\\n\".join(generated_diff)\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 data = diff_rows if includeDiffRows else None\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 raise PySparkAssertionError(\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 errorClass=\"DIFFERENT_ROWS\", messageParameters={\"error_msg\": error_msg}, data=data\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ){code}\r\nThe problem lies in the way that we zip the lines\r\n{code:java}\r\nzipped = list(zip_longest(rows1, rows2)){code}\r\nWith zip longest we assume that the rows are in order and we do position by position comparison but it does not work well with checkRowOrder which defaults to False.\r\n\r\nIf I have 1 line difference in 100 line dataframe the result percentage won't be 1% but the amount of rows that cascade towards on from that difference.\u00a0\r\n\r\nThe best solution here would be to have a set based comparison and return the percentage and the rows over that.\r\n\r\nA sample of what zip_longest is doing:\r\n{code:java}\r\nfrom itertools import zip_longest\r\nrows1 = [    'A',    'B',    'C',    'D',    'E']\r\nrows2 = [    'A',    'C',    'D',]\r\nzipped = list(zip_longest(rows1, rows2))zipped {code}\r\nResult:\r\n{code:java}\r\n[('A', 'A'), ('B', 'C'), ('C', 'D'), ('D', None), ('E', None)]{code}\r\nSo in this case we would have 80% rows failure.\r\n\r\nThis comes directly with the implementation of CheckRowOrder, when it is True we do not sort and it makes sense to use the zip_longest, when it is False it makes sense to use set based comparison since we have already sorted.\r\n\r\n\u00a0"}}}
{"project": "SPARK", "issue_key": "SPARK-54089", "title": "Add off-heap mode support for on-heap-only memory consumers", "status": "Open", "reporter": "Hongze Zhang", "assignee": null, "priority": "Major", "labels": [], "created": "2025-10-30T10:32:48.000+0000", "updated": "2025-10-31T15:02:55.000+0000", "description": "There are a few memory consumers that only support on-heap mode. Including:\r\n\r\n# LongToUnsafeRowMap (for long key hash join)\r\n# ExternalSorter (for non-serializable sort-based shuffle)\r\n\r\nIt's ideal to make them support off-heap when Spark is running with off-heap memory mode. For 3rd accelerator plugins that leverage Spark off-heap memory, having the large allocations in vanilla Spark supporting off-heap mode will significantly ease the memory settings for running the mixed query plan that contains both vanilla Spark and offloaded computations. ", "comments": [], "derived_tasks": {"summarization": "Add off-heap mode support for on-heap-only memory consumers", "classification": "feature", "qna": {"question": "What is the issue 'Add off-heap mode support for on-heap-only memory consumers' about?", "answer": "There are a few memory consumers that only support on-heap mode. Including:\r\n\r\n# LongToUnsafeRowMap (for long key hash join)\r\n# ExternalSorter (for non-serializable sort-based shuffle)\r\n\r\nIt's ideal to make them support off-heap when Spark is running with off-heap memory mode. For 3rd accelerator plugins that leverage Spark off-heap memory, having the large allocations in vanilla Spark supporting off-heap mode will significantly ease the memory settings for running the mixed query plan that contains both vanilla Spark and offloaded computations. "}}}
{"project": "SPARK", "issue_key": "SPARK-54088", "title": "When TaskSchedulerImpl searches for an executor to schedule an unschedulable task, it should exclude executors that are pending to remove", "status": "Open", "reporter": "angerszhu", "assignee": null, "priority": "Major", "labels": ["pull-request-available"], "created": "2025-10-30T09:14:40.000+0000", "updated": "2025-11-01T07:25:22.000+0000", "description": "Call kill executor\r\n\r\n!image-2025-10-30-17-22-25-127.png|width=1255,height=241!\r\n\r\nContainer 18 didn't kill it\r\n\u00a0\r\n!image-2025-10-30-17-24-03-632.png|width=1087,height=183!\r\n\r\n\u00a0\r\n\r\nPending container causing\u00a0 task can't be scheduled .\r\n\r\n\u00a0", "comments": [], "derived_tasks": {"summarization": "When TaskSchedulerImpl searches for an executor to schedule an unschedulable task, it should exclude executors that are pending to remove", "classification": "feature", "qna": {"question": "What is the issue 'When TaskSchedulerImpl searches for an executor to schedule an unschedulable task, it should exclude executors that are pending to remove' about?", "answer": "Call kill executor\r\n\r\n!image-2025-10-30-17-22-25-127.png|width=1255,height=241!\r\n\r\nContainer 18 didn't kill it\r\n\u00a0\r\n!image-2025-10-30-17-24-03-632.png|width=1087,height=183!\r\n\r\n\u00a0\r\n\r\nPending container causing\u00a0 task can't be scheduled .\r\n\r\n\u00a0"}}}
{"project": "SPARK", "issue_key": "SPARK-54087", "title": "Spark Executor launch task failed should return task killed message", "status": "Open", "reporter": "angerszhu", "assignee": null, "priority": "Major", "labels": ["pull-request-available"], "created": "2025-10-30T08:50:01.000+0000", "updated": "2025-10-30T09:02:56.000+0000", "description": "When not enable speculation ,executor launch task failed caused by OOM or other issue, DAGScheduler won;t know task failed and won't scheduler again, causing application stuck\r\n\r\n!image-2025-10-30-16-52-22-524.png|width=589,height=233!", "comments": [], "derived_tasks": {"summarization": "Spark Executor launch task failed should return task killed message", "classification": "feature", "qna": {"question": "What is the issue 'Spark Executor launch task failed should return task killed message' about?", "answer": "When not enable speculation ,executor launch task failed caused by OOM or other issue, DAGScheduler won;t know task failed and won't scheduler again, causing application stuck\r\n\r\n!image-2025-10-30-16-52-22-524.png|width=589,height=233!"}}}
{"project": "SPARK", "issue_key": "SPARK-54086", "title": "Support IO_URING Netty IO Mode", "status": "Open", "reporter": "Kent Yao", "assignee": "Kent Yao", "priority": "Major", "labels": ["pull-request-available"], "created": "2025-10-30T08:30:49.000+0000", "updated": "2025-10-31T22:47:01.000+0000", "description": null, "comments": [], "derived_tasks": {"summarization": "Support IO_URING Netty IO Mode", "classification": "feature", "qna": {"question": "What is the issue 'Support IO_URING Netty IO Mode' about?", "answer": null}}}
{"project": "SPARK", "issue_key": "SPARK-54085", "title": "Fix `initialize` to add `CREATE` option additionally in `DriverRunner`", "status": "Resolved", "reporter": "Chufeng Gao", "assignee": "Chufeng Gao", "priority": "Major", "labels": ["pull-request-available"], "created": "2025-10-30T08:12:22.000+0000", "updated": "2025-10-30T15:52:51.000+0000", "description": "When submitting jobs to standalone cluster using restful api, we get errors like:\r\n\r\n```\r\n25/10/30 16:02:59 INFO DriverRunner: Killing driver process!\r\n25/10/30 16:02:59 INFO CommandUtils: Redirection to /Users/ericgao/workspace/open_source/spark/work/driver-20251030160259-0020/stdout closed: Stream closed\r\n25/10/30 16:02:59 WARN Worker: Driver driver-20251030160259-0020 failed with unrecoverable exception: java.nio.file.NoSuchFileException: /Users/ericgao/workspace/open_source/spark/work/driver-20251030160259-0020/stderr\r\n\r\n```\r\n\r\nWe need to fix the usage of `Files.writeString` in DriverRunner", "comments": [], "derived_tasks": {"summarization": "Fix `initialize` to add `CREATE` option additionally in `DriverRunner`", "classification": "feature", "qna": {"question": "What is the issue 'Fix `initialize` to add `CREATE` option additionally in `DriverRunner`' about?", "answer": "When submitting jobs to standalone cluster using restful api, we get errors like:\r\n\r\n```\r\n25/10/30 16:02:59 INFO DriverRunner: Killing driver process!\r\n25/10/30 16:02:59 INFO CommandUtils: Redirection to /Users/ericgao/workspace/open_source/spark/work/driver-20251030160259-0020/stdout closed: Stream closed\r\n25/10/30 16:02:59 WARN Worker: Driver driver-20251030160259-0020 failed with unrecoverable exception: java.nio.file.NoSuchFileException: /Users/ericgao/workspace/open_source/spark/work/driver-20251030160259-0020/stderr\r\n\r\n```\r\n\r\nWe need to fix the usage of `Files.writeString` in DriverRunner"}}}
{"project": "SPARK", "issue_key": "SPARK-54084", "title": "Publish Apache Spark 4.1.0-preview3 to docker registry", "status": "Resolved", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "priority": "Major", "labels": ["pull-request-available"], "created": "2025-10-30T05:22:24.000+0000", "updated": "2025-10-31T03:59:01.000+0000", "description": null, "comments": [], "derived_tasks": {"summarization": "Publish Apache Spark 4.1.0-preview3 to docker registry", "classification": "feature", "qna": {"question": "What is the issue 'Publish Apache Spark 4.1.0-preview3 to docker registry' about?", "answer": null}}}
{"project": "SPARK", "issue_key": "SPARK-54083", "title": "Use `4.1.0-preview3` instead of `RC1`", "status": "Resolved", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "priority": "Major", "labels": ["pull-request-available"], "created": "2025-10-30T05:06:34.000+0000", "updated": "2025-10-30T16:04:02.000+0000", "description": null, "comments": [], "derived_tasks": {"summarization": "Use `4.1.0-preview3` instead of `RC1`", "classification": "feature", "qna": {"question": "What is the issue 'Use `4.1.0-preview3` instead of `RC1`' about?", "answer": null}}}
{"project": "SPARK", "issue_key": "SPARK-54082", "title": "Upgrade Spark to `4.1.0-preview3`", "status": "Resolved", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "priority": "Major", "labels": ["pull-request-available"], "created": "2025-10-30T04:59:24.000+0000", "updated": "2025-10-30T14:28:45.000+0000", "description": null, "comments": [], "derived_tasks": {"summarization": "Upgrade Spark to `4.1.0-preview3`", "classification": "feature", "qna": {"question": "What is the issue 'Upgrade Spark to `4.1.0-preview3`' about?", "answer": null}}}
{"project": "SPARK", "issue_key": "SPARK-54081", "title": "Add `word-count-preview.yaml` Example", "status": "Resolved", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "priority": "Major", "labels": ["pull-request-available"], "created": "2025-10-29T23:45:58.000+0000", "updated": "2025-10-30T04:15:51.000+0000", "description": null, "comments": [], "derived_tasks": {"summarization": "Add `word-count-preview.yaml` Example", "classification": "feature", "qna": {"question": "What is the issue 'Add `word-count-preview.yaml` Example' about?", "answer": null}}}
{"project": "SPARK", "issue_key": "SPARK-54080", "title": "Use Swift 6.2 as the minimum supported version", "status": "Resolved", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "priority": "Critical", "labels": ["pull-request-available"], "created": "2025-10-29T18:04:09.000+0000", "updated": "2025-10-29T18:32:26.000+0000", "description": null, "comments": [], "derived_tasks": {"summarization": "Use Swift 6.2 as the minimum supported version", "classification": "feature", "qna": {"question": "What is the issue 'Use Swift 6.2 as the minimum supported version' about?", "answer": null}}}
{"project": "SPARK", "issue_key": "SPARK-54079", "title": "Introduce the framework for adding ST expressions in Catalyst", "status": "Resolved", "reporter": "Uro\u0161 Bojani\u0107", "assignee": "Uro\u0161 Bojani\u0107", "priority": "Major", "labels": ["pull-request-available"], "created": "2025-10-29T17:47:59.000+0000", "updated": "2025-10-31T02:35:09.000+0000", "description": null, "comments": [], "derived_tasks": {"summarization": "Introduce the framework for adding ST expressions in Catalyst", "classification": "feature", "qna": {"question": "What is the issue 'Introduce the framework for adding ST expressions in Catalyst' about?", "answer": null}}}
{"project": "SPARK", "issue_key": "SPARK-54078", "title": "Deflake StateStoreSuite `SPARK-40492: maintenance before unload`", "status": "Resolved", "reporter": "Livia Zhu", "assignee": "Livia Zhu", "priority": "Major", "labels": ["pull-request-available"], "created": "2025-10-29T16:40:18.000+0000", "updated": "2025-10-30T17:13:36.000+0000", "description": "`SPARK-40492: maintenance before unload` in StateStoreSuite is flaky due to timing issues. Fix this by adding hooks to pause maintenance.", "comments": [], "derived_tasks": {"summarization": "Deflake StateStoreSuite `SPARK-40492: maintenance before unload`", "classification": "feature", "qna": {"question": "What is the issue 'Deflake StateStoreSuite `SPARK-40492: maintenance before unload`' about?", "answer": "`SPARK-40492: maintenance before unload` in StateStoreSuite is flaky due to timing issues. Fix this by adding hooks to pause maintenance."}}}
{"project": "SPARK", "issue_key": "SPARK-54077", "title": "Enable all disabled tests on Linux", "status": "Resolved", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "priority": "Major", "labels": ["pull-request-available"], "created": "2025-10-29T16:37:05.000+0000", "updated": "2025-10-29T17:24:39.000+0000", "description": null, "comments": [], "derived_tasks": {"summarization": "Enable all disabled tests on Linux", "classification": "feature", "qna": {"question": "What is the issue 'Enable all disabled tests on Linux' about?", "answer": null}}}
{"project": "SPARK", "issue_key": "SPARK-54076", "title": "Fix Flaky Test:`OracleJoinPushdownIntegrationSuite`", "status": "Resolved", "reporter": "Kousuke Saruta", "assignee": "Kousuke Saruta", "priority": "Major", "labels": ["pull-request-available"], "created": "2025-10-29T14:56:32.000+0000", "updated": "2025-10-30T04:48:07.000+0000", "description": "Recently, `OracleJoinPushdownIntegrationSuite` frequently fails.\r\n\r\nhttps://github.com/beliefer/spark/actions/runs/18904457292/job/53959322233#logs\r\n{code:java}\r\n[info] org.apache.spark.sql.jdbc.v2.join.OracleJoinPushdownIntegrationSuite *** ABORTED *** (10 minutes, 15 seconds)\r\n[info]   The code passed to eventually never returned normally. Attempted 599 times over 10.012648813883333 minutes. Last failure message: ORA-12541: Cannot connect. No listener at host 10.1.0.201 port 44551. (CONNECTION_ID=5rB4vZNTQR2sFHpk9xQC1A==)\r\n[info]   https://docs.oracle.com/error-help/db/ora-12541/. (DockerJDBCIntegrationSuite.scala:214)\r\n[info]   org.scalatest.exceptions.TestFailedDueToTimeoutException:\r\n[info]   at org.scalatest.enablers.Retrying$$anon$4.tryTryAgain$2(Retrying.scala:219)\r\n[info]   at org.scalatest.enablers.Retrying$$anon$4.retry(Retrying.scala:226)\r\n[info]   at org.scalatest.concurrent.Eventually.eventually(Eventually.scala:313)\r\n[info]   at org.scalatest.concurrent.Eventually.eventually$(Eventually.scala:312)\r\n[info]   at org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite.eventually(DockerJDBCIntegrationSuite.scala:103)\r\n[info]   at org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite.$anonfun$beforeAll$1(DockerJDBCIntegrationSuite.scala:214)\r\n[info]   at org.apache.spark.sql.jdbc.DockerIntegrationFunSuite.runIfTestsEnabled(DockerIntegrationFunSuite.scala:49)\r\n[info]   at org.apache.spark.sql.jdbc.DockerIntegrationFunSuite.runIfTestsEnabled$(DockerIntegrationFunSuite.scala:47)\r\n[info]   at org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite.runIfTestsEnabled(DockerJDBCIntegrationSuite.scala:103)\r\n[info]   at org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite.beforeAll(DockerJDBCIntegrationSuite.scala:133)\r\n[info]   at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:212)\r\n[info]   at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)\r\n[info]   at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208)\r\n[info]   at org.apache.spark.SparkFunSuite.run(SparkFunSuite.scala:68)\r\n[info]   at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:321)\r\n[info]   at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:517)\r\n[info]   at sbt.ForkMain$Run.lambda$runTest$1(ForkMain.java:414)\r\n[info]   at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\r\n[info]   at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n[info]   at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n[info]   at java.base/java.lang.Thread.run(Thread.java:840)\r\n[info]   Cause: java.sql.SQLException: ORA-12541: Cannot connect. No listener at host 10.1.0.201 port 44551. (CONNECTION_ID=5rB4vZNTQR2sFHpk9xQC1A==)\r\n[info] https://docs.oracle.com/error-help/db/ora-12541/\r\n[info]   at oracle.jdbc.driver.T4CConnection.handleLogonNetException(T4CConnection.java:1631)\r\n[info]   at oracle.jdbc.driver.T4CConnection.logon(T4CConnection.java:1151)\r\n[info]   at oracle.jdbc.driver.PhysicalConnection.connect(PhysicalConnection.java:1189)\r\n[info]   at oracle.jdbc.driver.T4CDriverExtension.getConnection(T4CDriverExtension.java:106)\r\n[info]   at oracle.jdbc.driver.OracleDriver.connect(OracleDriver.java:895)\r\n[info]   at oracle.jdbc.driver.OracleDriver.connect(OracleDriver.java:702)\r\n[info]   at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:681)\r\n[info]   at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:190)\r\n[info]   at org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite.getConnection(DockerJDBCIntegrationSuite.scala:250)\r\n[info]   at org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite.$anonfun$beforeAll$8(DockerJDBCIntegrationSuite.scala:215)\r\n[info]   at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\r\n[info]   at org.scalatest.enablers.Retrying$$anon$4.makeAValiantAttempt$1(Retrying.scala:184)\r\n[info]   at org.scalatest.enablers.Retrying$$anon$4.tryTryAgain$2(Retrying.scala:196)\r\n[info]   at org.scalatest.enablers.Retrying$$anon$4.retry(Retrying.scala:226)\r\n[info]   at org.scalatest.concurrent.Eventually.eventually(Eventually.scala:313)\r\n[info]   at org.scalatest.concurrent.Eventually.eventually$(Eventually.scala:312)\r\n[info]   at org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite.eventually(DockerJDBCIntegrationSuite.scala:103)\r\n[info]   at org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite.$anonfun$beforeAll$1(DockerJDBCIntegrationSuite.scala:214)\r\n[info]   at org.apache.spark.sql.jdbc.DockerIntegrationFunSuite.runIfTestsEnabled(DockerIntegrationFunSuite.scala:49)\r\n[info]   at org.apache.spark.sql.jdbc.DockerIntegrationFunSuite.runIfTestsEnabled$(DockerIntegrationFunSuite.scala:47)\r\n[info]   at org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite.runIfTestsEnabled(DockerJDBCIntegrationSuite.scala:103)\r\n[info]   at org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite.beforeAll(DockerJDBCIntegrationSuite.scala:133)\r\n[info]   at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:212)\r\n[info]   at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)\r\n[info]   at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208)\r\n[info]   at org.apache.spark.SparkFunSuite.run(SparkFunSuite.scala:68)\r\n[info]   at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:321)\r\n[info]   at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:517)\r\n[info]   at sbt.ForkMain$Run.lambda$runTest$1(ForkMain.java:414)\r\n[info]   at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\r\n[info]   at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n[info]   at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n[info]   at java.base/java.lang.Thread.run(Thread.java:840)\r\n[info]   Cause: oracle.net.ns.NetException: ORA-12541: Cannot connect. No listener at host 10.1.0.201 port 44551. (CONNECTION_ID=5rB4vZNTQR2sFHpk9xQC1A==)\r\n[info] https://docs.oracle.com/error-help/db/ora-12541/\r\n[info]   at oracle.net.nt.TcpNTAdapter.handleEstablishSocketException(TcpNTAdapter.java:418)\r\n[info]   at oracle.net.nt.TcpNTAdapter.establishSocket(TcpNTAdapter.java:350)\r\n[info]   at oracle.net.nt.TcpNTAdapter.connect(TcpNTAdapter.java:228)\r\n[info]   at oracle.net.nt.ConnOption.connect(ConnOption.java:333)\r\n[info]   at oracle.net.nt.ConnStrategy.executeConnOption(ConnStrategy.java:1223)\r\n[info]   at oracle.net.nt.ConnStrategy.execute(ConnStrategy.java:762)\r\n[info]   at oracle.net.resolver.AddrResolution.resolveAndExecute(AddrResolution.java:712)\r\n[info]   at oracle.net.ns.NSProtocol.establishConnection(NSProtocol.java:960)\r\n[info]   at oracle.net.ns.NSProtocol.connect(NSProtocol.java:329)\r\n[info]   at oracle.jdbc.driver.T4CConnection.connectNetworkSessionProtocol(T4CConnection.java:3462)\r\n[info]   at oracle.jdbc.driver.T4CConnection.logon(T4CConnection.java:1030)\r\n[info]   at oracle.jdbc.driver.PhysicalConnection.connect(PhysicalConnection.java:1189)\r\n[info]   at oracle.jdbc.driver.T4CDriverExtension.getConnection(T4CDriverExtension.java:106)\r\n[info]   at oracle.jdbc.driver.OracleDriver.connect(OracleDriver.java:895)\r\n[info]   at oracle.jdbc.driver.OracleDriver.connect(OracleDriver.java:702)\r\n[info]   at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:681)\r\n[info]   at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:190)\r\n[info]   at org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite.getConnection(DockerJDBCIntegrationSuite.scala:250)\r\n[info]   at org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite.$anonfun$beforeAll$8(DockerJDBCIntegrationSuite.scala:215)\r\n[info]   at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\r\n[info]   at org.scalatest.enablers.Retrying$$anon$4.makeAValiantAttempt$1(Retrying.scala:184)\r\n[info]   at org.scalatest.enablers.Retrying$$anon$4.tryTryAgain$2(Retrying.scala:196)\r\n[info]   at org.scalatest.enablers.Retrying$$anon$4.retry(Retrying.scala:226)\r\n[info]   at org.scalatest.concurrent.Eventually.eventually(Eventually.scala:313)\r\n[info]   at org.scalatest.concurrent.Eventually.eventually$(Eventually.scala:312)\r\n[info]   at org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite.eventually(DockerJDBCIntegrationSuite.scala:103)\r\n[info]   at org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite.$anonfun$beforeAll$1(DockerJDBCIntegrationSuite.scala:214)\r\n[info]   at org.apache.spark.sql.jdbc.DockerIntegrationFunSuite.runIfTestsEnabled(DockerIntegrationFunSuite.scala:49)\r\n[info]   at org.apache.spark.sql.jdbc.DockerIntegrationFunSuite.runIfTestsEnabled$(DockerIntegrationFunSuite.scala:47)\r\n[info]   at org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite.runIfTestsEnabled(DockerJDBCIntegrationSuite.scala:103)\r\n[info]   at org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite.beforeAll(DockerJDBCIntegrationSuite.scala:133)\r\n[info]   at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:212)\r\n[info]   at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)\r\n[info]   at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208)\r\n[info]   at org.apache.spark.SparkFunSuite.run(SparkFunSuite.scala:68)\r\n[info]   at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:321)\r\n[info]   at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:517)\r\n[info]   at sbt.ForkMain$Run.lambda$runTest$1(ForkMain.java:414)\r\n[info]   at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\r\n[info]   at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n[info]   at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n[info]   at java.base/java.lang.Thread.run(Thread.java:840)\r\n[info]   Cause: java.net.ConnectException: Connection refused\r\n[info]   at java.base/sun.nio.ch.Net.pollConnect(Native Method)\r\n[info]   at java.base/sun.nio.ch.Net.pollConnectNow(Net.java:672)\r\n[info]   at java.base/sun.nio.ch.SocketChannelImpl.finishTimedConnect(SocketChannelImpl.java:1141)\r\n[info]   at java.base/sun.nio.ch.SocketChannelImpl.blockingConnect(SocketChannelImpl.java:1183)\r\n[info]   at java.base/sun.nio.ch.SocketAdaptor.connect(SocketAdaptor.java:98)\r\n[info]   at oracle.net.nt.TimeoutSocketChannel.doConnect(TimeoutSocketChannel.java:289)\r\n[info]   at oracle.net.nt.TimeoutSocketChannel.initializeSocketChannel(TimeoutSocketChannel.java:269)\r\n[info]   at oracle.net.nt.TimeoutSocketChannel.connect(TimeoutSocketChannel.java:236)\r\n[info]   at oracle.net.nt.TimeoutSocketChannel.<init>(TimeoutSocketChannel.java:203)\r\n[info]   at oracle.net.nt.TcpNTAdapter.establishSocket(TcpNTAdapter.java:339)\r\n[info]   at oracle.net.nt.TcpNTAdapter.connect(TcpNTAdapter.java:228)\r\n[info]   at oracle.net.nt.ConnOption.connect(ConnOption.java:333)\r\n[info]   at oracle.net.nt.ConnStrategy.executeConnOption(ConnStrategy.java:1223)\r\n[info]   at oracle.net.nt.ConnStrategy.execute(ConnStrategy.java:762)\r\n[info]   at oracle.net.resolver.AddrResolution.resolveAndExecute(AddrResolution.java:712)\r\n[info]   at oracle.net.ns.NSProtocol.establishConnection(NSProtocol.java:960)\r\n[info]   at oracle.net.ns.NSProtocol.connect(NSProtocol.java:329)\r\n[info]   at oracle.jdbc.driver.T4CConnection.connectNetworkSessionProtocol(T4CConnection.java:3462)\r\n[info]   at oracle.jdbc.driver.T4CConnection.logon(T4CConnection.java:1030)\r\n[info]   at oracle.jdbc.driver.PhysicalConnection.connect(PhysicalConnection.java:1189)\r\n[info]   at oracle.jdbc.driver.T4CDriverExtension.getConnection(T4CDriverExtension.java:106)\r\n[info]   at oracle.jdbc.driver.OracleDriver.connect(OracleDriver.java:895)\r\n[info]   at oracle.jdbc.driver.OracleDriver.connect(OracleDriver.java:702)\r\n[info]   at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:681)\r\n[info]   at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:190)\r\n[info]   at org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite.getConnection(DockerJDBCIntegrationSuite.scala:250)\r\n[info]   at org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite.$anonfun$beforeAll$8(DockerJDBCIntegrationSuite.scala:215)\r\n[info]   at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\r\n[info]   at org.scalatest.enablers.Retrying$$anon$4.makeAValiantAttempt$1(Retrying.scala:184)\r\n[info]   at org.scalatest.enablers.Retrying$$anon$4.tryTryAgain$2(Retrying.scala:196)\r\n[info]   at org.scalatest.enablers.Retrying$$anon$4.retry(Retrying.scala:226)\r\n[info]   at org.scalatest.concurrent.Eventually.eventually(Eventually.scala:313)\r\n[info]   at org.scalatest.concurrent.Eventually.eventually$(Eventually.scala:312)\r\n[info]   at org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite.eventually(DockerJDBCIntegrationSuite.scala:103)\r\n[info]   at org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite.$anonfun$beforeAll$1(DockerJDBCIntegrationSuite.scala:214)\r\n[info]   at org.apache.spark.sql.jdbc.DockerIntegrationFunSuite.runIfTestsEnabled(DockerIntegrationFunSuite.scala:49)\r\n[info]   at org.apache.spark.sql.jdbc.DockerIntegrationFunSuite.runIfTestsEnabled$(DockerIntegrationFunSuite.scala:47)\r\n[info]   at org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite.runIfTestsEnabled(DockerJDBCIntegrationSuite.scala:103)\r\n[info]   at org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite.beforeAll(DockerJDBCIntegrationSuite.scala:133)\r\n[info]   at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:212)\r\n[info]   at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)\r\n[info]   at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208)\r\n[info]   at org.apache.spark.SparkFunSuite.run(SparkFunSuite.scala:68)\r\n[info]   at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:321)\r\n[info]   at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:517)\r\n[info]   at sbt.ForkMain$Run.lambda$runTest$1(ForkMain.java:414)\r\n[info]   at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\r\n[info]   at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n[info]   at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n[info]   at java.base/java.lang.Thread.run(Thread.java:840) {code}", "comments": [], "derived_tasks": {"summarization": "Fix Flaky Test:`OracleJoinPushdownIntegrationSuite`", "classification": "feature", "qna": {"question": "What is the issue 'Fix Flaky Test:`OracleJoinPushdownIntegrationSuite`' about?", "answer": "Recently, `OracleJoinPushdownIntegrationSuite` frequently fails.\r\n\r\nhttps://github.com/beliefer/spark/actions/runs/18904457292/job/53959322233#logs\r\n{code:java}\r\n[info] org.apache.spark.sql.jdbc.v2.join.OracleJoinPushdownIntegrationSuite *** ABORTED *** (10 minutes, 15 seconds)\r\n[info]   The code passed to eventually never returned normally. Attempted 599 times over 10.012648813883333 minutes. Last failure message: ORA-12541: Cannot connect. No listener at host 10.1.0.201 port 44551. (CONNECTION_ID=5rB4vZNTQR2sFHpk9xQC1A==)\r\n[info]   https://docs.oracle.com/error-help/db/ora-12541/. (DockerJDBCIntegrationSuite.scala:214)\r\n[info]   org.scalatest.exceptions.TestFailedDueToTimeoutException:\r\n[info]   at org.scalatest.enablers.Retrying$$anon$4.tryTryAgain$2(Retrying.scala:219)\r\n[info]   at org.scalatest.enablers.Retrying$$anon$4.retry(Retrying.scala:226)\r\n[info]   at org.scalatest.concurrent.Eventually.eventually(Eventually.scala:313)\r\n[info]   at org.scalatest.concurrent.Eventually.eventually$(Eventually.scala:312)\r\n[info]   at org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite.eventually(DockerJDBCIntegrationSuite.scala:103)\r\n[info]   at org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite.$anonfun$beforeAll$1(DockerJDBCIntegrationSuite.scala:214)\r\n[info]   at org.apache.spark.sql.jdbc.DockerIntegrationFunSuite.runIfTestsEnabled(DockerIntegrationFunSuite.scala:49)\r\n[info]   at org.apache.spark.sql.jdbc.DockerIntegrationFunSuite.runIfTestsEnabled$(DockerIntegrationFunSuite.scala:47)\r\n[info]   at org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite.runIfTestsEnabled(DockerJDBCIntegrationSuite.scala:103)\r\n[info]   at org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite.beforeAll(DockerJDBCIntegrationSuite.scala:133)\r\n[info]   at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:212)\r\n[info]   at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)\r\n[info]   at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208)\r\n[info]   at org.apache.spark.SparkFunSuite.run(SparkFunSuite.scala:68)\r\n[info]   at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:321)\r\n[info]   at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:517)\r\n[info]   at sbt.ForkMain$Run.lambda$runTest$1(ForkMain.java:414)\r\n[info]   at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\r\n[info]   at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n[info]   at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n[info]   at java.base/java.lang.Thread.run(Thread.java:840)\r\n[info]   Cause: java.sql.SQLException: ORA-12541: Cannot connect. No listener at host 10.1.0.201 port 44551. (CONNECTION_ID=5rB4vZNTQR2sFHpk9xQC1A==)\r\n[info] https://docs.oracle.com/error-help/db/ora-12541/\r\n[info]   at oracle.jdbc.driver.T4CConnection.handleLogonNetException(T4CConnection.java:1631)\r\n[info]   at oracle.jdbc.driver.T4CConnection.logon(T4CConnection.java:1151)\r\n[info]   at oracle.jdbc.driver.PhysicalConnection.connect(PhysicalConnection.java:1189)\r\n[info]   at oracle.jdbc.driver.T4CDriverExtension.getConnection(T4CDriverExtension.java:106)\r\n[info]   at oracle.jdbc.driver.OracleDriver.connect(OracleDriver.java:895)\r\n[info]   at oracle.jdbc.driver.OracleDriver.connect(OracleDriver.java:702)\r\n[info]   at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:681)\r\n[info]   at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:190)\r\n[info]   at org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite.getConnection(DockerJDBCIntegrationSuite.scala:250)\r\n[info]   at org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite.$anonfun$beforeAll$8(DockerJDBCIntegrationSuite.scala:215)\r\n[info]   at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\r\n[info]   at org.scalatest.enablers.Retrying$$anon$4.makeAValiantAttempt$1(Retrying.scala:184)\r\n[info]   at org.scalatest.enablers.Retrying$$anon$4.tryTryAgain$2(Retrying.scala:196)\r\n[info]   at org.scalatest.enablers.Retrying$$anon$4.retry(Retrying.scala:226)\r\n[info]   at org.scalatest.concurrent.Eventually.eventually(Eventually.scala:313)\r\n[info]   at org.scalatest.concurrent.Eventually.eventually$(Eventually.scala:312)\r\n[info]   at org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite.eventually(DockerJDBCIntegrationSuite.scala:103)\r\n[info]   at org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite.$anonfun$beforeAll$1(DockerJDBCIntegrationSuite.scala:214)\r\n[info]   at org.apache.spark.sql.jdbc.DockerIntegrationFunSuite.runIfTestsEnabled(DockerIntegrationFunSuite.scala:49)\r\n[info]   at org.apache.spark.sql.jdbc.DockerIntegrationFunSuite.runIfTestsEnabled$(DockerIntegrationFunSuite.scala:47)\r\n[info]   at org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite.runIfTestsEnabled(DockerJDBCIntegrationSuite.scala:103)\r\n[info]   at org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite.beforeAll(DockerJDBCIntegrationSuite.scala:133)\r\n[info]   at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:212)\r\n[info]   at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)\r\n[info]   at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208)\r\n[info]   at org.apache.spark.SparkFunSuite.run(SparkFunSuite.scala:68)\r\n[info]   at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:321)\r\n[info]   at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:517)\r\n[info]   at sbt.ForkMain$Run.lambda$runTest$1(ForkMain.java:414)\r\n[info]   at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\r\n[info]   at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n[info]   at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n[info]   at java.base/java.lang.Thread.run(Thread.java:840)\r\n[info]   Cause: oracle.net.ns.NetException: ORA-12541: Cannot connect. No listener at host 10.1.0.201 port 44551. (CONNECTION_ID=5rB4vZNTQR2sFHpk9xQC1A==)\r\n[info] https://docs.oracle.com/error-help/db/ora-12541/\r\n[info]   at oracle.net.nt.TcpNTAdapter.handleEstablishSocketException(TcpNTAdapter.java:418)\r\n[info]   at oracle.net.nt.TcpNTAdapter.establishSocket(TcpNTAdapter.java:350)\r\n[info]   at oracle.net.nt.TcpNTAdapter.connect(TcpNTAdapter.java:228)\r\n[info]   at oracle.net.nt.ConnOption.connect(ConnOption.java:333)\r\n[info]   at oracle.net.nt.ConnStrategy.executeConnOption(ConnStrategy.java:1223)\r\n[info]   at oracle.net.nt.ConnStrategy.execute(ConnStrategy.java:762)\r\n[info]   at oracle.net.resolver.AddrResolution.resolveAndExecute(AddrResolution.java:712)\r\n[info]   at oracle.net.ns.NSProtocol.establishConnection(NSProtocol.java:960)\r\n[info]   at oracle.net.ns.NSProtocol.connect(NSProtocol.java:329)\r\n[info]   at oracle.jdbc.driver.T4CConnection.connectNetworkSessionProtocol(T4CConnection.java:3462)\r\n[info]   at oracle.jdbc.driver.T4CConnection.logon(T4CConnection.java:1030)\r\n[info]   at oracle.jdbc.driver.PhysicalConnection.connect(PhysicalConnection.java:1189)\r\n[info]   at oracle.jdbc.driver.T4CDriverExtension.getConnection(T4CDriverExtension.java:106)\r\n[info]   at oracle.jdbc.driver.OracleDriver.connect(OracleDriver.java:895)\r\n[info]   at oracle.jdbc.driver.OracleDriver.connect(OracleDriver.java:702)\r\n[info]   at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:681)\r\n[info]   at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:190)\r\n[info]   at org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite.getConnection(DockerJDBCIntegrationSuite.scala:250)\r\n[info]   at org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite.$anonfun$beforeAll$8(DockerJDBCIntegrationSuite.scala:215)\r\n[info]   at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\r\n[info]   at org.scalatest.enablers.Retrying$$anon$4.makeAValiantAttempt$1(Retrying.scala:184)\r\n[info]   at org.scalatest.enablers.Retrying$$anon$4.tryTryAgain$2(Retrying.scala:196)\r\n[info]   at org.scalatest.enablers.Retrying$$anon$4.retry(Retrying.scala:226)\r\n[info]   at org.scalatest.concurrent.Eventually.eventually(Eventually.scala:313)\r\n[info]   at org.scalatest.concurrent.Eventually.eventually$(Eventually.scala:312)\r\n[info]   at org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite.eventually(DockerJDBCIntegrationSuite.scala:103)\r\n[info]   at org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite.$anonfun$beforeAll$1(DockerJDBCIntegrationSuite.scala:214)\r\n[info]   at org.apache.spark.sql.jdbc.DockerIntegrationFunSuite.runIfTestsEnabled(DockerIntegrationFunSuite.scala:49)\r\n[info]   at org.apache.spark.sql.jdbc.DockerIntegrationFunSuite.runIfTestsEnabled$(DockerIntegrationFunSuite.scala:47)\r\n[info]   at org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite.runIfTestsEnabled(DockerJDBCIntegrationSuite.scala:103)\r\n[info]   at org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite.beforeAll(DockerJDBCIntegrationSuite.scala:133)\r\n[info]   at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:212)\r\n[info]   at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)\r\n[info]   at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208)\r\n[info]   at org.apache.spark.SparkFunSuite.run(SparkFunSuite.scala:68)\r\n[info]   at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:321)\r\n[info]   at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:517)\r\n[info]   at sbt.ForkMain$Run.lambda$runTest$1(ForkMain.java:414)\r\n[info]   at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\r\n[info]   at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n[info]   at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n[info]   at java.base/java.lang.Thread.run(Thread.java:840)\r\n[info]   Cause: java.net.ConnectException: Connection refused\r\n[info]   at java.base/sun.nio.ch.Net.pollConnect(Native Method)\r\n[info]   at java.base/sun.nio.ch.Net.pollConnectNow(Net.java:672)\r\n[info]   at java.base/sun.nio.ch.SocketChannelImpl.finishTimedConnect(SocketChannelImpl.java:1141)\r\n[info]   at java.base/sun.nio.ch.SocketChannelImpl.blockingConnect(SocketChannelImpl.java:1183)\r\n[info]   at java.base/sun.nio.ch.SocketAdaptor.connect(SocketAdaptor.java:98)\r\n[info]   at oracle.net.nt.TimeoutSocketChannel.doConnect(TimeoutSocketChannel.java:289)\r\n[info]   at oracle.net.nt.TimeoutSocketChannel.initializeSocketChannel(TimeoutSocketChannel.java:269)\r\n[info]   at oracle.net.nt.TimeoutSocketChannel.connect(TimeoutSocketChannel.java:236)\r\n[info]   at oracle.net.nt.TimeoutSocketChannel.<init>(TimeoutSocketChannel.java:203)\r\n[info]   at oracle.net.nt.TcpNTAdapter.establishSocket(TcpNTAdapter.java:339)\r\n[info]   at oracle.net.nt.TcpNTAdapter.connect(TcpNTAdapter.java:228)\r\n[info]   at oracle.net.nt.ConnOption.connect(ConnOption.java:333)\r\n[info]   at oracle.net.nt.ConnStrategy.executeConnOption(ConnStrategy.java:1223)\r\n[info]   at oracle.net.nt.ConnStrategy.execute(ConnStrategy.java:762)\r\n[info]   at oracle.net.resolver.AddrResolution.resolveAndExecute(AddrResolution.java:712)\r\n[info]   at oracle.net.ns.NSProtocol.establishConnection(NSProtocol.java:960)\r\n[info]   at oracle.net.ns.NSProtocol.connect(NSProtocol.java:329)\r\n[info]   at oracle.jdbc.driver.T4CConnection.connectNetworkSessionProtocol(T4CConnection.java:3462)\r\n[info]   at oracle.jdbc.driver.T4CConnection.logon(T4CConnection.java:1030)\r\n[info]   at oracle.jdbc.driver.PhysicalConnection.connect(PhysicalConnection.java:1189)\r\n[info]   at oracle.jdbc.driver.T4CDriverExtension.getConnection(T4CDriverExtension.java:106)\r\n[info]   at oracle.jdbc.driver.OracleDriver.connect(OracleDriver.java:895)\r\n[info]   at oracle.jdbc.driver.OracleDriver.connect(OracleDriver.java:702)\r\n[info]   at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:681)\r\n[info]   at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:190)\r\n[info]   at org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite.getConnection(DockerJDBCIntegrationSuite.scala:250)\r\n[info]   at org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite.$anonfun$beforeAll$8(DockerJDBCIntegrationSuite.scala:215)\r\n[info]   at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\r\n[info]   at org.scalatest.enablers.Retrying$$anon$4.makeAValiantAttempt$1(Retrying.scala:184)\r\n[info]   at org.scalatest.enablers.Retrying$$anon$4.tryTryAgain$2(Retrying.scala:196)\r\n[info]   at org.scalatest.enablers.Retrying$$anon$4.retry(Retrying.scala:226)\r\n[info]   at org.scalatest.concurrent.Eventually.eventually(Eventually.scala:313)\r\n[info]   at org.scalatest.concurrent.Eventually.eventually$(Eventually.scala:312)\r\n[info]   at org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite.eventually(DockerJDBCIntegrationSuite.scala:103)\r\n[info]   at org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite.$anonfun$beforeAll$1(DockerJDBCIntegrationSuite.scala:214)\r\n[info]   at org.apache.spark.sql.jdbc.DockerIntegrationFunSuite.runIfTestsEnabled(DockerIntegrationFunSuite.scala:49)\r\n[info]   at org.apache.spark.sql.jdbc.DockerIntegrationFunSuite.runIfTestsEnabled$(DockerIntegrationFunSuite.scala:47)\r\n[info]   at org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite.runIfTestsEnabled(DockerJDBCIntegrationSuite.scala:103)\r\n[info]   at org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite.beforeAll(DockerJDBCIntegrationSuite.scala:133)\r\n[info]   at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:212)\r\n[info]   at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)\r\n[info]   at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208)\r\n[info]   at org.apache.spark.SparkFunSuite.run(SparkFunSuite.scala:68)\r\n[info]   at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:321)\r\n[info]   at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:517)\r\n[info]   at sbt.ForkMain$Run.lambda$runTest$1(ForkMain.java:414)\r\n[info]   at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\r\n[info]   at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n[info]   at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n[info]   at java.base/java.lang.Thread.run(Thread.java:840) {code}"}}}
{"project": "SPARK", "issue_key": "SPARK-54075", "title": "Make ResolvedCollation evaluable", "status": "Resolved", "reporter": "Mihailo Aleksic", "assignee": "Mihailo Aleksic", "priority": "Major", "labels": ["pull-request-available"], "created": "2025-10-29T14:48:46.000+0000", "updated": "2025-10-29T17:35:50.000+0000", "description": "In this PR I propose to make ResolvedCollation evaluable. By making ResolvedCollation evaluable, it can now pass the canEvaluateWithinJoin check, allowing the optimizer to use efficient hash joins for queries with inline COLLATE in join conditions (before this change we would fallback to BroadcastNestedLoopJoin).", "comments": [], "derived_tasks": {"summarization": "Make ResolvedCollation evaluable", "classification": "feature", "qna": {"question": "What is the issue 'Make ResolvedCollation evaluable' about?", "answer": "In this PR I propose to make ResolvedCollation evaluable. By making ResolvedCollation evaluable, it can now pass the canEvaluateWithinJoin check, allowing the optimizer to use efficient hash joins for queries with inline COLLATE in join conditions (before this change we would fallback to BroadcastNestedLoopJoin)."}}}
{"project": "SPARK", "issue_key": "SPARK-54074", "title": "Fix Flaky Test: `KafkaMicroBatchV1SourceWithConsumerSuite.Query with Trigger.AvailableNow should throw error when offset(s) in planned topic partitions got unavailable during subsequent batches`", "status": "Resolved", "reporter": "Kousuke Saruta", "assignee": "Kousuke Saruta", "priority": "Major", "labels": ["pull-request-available"], "created": "2025-10-29T10:36:24.000+0000", "updated": "2025-10-29T16:27:25.000+0000", "description": "Similar to `Query with Trigger.AvailableNow should throw error when topic partitions got unavailable during subsequent batches` resolved in SPARK-54058, `Fix Flaky Test: `KafkaMicroBatchV1SourceWithConsumerSuite` also seems flaky.\r\n\r\nhttps://github.com/micheal-o/spark/actions/runs/18895266750/job/53931238163\r\n\r\n{code}\r\n[info] - Query with Trigger.AvailableNow should throw error when offset(s) in planned topic partitions got unavailable during subsequent batches *** FAILED *** (1 minute)\r\n[info]   java.lang.AssertionError: assertion failed: Exception tree doesn't contain the expected exception with message: Some of partitions in Kafka topic(s) report available offset which is less than end offset during running query with Trigger.AvailableNow.\r\n[info] org.scalatest.exceptions.TestFailedException: isPropagated was false Partition [topic-42, 1] metadata not propagated after timeout\r\n[info] \tat org.scalatest.Assertions.newAssertionFailedException(Assertions.scala:472)\r\n[info] \tat org.scalatest.Assertions.newAssertionFailedException$(Assertions.scala:471)\r\n[info] \tat org.scalatest.Assertions$.newAssertionFailedException(Assertions.scala:1231)\r\n[info] \tat org.scalatest.Assertions$AssertionsHelper.macroAssert(Assertions.scala:1295)\r\n[info] \tat org.apache.spark.sql.kafka010.KafkaTestUtils.$anonfun$waitUntilMetadataIsPropagated$1(KafkaTestUtils.scala:614)\r\n[info] \tat org.scalatest.enablers.Retrying$$anon$4.makeAValiantAttempt$1(Retrying.scala:184)\r\n[info] \tat org.scalatest.enablers.Retrying$$anon$4.tryTryAgain$2(Retrying.scala:196)\r\n[info] \tat org.scalatest.enablers.Retrying$$anon$4.retry(Retrying.scala:226)\r\n[info] \tat org.scalatest.concurrent.Eventually.eventually(Eventually.scala:348)\r\n[info] \tat org.scalatest.concurrent.Eventually.eventually$(Eventually.scala:347)\r\n[info] \tat org.scalatest.concurrent.Eventually$.eventually(Eventually.scala:457)\r\n[info] \tat org.apache.spark.sql.kafka010.KafkaTestUtils.waitUntilMetadataIsPropagated(KafkaTestUtils.scala:613)\r\n[info] \tat org.apache.spark.sql.kafka010.KafkaTestUtils.$anonfun$createTopic$1(KafkaTestUtils.scala:378)\r\n[info] \tat scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:256)\r\n[info] \tat org.apache.spark.sql.kafka010.KafkaTestUtils.createTopic(KafkaTestUtils.scala:377)\r\n[info] \tat org.apache.spark.sql.kafka010.KafkaMicroBatchSourceSuiteBase.$anonfun$new$16(KafkaMicroBatchSourceSuite.scala:406)\r\n[info] \tat org.apache.spark.sql.kafka010.KafkaMicroBatchSourceSuiteBase.$anonfun$new$16$adapted(KafkaMicroBatchSourceSuite.scala:403)\r\n[info] \tat org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.callBatchWriter(ForeachBatchSink.scala:56)\r\n[info] \tat org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:49)\r\n[info] \tat org.apache.spark.sql.execution.streaming.runtime.MicroBatchExecution.$anonfun$runBatch$19(MicroBatchExecution.scala:1063)\r\n[info] \tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:176)\r\n[info] \tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:284)\r\n[info] \tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:138)\r\n[info] \tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\r\n[info] \tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\r\n[info] \tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)\r\n[info] \tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\r\n[info] \tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:138)\r\n[info] \tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:307)\r\n[info] \tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:137)\r\n[info] \tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\r\n[info] \tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:91)\r\n[info] \tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:249)\r\n[info] \tat org.apache.spark.sql.execution.streaming.runtime.MicroBatchExecution.$anonfun$runBatch$18(MicroBatchExecution.scala:1054)\r\n[info] \tat org.apache.spark.sql.execution.streaming.runtime.ProgressContext.reportTimeTaken(ProgressReporter.scala:200)\r\n[info] \tat org.apache.spark.sql.execution.streaming.runtime.MicroBatchExecution.runBatch(MicroBatchExecution.scala:1054)\r\n[info] \tat org.apache.spark.sql.execution.streaming.runtime.MicroBatchExecution.$anonfun$executeOneBatch$2(MicroBatchExecution.scala:513)\r\n[info] \tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\r\n[info] \tat org.apache.spark.sql.execution.streaming.runtime.ProgressContext.reportTimeTaken(ProgressReporter.scala:200)\r\n[info] \tat org.apache.spark.sql.execution.streaming.runtime.MicroBatchExecution.executeOneBatch(MicroBatchExecution.scala:478)\r\n[info] \tat org.apache.spark.sql.execution.streaming.runtime.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:458)\r\n[info] \tat org.apache.spark.sql.execution.streaming.runtime.MicroBatchExecution.$anonfun$runActivatedStream$1$adapted(MicroBatchExecution.scala:458)\r\n[info] \tat org.apache.spark.sql.execution.streaming.runtime.TriggerExecutor.runOneBatch(TriggerExecutor.scala:40)\r\n[info] \tat org.apache.spark.sql.execution.streaming.runtime.TriggerExecutor.runOneBatch$(TriggerExecutor.scala:38)\r\n[info] \tat org.apache.spark.sql.execution.streaming.runtime.MultiBatchExecutor.runOneBatch(TriggerExecutor.scala:60)\r\n[info] \tat org.apache.spark.sql.execution.streaming.runtime.MultiBatchExecutor.execute(TriggerExecutor.scala:65)\r\n[info] \tat org.apache.spark.sql.execution.streaming.runtime.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:458)\r\n[info] \tat org.apache.spark.sql.execution.streaming.runtime.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:347)\r\n[info] \tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\r\n[info] \tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\r\n[info] \tat org.apache.spark.sql.execution.streaming.runtime.StreamExecution.org$apache$spark$sql$execution$streaming$runtime$StreamExecution$$runStream(StreamExecution.scala:307)\r\n[info] \tat org.apache.spark.sql.execution.streaming.runtime.StreamExecution$$anon$1.run(StreamExecution.scala:230)\r\n[info]   at scala.Predef$.assert(Predef.scala:279)\r\n[info]   at org.apache.spark.TestUtils$.assertExceptionMsg(TestUtils.scala:198)\r\n[info]   at org.apache.spark.sql.kafka010.KafkaMicroBatchSourceSuiteBase.$anonfun$new$14(KafkaMicroBatchSourceSuite.scala:428)\r\n[info]   at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\r\n[info]   at org.scalatest.enablers.Timed$$anon$1.timeoutAfter(Timed.scala:127)\r\n[info]   at org.scalatest.concurrent.TimeLimits$.failAfterImpl(TimeLimits.scala:282)\r\n[info]   at org.scalatest.concurrent.TimeLimits.failAfter(TimeLimits.scala:231)\r\n[info]   at org.scalatest.concurrent.TimeLimits.failAfter$(TimeLimits.scala:230)\r\n[info]   at org.apache.spark.SparkFunSuite.failAfter(SparkFunSuite.scala:68)\r\n[info]   at org.apache.spark.SparkFunSuite.$anonfun$test$2(SparkFunSuite.scala:154)\r\n[info]   at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)\r\n[info]   at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)\r\n[info]   at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)\r\n[info]   at org.scalatest.Transformer.apply(Transformer.scala:22)\r\n[info]   at org.scalatest.Transformer.apply(Transformer.scala:20)\r\n[info]   at org.scalatest.funsuite.AnyFunSuiteLike$$anon$1.apply(AnyFunSuiteLike.scala:226)\r\n[info]   at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:226)\r\n[info]   at org.scalatest.funsuite.AnyFunSuiteLike.invokeWithFixture$1(AnyFunSuiteLike.scala:224)\r\n[info]   at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTest$1(AnyFunSuiteLike.scala:236)\r\n[info]   at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)\r\n[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTest(AnyFunSuiteLike.scala:236)\r\n[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTest$(AnyFunSuiteLike.scala:218)\r\n[info]   at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterEach$$super$runTest(SparkFunSuite.scala:68)\r\n[info]   at org.scalatest.BeforeAndAfterEach.runTest(BeforeAndAfterEach.scala:234)\r\n[info]   at org.scalatest.BeforeAndAfterEach.runTest$(BeforeAndAfterEach.scala:227)\r\n[info]   at org.apache.spark.SparkFunSuite.runTest(SparkFunSuite.scala:68)\r\n[info]   at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTests$1(AnyFunSuiteLike.scala:269)\r\n[info]   at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:413)\r\n[info]   at scala.collection.immutable.List.foreach(List.scala:323)\r\n[info]   at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)\r\n[info]   at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:396)\r\n[info]   at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:475)\r\n[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTests(AnyFunSuiteLike.scala:269)\r\n[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTests$(AnyFunSuiteLike.scala:268)\r\n[info]   at org.scalatest.funsuite.AnyFunSuite.runTests(AnyFunSuite.scala:1564)\r\n[info]   at org.scalatest.Suite.run(Suite.scala:1114)\r\n[info]   at org.scalatest.Suite.run$(Suite.scala:1096)\r\n[info]   at org.scalatest.funsuite.AnyFunSuite.org$scalatest$funsuite$AnyFunSuiteLike$$super$run(AnyFunSuite.scala:1564)\r\n[info]   at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$run$1(AnyFunSuiteLike.scala:273)\r\n[info]   at org.scalatest.SuperEngine.runImpl(Engine.scala:535)\r\n[info]   at org.scalatest.funsuite.AnyFunSuiteLike.run(AnyFunSuiteLike.scala:273)\r\n[info]   at org.scalatest.funsuite.AnyFunSuiteLike.run$(AnyFunSuiteLike.scala:272)\r\n[info]   at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterAll$$super$run(SparkFunSuite.scala:68)\r\n[info]   at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:213)\r\n[info]   at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)\r\n[info]   at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208)\r\n[info]   at org.apache.spark.SparkFunSuite.run(SparkFunSuite.scala:68)\r\n[info]   at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:321)\r\n[info]   at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:517)\r\n[info]   at sbt.ForkMain$Run.lambda$runTest$1(ForkMain.java:414)\r\n[info]   at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\r\n[info]   at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n[info]   at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n[info]   at java.base/java.lang.Thread.run(Thread.java:840)\r\n{code}", "comments": [], "derived_tasks": {"summarization": "Fix Flaky Test: `KafkaMicroBatchV1SourceWithConsumerSuite.Query with Trigger.AvailableNow should throw error when offset(s) in planned topic partitions got unavailable during subsequent batches`", "classification": "feature", "qna": {"question": "What is the issue 'Fix Flaky Test: `KafkaMicroBatchV1SourceWithConsumerSuite.Query with Trigger.AvailableNow should throw error when offset(s) in planned topic partitions got unavailable during subsequent batches`' about?", "answer": "Similar to `Query with Trigger.AvailableNow should throw error when topic partitions got unavailable during subsequent batches` resolved in SPARK-54058, `Fix Flaky Test: `KafkaMicroBatchV1SourceWithConsumerSuite` also seems flaky.\r\n\r\nhttps://github.com/micheal-o/spark/actions/runs/18895266750/job/53931238163\r\n\r\n{code}\r\n[info] - Query with Trigger.AvailableNow should throw error when offset(s) in planned topic partitions got unavailable during subsequent batches *** FAILED *** (1 minute)\r\n[info]   java.lang.AssertionError: assertion failed: Exception tree doesn't contain the expected exception with message: Some of partitions in Kafka topic(s) report available offset which is less than end offset during running query with Trigger.AvailableNow.\r\n[info] org.scalatest.exceptions.TestFailedException: isPropagated was false Partition [topic-42, 1] metadata not propagated after timeout\r\n[info] \tat org.scalatest.Assertions.newAssertionFailedException(Assertions.scala:472)\r\n[info] \tat org.scalatest.Assertions.newAssertionFailedException$(Assertions.scala:471)\r\n[info] \tat org.scalatest.Assertions$.newAssertionFailedException(Assertions.scala:1231)\r\n[info] \tat org.scalatest.Assertions$AssertionsHelper.macroAssert(Assertions.scala:1295)\r\n[info] \tat org.apache.spark.sql.kafka010.KafkaTestUtils.$anonfun$waitUntilMetadataIsPropagated$1(KafkaTestUtils.scala:614)\r\n[info] \tat org.scalatest.enablers.Retrying$$anon$4.makeAValiantAttempt$1(Retrying.scala:184)\r\n[info] \tat org.scalatest.enablers.Retrying$$anon$4.tryTryAgain$2(Retrying.scala:196)\r\n[info] \tat org.scalatest.enablers.Retrying$$anon$4.retry(Retrying.scala:226)\r\n[info] \tat org.scalatest.concurrent.Eventually.eventually(Eventually.scala:348)\r\n[info] \tat org.scalatest.concurrent.Eventually.eventually$(Eventually.scala:347)\r\n[info] \tat org.scalatest.concurrent.Eventually$.eventually(Eventually.scala:457)\r\n[info] \tat org.apache.spark.sql.kafka010.KafkaTestUtils.waitUntilMetadataIsPropagated(KafkaTestUtils.scala:613)\r\n[info] \tat org.apache.spark.sql.kafka010.KafkaTestUtils.$anonfun$createTopic$1(KafkaTestUtils.scala:378)\r\n[info] \tat scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:256)\r\n[info] \tat org.apache.spark.sql.kafka010.KafkaTestUtils.createTopic(KafkaTestUtils.scala:377)\r\n[info] \tat org.apache.spark.sql.kafka010.KafkaMicroBatchSourceSuiteBase.$anonfun$new$16(KafkaMicroBatchSourceSuite.scala:406)\r\n[info] \tat org.apache.spark.sql.kafka010.KafkaMicroBatchSourceSuiteBase.$anonfun$new$16$adapted(KafkaMicroBatchSourceSuite.scala:403)\r\n[info] \tat org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.callBatchWriter(ForeachBatchSink.scala:56)\r\n[info] \tat org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:49)\r\n[info] \tat org.apache.spark.sql.execution.streaming.runtime.MicroBatchExecution.$anonfun$runBatch$19(MicroBatchExecution.scala:1063)\r\n[info] \tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:176)\r\n[info] \tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:284)\r\n[info] \tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:138)\r\n[info] \tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\r\n[info] \tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\r\n[info] \tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)\r\n[info] \tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\r\n[info] \tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:138)\r\n[info] \tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:307)\r\n[info] \tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:137)\r\n[info] \tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\r\n[info] \tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:91)\r\n[info] \tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:249)\r\n[info] \tat org.apache.spark.sql.execution.streaming.runtime.MicroBatchExecution.$anonfun$runBatch$18(MicroBatchExecution.scala:1054)\r\n[info] \tat org.apache.spark.sql.execution.streaming.runtime.ProgressContext.reportTimeTaken(ProgressReporter.scala:200)\r\n[info] \tat org.apache.spark.sql.execution.streaming.runtime.MicroBatchExecution.runBatch(MicroBatchExecution.scala:1054)\r\n[info] \tat org.apache.spark.sql.execution.streaming.runtime.MicroBatchExecution.$anonfun$executeOneBatch$2(MicroBatchExecution.scala:513)\r\n[info] \tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\r\n[info] \tat org.apache.spark.sql.execution.streaming.runtime.ProgressContext.reportTimeTaken(ProgressReporter.scala:200)\r\n[info] \tat org.apache.spark.sql.execution.streaming.runtime.MicroBatchExecution.executeOneBatch(MicroBatchExecution.scala:478)\r\n[info] \tat org.apache.spark.sql.execution.streaming.runtime.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:458)\r\n[info] \tat org.apache.spark.sql.execution.streaming.runtime.MicroBatchExecution.$anonfun$runActivatedStream$1$adapted(MicroBatchExecution.scala:458)\r\n[info] \tat org.apache.spark.sql.execution.streaming.runtime.TriggerExecutor.runOneBatch(TriggerExecutor.scala:40)\r\n[info] \tat org.apache.spark.sql.execution.streaming.runtime.TriggerExecutor.runOneBatch$(TriggerExecutor.scala:38)\r\n[info] \tat org.apache.spark.sql.execution.streaming.runtime.MultiBatchExecutor.runOneBatch(TriggerExecutor.scala:60)\r\n[info] \tat org.apache.spark.sql.execution.streaming.runtime.MultiBatchExecutor.execute(TriggerExecutor.scala:65)\r\n[info] \tat org.apache.spark.sql.execution.streaming.runtime.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:458)\r\n[info] \tat org.apache.spark.sql.execution.streaming.runtime.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:347)\r\n[info] \tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\r\n[info] \tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\r\n[info] \tat org.apache.spark.sql.execution.streaming.runtime.StreamExecution.org$apache$spark$sql$execution$streaming$runtime$StreamExecution$$runStream(StreamExecution.scala:307)\r\n[info] \tat org.apache.spark.sql.execution.streaming.runtime.StreamExecution$$anon$1.run(StreamExecution.scala:230)\r\n[info]   at scala.Predef$.assert(Predef.scala:279)\r\n[info]   at org.apache.spark.TestUtils$.assertExceptionMsg(TestUtils.scala:198)\r\n[info]   at org.apache.spark.sql.kafka010.KafkaMicroBatchSourceSuiteBase.$anonfun$new$14(KafkaMicroBatchSourceSuite.scala:428)\r\n[info]   at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\r\n[info]   at org.scalatest.enablers.Timed$$anon$1.timeoutAfter(Timed.scala:127)\r\n[info]   at org.scalatest.concurrent.TimeLimits$.failAfterImpl(TimeLimits.scala:282)\r\n[info]   at org.scalatest.concurrent.TimeLimits.failAfter(TimeLimits.scala:231)\r\n[info]   at org.scalatest.concurrent.TimeLimits.failAfter$(TimeLimits.scala:230)\r\n[info]   at org.apache.spark.SparkFunSuite.failAfter(SparkFunSuite.scala:68)\r\n[info]   at org.apache.spark.SparkFunSuite.$anonfun$test$2(SparkFunSuite.scala:154)\r\n[info]   at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)\r\n[info]   at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)\r\n[info]   at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)\r\n[info]   at org.scalatest.Transformer.apply(Transformer.scala:22)\r\n[info]   at org.scalatest.Transformer.apply(Transformer.scala:20)\r\n[info]   at org.scalatest.funsuite.AnyFunSuiteLike$$anon$1.apply(AnyFunSuiteLike.scala:226)\r\n[info]   at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:226)\r\n[info]   at org.scalatest.funsuite.AnyFunSuiteLike.invokeWithFixture$1(AnyFunSuiteLike.scala:224)\r\n[info]   at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTest$1(AnyFunSuiteLike.scala:236)\r\n[info]   at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)\r\n[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTest(AnyFunSuiteLike.scala:236)\r\n[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTest$(AnyFunSuiteLike.scala:218)\r\n[info]   at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterEach$$super$runTest(SparkFunSuite.scala:68)\r\n[info]   at org.scalatest.BeforeAndAfterEach.runTest(BeforeAndAfterEach.scala:234)\r\n[info]   at org.scalatest.BeforeAndAfterEach.runTest$(BeforeAndAfterEach.scala:227)\r\n[info]   at org.apache.spark.SparkFunSuite.runTest(SparkFunSuite.scala:68)\r\n[info]   at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTests$1(AnyFunSuiteLike.scala:269)\r\n[info]   at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:413)\r\n[info]   at scala.collection.immutable.List.foreach(List.scala:323)\r\n[info]   at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)\r\n[info]   at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:396)\r\n[info]   at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:475)\r\n[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTests(AnyFunSuiteLike.scala:269)\r\n[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTests$(AnyFunSuiteLike.scala:268)\r\n[info]   at org.scalatest.funsuite.AnyFunSuite.runTests(AnyFunSuite.scala:1564)\r\n[info]   at org.scalatest.Suite.run(Suite.scala:1114)\r\n[info]   at org.scalatest.Suite.run$(Suite.scala:1096)\r\n[info]   at org.scalatest.funsuite.AnyFunSuite.org$scalatest$funsuite$AnyFunSuiteLike$$super$run(AnyFunSuite.scala:1564)\r\n[info]   at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$run$1(AnyFunSuiteLike.scala:273)\r\n[info]   at org.scalatest.SuperEngine.runImpl(Engine.scala:535)\r\n[info]   at org.scalatest.funsuite.AnyFunSuiteLike.run(AnyFunSuiteLike.scala:273)\r\n[info]   at org.scalatest.funsuite.AnyFunSuiteLike.run$(AnyFunSuiteLike.scala:272)\r\n[info]   at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterAll$$super$run(SparkFunSuite.scala:68)\r\n[info]   at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:213)\r\n[info]   at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)\r\n[info]   at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208)\r\n[info]   at org.apache.spark.SparkFunSuite.run(SparkFunSuite.scala:68)\r\n[info]   at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:321)\r\n[info]   at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:517)\r\n[info]   at sbt.ForkMain$Run.lambda$runTest$1(ForkMain.java:414)\r\n[info]   at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\r\n[info]   at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n[info]   at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n[info]   at java.base/java.lang.Thread.run(Thread.java:840)\r\n{code}"}}}
{"project": "SPARK", "issue_key": "SPARK-54073", "title": "Improve `ConfOptionDocGenerator` to generate a sorted doc by config key", "status": "Resolved", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "priority": "Major", "labels": ["pull-request-available"], "created": "2025-10-29T04:41:02.000+0000", "updated": "2025-10-29T15:07:17.000+0000", "description": null, "comments": [], "derived_tasks": {"summarization": "Improve `ConfOptionDocGenerator` to generate a sorted doc by config key", "classification": "feature", "qna": {"question": "What is the issue 'Improve `ConfOptionDocGenerator` to generate a sorted doc by config key' about?", "answer": null}}}
{"project": "SPARK", "issue_key": "SPARK-54072", "title": "Make sure we don't upload empty files in RocksDB snapshot", "status": "Resolved", "reporter": "B. Micheal Okutubo", "assignee": "B. Micheal Okutubo", "priority": "Major", "labels": ["pull-request-available"], "created": "2025-10-29T02:42:37.000+0000", "updated": "2025-10-29T21:08:09.000+0000", "description": "This change is to make sure the RocksDB snapshot zip file doesn't contain empty files (except RocksDB log file). Other files such as OPTION, manifest, metadata are not expected to be empty in any situation.\r\n\r\nIntroduced a conf\u00a0{{verifyNonEmptyFilesInZip}}\u00a0to make sure we can turn this off if needed.", "comments": [], "derived_tasks": {"summarization": "Make sure we don't upload empty files in RocksDB snapshot", "classification": "feature", "qna": {"question": "What is the issue 'Make sure we don't upload empty files in RocksDB snapshot' about?", "answer": "This change is to make sure the RocksDB snapshot zip file doesn't contain empty files (except RocksDB log file). Other files such as OPTION, manifest, metadata are not expected to be empty in any situation.\r\n\r\nIntroduced a conf\u00a0{{verifyNonEmptyFilesInZip}}\u00a0to make sure we can turn this off if needed."}}}
{"project": "SPARK", "issue_key": "SPARK-54071", "title": "Spark Structured Streaming Filesink can not generate open lineage with output details", "status": "Open", "reporter": "Yang Guo", "assignee": null, "priority": "Major", "labels": [], "created": "2025-10-28T23:15:44.000+0000", "updated": "2025-10-29T00:01:44.000+0000", "description": "h2. Environment details\r\nh3. OpenLineage version\r\n\r\n{quote}io.openlineage:openlineage-spark_2.13:1.39.0\r\nTechnology and package versions\r\n\r\nPython: 3.13.3\r\nScala: 2.13.16\r\nJava: OpenJDK 64-Bit Server VM, 17.0.16\r\n\r\npip freeze\r\npy4j==0.10.9.9\r\npyspark==4.0.1{quote}\r\n\r\n\r\nFor the openlineage set up, I used the default setting:\r\n\r\n\r\n{quote}$ git clone https://github.com/MarquezProject/marquez.git && cd marquez\r\n\r\n$ ./docker/up.sh{quote}\r\n\r\nh3. Spark Deployment details\r\n\r\nI used native spark on local machine. There is no managed services involved.\r\nProblem details\r\n\r\nh2. Issue details\r\nWhen using Spark structured streaming to write parquet file to file systems,\r\n\r\n*     File sink will only generate openlineage event with streaming processing type with output information as empty.\r\n*     Foreachbatch sink will generate openlineage event with both streaming processing type and batch processing type. The batch processing type will have valid output information.\r\n\r\nThe bug is that File sink in Spark structured streaming does not generate open lineage event with output details.\r\n\r\nMore details about the sample code and sample events are following.\r\nFile sink:\r\nSample code:\r\n{quote}\r\nquery = streaming_df.writeStream \\\r\n    .format('parquet') \\\r\n    .outputMode('append') \\\r\n    .partitionBy('year', 'month', 'day') \\\r\n    .option('checkpointLocation', checkpoint_path) \\\r\n    .option('path', output_path) \\\r\n    .queryName('filesink') \\\r\n    .start()\r\n{quote}\r\nSample event for \"processingType\":\"STREAMING\"\r\n{quote}\r\n25/10/29 00:49:02 DEBUG wire: http-outgoing-52 >> \"{\"eventTime\":\"2025-10-28T13:45:34.282Z\",\"producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.39.0/integration/spark\",\"schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent\",\"eventType\":\"COMPLETE\",\"run\":{\"runId\":\"019a2b14-4e9d-7574-95a9-55182f07591d\",\"facets\":{\"parent\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.39.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-0/ParentRunFacet.json#/$defs/ParentRunFacet\",\"run\":{\"runId\":\"019a2b10-5218-7ba3-9cf4-1ce4b1800752\"},\"job\":{\"namespace\":\"spark_namespace\",\"name\":\"filesink\"},\"root\":{\"run\":{\"runId\":\"019a2b10-5218-7ba3-9cf4-1ce4b1800752\"},\"job\":{\"namespace\":\"airflow_namespace\",\"name\":\"airflow_dag.airflow_task\"}}},\"spark_properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.39.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"properties\":{\"spark.master\":\"local[*]\",\"spark.app.name\":\"filesink\"}},\"processing_engine\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.39.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/ProcessingEngineRunFacet.json#/$defs/ProcessingEngineRunFacet\",\"version\":\"4.0.1\",\"name\":\"spark\",\"openlineageAdapterVersion\":\"1.39.0\"},\"environment-properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.39.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"environment-properties\":{}}}},\"job\":{\"namespace\":\"spark_namespace\",\"name\":\"filesink.project\",\"facets\":{\"jobType\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.39.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/2-0-3/JobTypeJobFacet.json#/$defs/JobTypeJobFacet\",\"processingType\":\"STREAMING\",\"integration\":\"SPARK\",\"jobType\":\"SQL_JOB\"}}},\"inputs\":[],\"outputs\":[]}\"{quote}\r\n\r\nforeachbatch sink\r\nsample code\r\n{quote}\r\ndef write_to_file(batch_df, batch_id):\r\n    if batch_df.count() > 0:\r\n        batch_df.write \\\r\n            .mode(\"append\") \\\r\n            .partitionBy(\"year\", \"month\", \"day\") \\\r\n            .parquet(output_path)\r\n{quote}\r\n{quote}\r\nquery = streaming_df \\\r\n    .writeStream \\\r\n    .outputMode(\"append\") \\\r\n    .foreachBatch(write_to_file) \\\r\n    .option(\"checkpointLocation\", checkpoint_path) \\\r\n    .trigger(processingTime='10 seconds') \\\r\n    .start()\r\n{quote}\r\n\r\nThe above code with generate both streaming and batch processing type event.\r\nSample streaming type event\r\n{quote}\r\n25/10/29 01:04:45 DEBUG wire: http-outgoing-1 >> \"{\"eventTime\":\"2025-10-28T14:04:43.373Z\",\"producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.39.0/integration/spark\",\"schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent\",\"eventType\":\"COMPLETE\",\"run\":{\"runId\":\"019a2b22-b364-79ca-be87-2d173c25c16c\",\"facets\":{\"parent\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.39.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-0/ParentRunFacet.json#/$defs/ParentRunFacet\",\"run\":{\"runId\":\"019a2b22-9e86-770e-8af9-f9136ec6594c\"},\"job\":{\"namespace\":\"spark_namespace\",\"name\":\"foreachbatchsink\"},\"root\":{\"run\":{\"runId\":\"019a2b22-9e86-770e-8af9-f9136ec6594c\"},\"job\":{\"namespace\":\"airflow_namespace\",\"name\":\"airflow_dag.airflow_task\"}}},\"spark_properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.39.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"properties\":{\"spark.master\":\"local[*]\",\"spark.app.name\":\"foreachbatchsink\"}},\"processing_engine\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.39.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/ProcessingEngineRunFacet.json#/$defs/ProcessingEngineRunFacet\",\"version\":\"4.0.1\",\"name\":\"spark\",\"openlineageAdapterVersion\":\"1.39.0\"},\"environment-properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.39.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"environment-properties\":{}}}},\"job\":{\"namespace\":\"spark_namespace\",\"name\":\"foreachbatchsink.project\",\"facets\":{\"jobType\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.39.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/2-0-3/JobTypeJobFacet.json#/$defs/JobTypeJobFacet\",\"processingType\":\"STREAMING\",\"integration\":\"SPARK\",\"jobType\":\"SQL_JOB\"}}},\"inputs\":[],\"outputs\":[]}\"\r\n{quote}\r\nSample batch type event\r\n{quote}\r\n25/10/29 01:07:26 DEBUG wire: http-outgoing-33 >> \"{\"eventTime\":\"2025-10-28T14:07:20.711Z\",\"producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.39.0/integration/spark\",\"schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent\",\"eventType\":\"COMPLETE\",\"run\":{\"runId\":\"019a2b25-28f6-7fa0-a4e2-2aaba4f61d7e\",\"facets\":{\"parent\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.39.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-0/ParentRunFacet.json#/$defs/ParentRunFacet\",\"run\":{\"runId\":\"019a2b22-9e86-770e-8af9-f9136ec6594c\"},\"job\":{\"namespace\":\"spark_namespace\",\"name\":\"foreachbatchsink\"},\"root\":{\"run\":{\"runId\":\"019a2b22-9e86-770e-8af9-f9136ec6594c\"},\"job\":{\"namespace\":\"airflow_namespace\",\"name\":\"airflow_dag.airflow_task\"}}},\"spark_properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.39.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"properties\":{\"spark.master\":\"local[*]\",\"spark.app.name\":\"foreachbatchsink\"}},\"processing_engine\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.39.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/ProcessingEngineRunFacet.json#/$defs/ProcessingEngineRunFacet\",\"version\":\"4.0.1\",\"name\":\"spark\",\"openlineageAdapterVersion\":\"1.39.0\"},\"environment-properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.39.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"environment-properties\":{}}}},\"job\":{\"namespace\":\"spark_namespace\",\"name\":\"foreachbatchsink.execute_insert_into_hadoop_fs_relation_command.tests_output_foreachbatchsink\",\"facets\":{\"jobType\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.39.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/2-0-3/JobTypeJobFacet.json#/$defs/JobTypeJobFacet\",\"processingType\":\"BATCH\",\"integration\":\"SPARK\",\"jobType\":\"SQL_JOB\"}}},\"inputs\":[],\"outputs\":[{\"namespace\":\"file\",\"name\":\"/Users/xxxx/venvs/tests/output_foreachbatchsink\",\"facets\":{\"dataSource\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.39.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/DatasourceDatasetFacet.json#/$defs/DatasourceDatasetFacet\",\"name\":\"file\",\"uri\":\"file\"},\"schema\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.39.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/SchemaDatasetFacet.json#/$defs/SchemaDatasetFacet\",\"fields\":[{\"name\":\"id\",\"type\":\"long\"},{\"name\":\"name\",\"type\":\"string\"},{\"name\":\"timestamp\",\"type\":\"timestamp\"},{\"name\":\"value\",\"type\":\"double\"},{\"name\":\"year\",\"type\":\"integer\"},{\"name\":\"month\",\"type\":\"integer\"},{\"name\":\"day\",\"type\":\"integer\"}]}},\"outputFacets\":{\"outputStatistics\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.39.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-2/OutputStatisticsOutputDatasetFacet.json#/$defs/OutputStatisticsOutputDatasetFacet\",\"rowCount\":10,\"size\":13259,\"fileCount\":10}}}]}\"{quote}\r\n\r\nWhat you think should happen instead\r\n\r\nFile sink for spark structured streaming should create open lineage event with valid output details as the information is in the spark query logic plan.\r\n\r\nHere is the logic plan for streaming query using file sink.\r\n{quote}\r\n== Analyzed Logical Plan ==\r\nid: bigint, name: string, timestamp: timestamp, value: double, year: int, month: int, day: int\r\n~WriteToMicroBatchDataSourceV1 FileSink[file:/Users/xxx/venvs/tests/output_filesink], 753bdc9a-07cd-4788-a17d-27ff622ababc, [checkpointLocation=checkpoint_filesink, path=output_filesink, queryName=filesink], Append, 1\r\n+- ~Project [id#2L, name#3, timestamp#0, value#4, year#5, month#6, dayofmonth(cast(timestamp#0 as date)) AS day#7]\r\n   +- ~Project [id#2L, name#3, timestamp#0, value#4, year#5, month(cast(timestamp#0 as date)) AS month#6]\r\n      +- ~Project [id#2L, name#3, timestamp#0, value#4, year(cast(timestamp#0 as date)) AS year#5]\r\n         +- ~Project [(value#1L % cast(1000 as bigint)) AS id#2L, concat(user_, cast((value#1L % cast(100 as bigint)) as string)) AS name#3, timestamp#0, (rand(-1344458628259366487) * cast(100 as double)) AS value#4]\r\n            +- ~StreamingDataSourceV2ScanRelation[timestamp#0, value#1L] RateStream(rowsPerSecond=1, rampUpTimeSeconds=0, numPartitions=12)\r\n{quote}\r\nHow to reproduce\r\n\r\n*     step 1: install pyspark on your local machine https://spark.apache.org/docs/latest/api/python/getting_started/install.html\r\n*     step 2: install openlineage server on your local machine https://openlineage.io/getting-started\r\n*     step 3: refer to following spark-submit command to run file_sink.py and foreachbatch_sink.py. You will see the open lineage event in the debug logs.\r\n\r\n{quote}\r\nspark-submit   --packages io.openlineage:openlineage-spark_2.13:1.39.0   --conf \"spark.extraListeners=io.openlineage.spark.agent.OpenLineageSparkListener\"   --conf \"spark.openlineage.transport.type=http\"   --conf \"spark.openlineage.transport.url=http://localhost:5000\"   --conf \"spark.openlineage.namespace=spark_namespace\"   --conf \"spark.openlineage.parentJobNamespace=airflow_namespace\"   --conf \"spark.openlineage.parentJobName=airflow_dag.airflow_task\"   --conf \"spark.openlineage.parentRunId=xxxx-xxxx-xxxx-xxxx\"   [filename].py\r\n{quote}\r\n", "comments": [], "derived_tasks": {"summarization": "Spark Structured Streaming Filesink can not generate open lineage with output details", "classification": "feature", "qna": {"question": "What is the issue 'Spark Structured Streaming Filesink can not generate open lineage with output details' about?", "answer": "h2. Environment details\r\nh3. OpenLineage version\r\n\r\n{quote}io.openlineage:openlineage-spark_2.13:1.39.0\r\nTechnology and package versions\r\n\r\nPython: 3.13.3\r\nScala: 2.13.16\r\nJava: OpenJDK 64-Bit Server VM, 17.0.16\r\n\r\npip freeze\r\npy4j==0.10.9.9\r\npyspark==4.0.1{quote}\r\n\r\n\r\nFor the openlineage set up, I used the default setting:\r\n\r\n\r\n{quote}$ git clone https://github.com/MarquezProject/marquez.git && cd marquez\r\n\r\n$ ./docker/up.sh{quote}\r\n\r\nh3. Spark Deployment details\r\n\r\nI used native spark on local machine. There is no managed services involved.\r\nProblem details\r\n\r\nh2. Issue details\r\nWhen using Spark structured streaming to write parquet file to file systems,\r\n\r\n*     File sink will only generate openlineage event with streaming processing type with output information as empty.\r\n*     Foreachbatch sink will generate openlineage event with both streaming processing type and batch processing type. The batch processing type will have valid output information.\r\n\r\nThe bug is that File sink in Spark structured streaming does not generate open lineage event with output details.\r\n\r\nMore details about the sample code and sample events are following.\r\nFile sink:\r\nSample code:\r\n{quote}\r\nquery = streaming_df.writeStream \\\r\n    .format('parquet') \\\r\n    .outputMode('append') \\\r\n    .partitionBy('year', 'month', 'day') \\\r\n    .option('checkpointLocation', checkpoint_path) \\\r\n    .option('path', output_path) \\\r\n    .queryName('filesink') \\\r\n    .start()\r\n{quote}\r\nSample event for \"processingType\":\"STREAMING\"\r\n{quote}\r\n25/10/29 00:49:02 DEBUG wire: http-outgoing-52 >> \"{\"eventTime\":\"2025-10-28T13:45:34.282Z\",\"producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.39.0/integration/spark\",\"schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent\",\"eventType\":\"COMPLETE\",\"run\":{\"runId\":\"019a2b14-4e9d-7574-95a9-55182f07591d\",\"facets\":{\"parent\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.39.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-0/ParentRunFacet.json#/$defs/ParentRunFacet\",\"run\":{\"runId\":\"019a2b10-5218-7ba3-9cf4-1ce4b1800752\"},\"job\":{\"namespace\":\"spark_namespace\",\"name\":\"filesink\"},\"root\":{\"run\":{\"runId\":\"019a2b10-5218-7ba3-9cf4-1ce4b1800752\"},\"job\":{\"namespace\":\"airflow_namespace\",\"name\":\"airflow_dag.airflow_task\"}}},\"spark_properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.39.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"properties\":{\"spark.master\":\"local[*]\",\"spark.app.name\":\"filesink\"}},\"processing_engine\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.39.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/ProcessingEngineRunFacet.json#/$defs/ProcessingEngineRunFacet\",\"version\":\"4.0.1\",\"name\":\"spark\",\"openlineageAdapterVersion\":\"1.39.0\"},\"environment-properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.39.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"environment-properties\":{}}}},\"job\":{\"namespace\":\"spark_namespace\",\"name\":\"filesink.project\",\"facets\":{\"jobType\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.39.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/2-0-3/JobTypeJobFacet.json#/$defs/JobTypeJobFacet\",\"processingType\":\"STREAMING\",\"integration\":\"SPARK\",\"jobType\":\"SQL_JOB\"}}},\"inputs\":[],\"outputs\":[]}\"{quote}\r\n\r\nforeachbatch sink\r\nsample code\r\n{quote}\r\ndef write_to_file(batch_df, batch_id):\r\n    if batch_df.count() > 0:\r\n        batch_df.write \\\r\n            .mode(\"append\") \\\r\n            .partitionBy(\"year\", \"month\", \"day\") \\\r\n            .parquet(output_path)\r\n{quote}\r\n{quote}\r\nquery = streaming_df \\\r\n    .writeStream \\\r\n    .outputMode(\"append\") \\\r\n    .foreachBatch(write_to_file) \\\r\n    .option(\"checkpointLocation\", checkpoint_path) \\\r\n    .trigger(processingTime='10 seconds') \\\r\n    .start()\r\n{quote}\r\n\r\nThe above code with generate both streaming and batch processing type event.\r\nSample streaming type event\r\n{quote}\r\n25/10/29 01:04:45 DEBUG wire: http-outgoing-1 >> \"{\"eventTime\":\"2025-10-28T14:04:43.373Z\",\"producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.39.0/integration/spark\",\"schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent\",\"eventType\":\"COMPLETE\",\"run\":{\"runId\":\"019a2b22-b364-79ca-be87-2d173c25c16c\",\"facets\":{\"parent\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.39.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-0/ParentRunFacet.json#/$defs/ParentRunFacet\",\"run\":{\"runId\":\"019a2b22-9e86-770e-8af9-f9136ec6594c\"},\"job\":{\"namespace\":\"spark_namespace\",\"name\":\"foreachbatchsink\"},\"root\":{\"run\":{\"runId\":\"019a2b22-9e86-770e-8af9-f9136ec6594c\"},\"job\":{\"namespace\":\"airflow_namespace\",\"name\":\"airflow_dag.airflow_task\"}}},\"spark_properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.39.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"properties\":{\"spark.master\":\"local[*]\",\"spark.app.name\":\"foreachbatchsink\"}},\"processing_engine\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.39.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/ProcessingEngineRunFacet.json#/$defs/ProcessingEngineRunFacet\",\"version\":\"4.0.1\",\"name\":\"spark\",\"openlineageAdapterVersion\":\"1.39.0\"},\"environment-properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.39.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"environment-properties\":{}}}},\"job\":{\"namespace\":\"spark_namespace\",\"name\":\"foreachbatchsink.project\",\"facets\":{\"jobType\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.39.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/2-0-3/JobTypeJobFacet.json#/$defs/JobTypeJobFacet\",\"processingType\":\"STREAMING\",\"integration\":\"SPARK\",\"jobType\":\"SQL_JOB\"}}},\"inputs\":[],\"outputs\":[]}\"\r\n{quote}\r\nSample batch type event\r\n{quote}\r\n25/10/29 01:07:26 DEBUG wire: http-outgoing-33 >> \"{\"eventTime\":\"2025-10-28T14:07:20.711Z\",\"producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.39.0/integration/spark\",\"schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent\",\"eventType\":\"COMPLETE\",\"run\":{\"runId\":\"019a2b25-28f6-7fa0-a4e2-2aaba4f61d7e\",\"facets\":{\"parent\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.39.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-0/ParentRunFacet.json#/$defs/ParentRunFacet\",\"run\":{\"runId\":\"019a2b22-9e86-770e-8af9-f9136ec6594c\"},\"job\":{\"namespace\":\"spark_namespace\",\"name\":\"foreachbatchsink\"},\"root\":{\"run\":{\"runId\":\"019a2b22-9e86-770e-8af9-f9136ec6594c\"},\"job\":{\"namespace\":\"airflow_namespace\",\"name\":\"airflow_dag.airflow_task\"}}},\"spark_properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.39.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"properties\":{\"spark.master\":\"local[*]\",\"spark.app.name\":\"foreachbatchsink\"}},\"processing_engine\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.39.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/ProcessingEngineRunFacet.json#/$defs/ProcessingEngineRunFacet\",\"version\":\"4.0.1\",\"name\":\"spark\",\"openlineageAdapterVersion\":\"1.39.0\"},\"environment-properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.39.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"environment-properties\":{}}}},\"job\":{\"namespace\":\"spark_namespace\",\"name\":\"foreachbatchsink.execute_insert_into_hadoop_fs_relation_command.tests_output_foreachbatchsink\",\"facets\":{\"jobType\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.39.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/2-0-3/JobTypeJobFacet.json#/$defs/JobTypeJobFacet\",\"processingType\":\"BATCH\",\"integration\":\"SPARK\",\"jobType\":\"SQL_JOB\"}}},\"inputs\":[],\"outputs\":[{\"namespace\":\"file\",\"name\":\"/Users/xxxx/venvs/tests/output_foreachbatchsink\",\"facets\":{\"dataSource\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.39.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/DatasourceDatasetFacet.json#/$defs/DatasourceDatasetFacet\",\"name\":\"file\",\"uri\":\"file\"},\"schema\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.39.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/SchemaDatasetFacet.json#/$defs/SchemaDatasetFacet\",\"fields\":[{\"name\":\"id\",\"type\":\"long\"},{\"name\":\"name\",\"type\":\"string\"},{\"name\":\"timestamp\",\"type\":\"timestamp\"},{\"name\":\"value\",\"type\":\"double\"},{\"name\":\"year\",\"type\":\"integer\"},{\"name\":\"month\",\"type\":\"integer\"},{\"name\":\"day\",\"type\":\"integer\"}]}},\"outputFacets\":{\"outputStatistics\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.39.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-2/OutputStatisticsOutputDatasetFacet.json#/$defs/OutputStatisticsOutputDatasetFacet\",\"rowCount\":10,\"size\":13259,\"fileCount\":10}}}]}\"{quote}\r\n\r\nWhat you think should happen instead\r\n\r\nFile sink for spark structured streaming should create open lineage event with valid output details as the information is in the spark query logic plan.\r\n\r\nHere is the logic plan for streaming query using file sink.\r\n{quote}\r\n== Analyzed Logical Plan ==\r\nid: bigint, name: string, timestamp: timestamp, value: double, year: int, month: int, day: int\r\n~WriteToMicroBatchDataSourceV1 FileSink[file:/Users/xxx/venvs/tests/output_filesink], 753bdc9a-07cd-4788-a17d-27ff622ababc, [checkpointLocation=checkpoint_filesink, path=output_filesink, queryName=filesink], Append, 1\r\n+- ~Project [id#2L, name#3, timestamp#0, value#4, year#5, month#6, dayofmonth(cast(timestamp#0 as date)) AS day#7]\r\n   +- ~Project [id#2L, name#3, timestamp#0, value#4, year#5, month(cast(timestamp#0 as date)) AS month#6]\r\n      +- ~Project [id#2L, name#3, timestamp#0, value#4, year(cast(timestamp#0 as date)) AS year#5]\r\n         +- ~Project [(value#1L % cast(1000 as bigint)) AS id#2L, concat(user_, cast((value#1L % cast(100 as bigint)) as string)) AS name#3, timestamp#0, (rand(-1344458628259366487) * cast(100 as double)) AS value#4]\r\n            +- ~StreamingDataSourceV2ScanRelation[timestamp#0, value#1L] RateStream(rowsPerSecond=1, rampUpTimeSeconds=0, numPartitions=12)\r\n{quote}\r\nHow to reproduce\r\n\r\n*     step 1: install pyspark on your local machine https://spark.apache.org/docs/latest/api/python/getting_started/install.html\r\n*     step 2: install openlineage server on your local machine https://openlineage.io/getting-started\r\n*     step 3: refer to following spark-submit command to run file_sink.py and foreachbatch_sink.py. You will see the open lineage event in the debug logs.\r\n\r\n{quote}\r\nspark-submit   --packages io.openlineage:openlineage-spark_2.13:1.39.0   --conf \"spark.extraListeners=io.openlineage.spark.agent.OpenLineageSparkListener\"   --conf \"spark.openlineage.transport.type=http\"   --conf \"spark.openlineage.transport.url=http://localhost:5000\"   --conf \"spark.openlineage.namespace=spark_namespace\"   --conf \"spark.openlineage.parentJobNamespace=airflow_namespace\"   --conf \"spark.openlineage.parentJobName=airflow_dag.airflow_task\"   --conf \"spark.openlineage.parentRunId=xxxx-xxxx-xxxx-xxxx\"   [filename].py\r\n{quote}\r\n"}}}
{"project": "SPARK", "issue_key": "SPARK-54070", "title": "Add spark.logConf configuration to operator docs", "status": "Resolved", "reporter": "Zhou JIANG", "assignee": "Zhou JIANG", "priority": "Minor", "labels": ["pull-request-available"], "created": "2025-10-28T21:58:38.000+0000", "updated": "2025-10-29T06:29:07.000+0000", "description": null, "comments": [], "derived_tasks": {"summarization": "Add spark.logConf configuration to operator docs", "classification": "feature", "qna": {"question": "What is the issue 'Add spark.logConf configuration to operator docs' about?", "answer": null}}}
{"project": "SPARK", "issue_key": "SPARK-54069", "title": "Skip `test_to_feather` in Python 3.14", "status": "Resolved", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "priority": "Major", "labels": ["pull-request-available"], "created": "2025-10-28T20:56:28.000+0000", "updated": "2025-10-29T03:15:29.000+0000", "description": null, "comments": [], "derived_tasks": {"summarization": "Skip `test_to_feather` in Python 3.14", "classification": "feature", "qna": {"question": "What is the issue 'Skip `test_to_feather` in Python 3.14' about?", "answer": null}}}
{"project": "SPARK", "issue_key": "SPARK-54068", "title": "Fix `pyspark.pandas.tests.connect.io.test_parity_feather.FeatherParityTests.test_to_feather` in Python 3.14", "status": "Open", "reporter": "Dongjoon Hyun", "assignee": null, "priority": "Major", "labels": [], "created": "2025-10-28T20:55:17.000+0000", "updated": "2025-10-28T20:55:17.000+0000", "description": "{code}\r\n======================================================================\r\nERROR [1.960s]: test_to_feather (pyspark.pandas.tests.connect.io.test_parity_feather.FeatherParityTests.test_to_feather)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/__w/spark/spark/python/pyspark/pandas/tests/io/test_feather.py\", line 43, in test_to_feather\r\n    self.psdf.to_feather(path2)\r\n    ~~~~~~~~~~~~~~~~~~~~^^^^^^^\r\n  File \"/__w/spark/spark/python/pyspark/pandas/frame.py\", line 2702, in to_feather\r\n    return validate_arguments_and_invoke_function(\r\n        self._to_internal_pandas(), self.to_feather, pd.DataFrame.to_feather, args\r\n    )\r\n  File \"/__w/spark/spark/python/pyspark/pandas/utils.py\", line 592, in validate_arguments_and_invoke_function\r\n    return pandas_func(**args)\r\n  File \"/usr/local/lib/python3.14/dist-packages/pandas/core/frame.py\", line 2949, in to_feather\r\n    to_feather(self, path, **kwargs)\r\n    ~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.14/dist-packages/pandas/io/feather_format.py\", line 65, in to_feather\r\n    feather.write_feather(df, handles.handle, **kwargs)\r\n    ~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.14/dist-packages/pyarrow/feather.py\", line 156, in write_feather\r\n    table = Table.from_pandas(df, preserve_index=preserve_index)\r\n  File \"pyarrow/table.pxi\", line 4795, in pyarrow.lib.Table.from_pandas\r\n  File \"/usr/local/lib/python3.14/dist-packages/pyarrow/pandas_compat.py\", line 663, in dataframe_to_arrays\r\n    pandas_metadata = construct_metadata(\r\n        columns_to_convert, df, column_names, index_columns, index_descriptors,\r\n        preserve_index, types, column_field_names=column_field_names\r\n    )\r\n  File \"/usr/local/lib/python3.14/dist-packages/pyarrow/pandas_compat.py\", line 281, in construct_metadata\r\n    b'pandas': json.dumps({\r\n               ~~~~~~~~~~^^\r\n        'index_columns': index_descriptors,\r\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n    ...<7 lines>...\r\n        'pandas_version': _pandas_api.version\r\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n    }).encode('utf8')\r\n    ^^\r\n  File \"/usr/lib/python3.14/json/__init__.py\", line 231, in dumps\r\n    return _default_encoder.encode(obj)\r\n           ~~~~~~~~~~~~~~~~~~~~~~~^^^^^\r\n  File \"/usr/lib/python3.14/json/encoder.py\", line 200, in encode\r\n    chunks = self.iterencode(o, _one_shot=True)\r\n  File \"/usr/lib/python3.14/json/encoder.py\", line 261, in iterencode\r\n    return _iterencode(o, 0)\r\n  File \"/usr/lib/python3.14/json/encoder.py\", line 180, in default\r\n    raise TypeError(f'Object of type {o.__class__.__name__} '\r\n                    f'is not JSON serializable')\r\nTypeError: Object of type PlanMetrics is not JSON serializable\r\nwhen serializing list item 0\r\nwhen serializing dict item 'metrics'\r\nwhen serializing dict item 'attributes'\r\n{code}", "comments": [], "derived_tasks": {"summarization": "Fix `pyspark.pandas.tests.connect.io.test_parity_feather.FeatherParityTests.test_to_feather` in Python 3.14", "classification": "feature", "qna": {"question": "What is the issue 'Fix `pyspark.pandas.tests.connect.io.test_parity_feather.FeatherParityTests.test_to_feather` in Python 3.14' about?", "answer": "{code}\r\n======================================================================\r\nERROR [1.960s]: test_to_feather (pyspark.pandas.tests.connect.io.test_parity_feather.FeatherParityTests.test_to_feather)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/__w/spark/spark/python/pyspark/pandas/tests/io/test_feather.py\", line 43, in test_to_feather\r\n    self.psdf.to_feather(path2)\r\n    ~~~~~~~~~~~~~~~~~~~~^^^^^^^\r\n  File \"/__w/spark/spark/python/pyspark/pandas/frame.py\", line 2702, in to_feather\r\n    return validate_arguments_and_invoke_function(\r\n        self._to_internal_pandas(), self.to_feather, pd.DataFrame.to_feather, args\r\n    )\r\n  File \"/__w/spark/spark/python/pyspark/pandas/utils.py\", line 592, in validate_arguments_and_invoke_function\r\n    return pandas_func(**args)\r\n  File \"/usr/local/lib/python3.14/dist-packages/pandas/core/frame.py\", line 2949, in to_feather\r\n    to_feather(self, path, **kwargs)\r\n    ~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.14/dist-packages/pandas/io/feather_format.py\", line 65, in to_feather\r\n    feather.write_feather(df, handles.handle, **kwargs)\r\n    ~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.14/dist-packages/pyarrow/feather.py\", line 156, in write_feather\r\n    table = Table.from_pandas(df, preserve_index=preserve_index)\r\n  File \"pyarrow/table.pxi\", line 4795, in pyarrow.lib.Table.from_pandas\r\n  File \"/usr/local/lib/python3.14/dist-packages/pyarrow/pandas_compat.py\", line 663, in dataframe_to_arrays\r\n    pandas_metadata = construct_metadata(\r\n        columns_to_convert, df, column_names, index_columns, index_descriptors,\r\n        preserve_index, types, column_field_names=column_field_names\r\n    )\r\n  File \"/usr/local/lib/python3.14/dist-packages/pyarrow/pandas_compat.py\", line 281, in construct_metadata\r\n    b'pandas': json.dumps({\r\n               ~~~~~~~~~~^^\r\n        'index_columns': index_descriptors,\r\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n    ...<7 lines>...\r\n        'pandas_version': _pandas_api.version\r\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n    }).encode('utf8')\r\n    ^^\r\n  File \"/usr/lib/python3.14/json/__init__.py\", line 231, in dumps\r\n    return _default_encoder.encode(obj)\r\n           ~~~~~~~~~~~~~~~~~~~~~~~^^^^^\r\n  File \"/usr/lib/python3.14/json/encoder.py\", line 200, in encode\r\n    chunks = self.iterencode(o, _one_shot=True)\r\n  File \"/usr/lib/python3.14/json/encoder.py\", line 261, in iterencode\r\n    return _iterencode(o, 0)\r\n  File \"/usr/lib/python3.14/json/encoder.py\", line 180, in default\r\n    raise TypeError(f'Object of type {o.__class__.__name__} '\r\n                    f'is not JSON serializable')\r\nTypeError: Object of type PlanMetrics is not JSON serializable\r\nwhen serializing list item 0\r\nwhen serializing dict item 'metrics'\r\nwhen serializing dict item 'attributes'\r\n{code}"}}}
{"project": "SPARK", "issue_key": "SPARK-54067", "title": "Improve `SparkSubmit` to invoke `exitFn` with the root cause instead of `SparkUserAppException`", "status": "Resolved", "reporter": "Sanford Ryza", "assignee": "Sanford Ryza", "priority": "Major", "labels": ["pull-request-available"], "created": "2025-10-28T20:32:23.000+0000", "updated": "2025-10-30T17:47:37.000+0000", "description": "I hit this\u00a0when I ran a pipeline that had no flows:\r\n```\r\norg.apache.spark.SparkUserAppException: User application exited with 1\r\n\tat org.apache.spark.deploy.PythonRunner$.main(PythonRunner.scala:127)\r\n\tat org.apache.spark.deploy.PythonRunner.main(PythonRunner.scala)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\r\n\tat org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)\r\n\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:1028)\r\n\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:203)\r\n\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:226)\r\n\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:95)\r\n\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1166)\r\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1175)\r\n\tat org.apache.spark.deploy.SparkPipelines$.main(SparkPipelines.scala:42)\r\n\tat org.apache.spark.deploy.SparkPipelines.main(SparkPipelines.scala)\r\n```\r\n\r\nThis is not information that's relevant to the user.", "comments": [], "derived_tasks": {"summarization": "Improve `SparkSubmit` to invoke `exitFn` with the root cause instead of `SparkUserAppException`", "classification": "feature", "qna": {"question": "What is the issue 'Improve `SparkSubmit` to invoke `exitFn` with the root cause instead of `SparkUserAppException`' about?", "answer": "I hit this\u00a0when I ran a pipeline that had no flows:\r\n```\r\norg.apache.spark.SparkUserAppException: User application exited with 1\r\n\tat org.apache.spark.deploy.PythonRunner$.main(PythonRunner.scala:127)\r\n\tat org.apache.spark.deploy.PythonRunner.main(PythonRunner.scala)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\r\n\tat org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)\r\n\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:1028)\r\n\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:203)\r\n\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:226)\r\n\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:95)\r\n\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1166)\r\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1175)\r\n\tat org.apache.spark.deploy.SparkPipelines$.main(SparkPipelines.scala:42)\r\n\tat org.apache.spark.deploy.SparkPipelines.main(SparkPipelines.scala)\r\n```\r\n\r\nThis is not information that's relevant to the user."}}}
{"project": "SPARK", "issue_key": "SPARK-54066", "title": "Skip `test_in_memory_data_source` in Python 3.14", "status": "Resolved", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "priority": "Major", "labels": ["pull-request-available"], "created": "2025-10-28T19:51:29.000+0000", "updated": "2025-10-29T03:14:52.000+0000", "description": null, "comments": [], "derived_tasks": {"summarization": "Skip `test_in_memory_data_source` in Python 3.14", "classification": "feature", "qna": {"question": "What is the issue 'Skip `test_in_memory_data_source` in Python 3.14' about?", "answer": null}}}
{"project": "SPARK", "issue_key": "SPARK-54065", "title": "Fix `test_in_memory_data_source` in Python 3.14", "status": "Open", "reporter": "Dongjoon Hyun", "assignee": null, "priority": "Major", "labels": [], "created": "2025-10-28T19:49:33.000+0000", "updated": "2025-10-28T19:54:26.000+0000", "description": "{code}\r\n======================================================================\r\nERROR [0.007s]: test_in_memory_data_source (pyspark.sql.tests.test_python_datasource.PythonDataSourceTests.test_in_memory_data_source)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/__w/spark/spark/python/pyspark/serializers.py\", line 460, in dumps\r\n    return cloudpickle.dumps(obj, pickle_protocol)\r\n           ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/__w/spark/spark/python/pyspark/cloudpickle/cloudpickle.py\", line 1537, in dumps\r\n    cp.dump(obj)\r\n    ~~~~~~~^^^^^\r\n  File \"/__w/spark/spark/python/pyspark/cloudpickle/cloudpickle.py\", line 1303, in dump\r\n    return super().dump(obj)\r\n           ~~~~~~~~~~~~^^^^^\r\nTypeError: cannot pickle '_abc._abc_data' object\r\nwhen serializing dict item '_abc_impl'\r\nwhen serializing tuple item 0\r\nwhen serializing cell reconstructor arguments\r\nwhen serializing cell object\r\nwhen serializing tuple item 0\r\nwhen serializing dict item '__closure__'\r\nwhen serializing tuple item 1\r\nwhen serializing function state\r\nwhen serializing function object\r\nwhen serializing dict item '__annotate_func__'\r\nwhen serializing tuple item 0\r\nwhen serializing abc.ABCMeta state\r\nwhen serializing abc.ABCMeta object\r\nwhen serializing tuple item 0\r\nwhen serializing cell reconstructor arguments\r\nwhen serializing cell object\r\nwhen serializing tuple item 0\r\nwhen serializing dict item '__closure__'\r\nwhen serializing tuple item 1\r\nwhen serializing function state\r\nwhen serializing function object\r\nwhen serializing dict item 'reader'\r\nwhen serializing tuple item 0\r\nwhen serializing abc.ABCMeta state\r\nwhen serializing abc.ABCMeta object\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/__w/spark/spark/python/pyspark/sql/tests/test_python_datasource.py\", line 283, in test_in_memory_data_source\r\n    self.spark.dataSource.register(InMemoryDataSource)\r\n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^\r\n  File \"/__w/spark/spark/python/pyspark/sql/datasource.py\", line 1197, in register\r\n    wrapped = _wrap_function(sc, dataSource)\r\n  File \"/__w/spark/spark/python/pyspark/sql/udf.py\", line 59, in _wrap_function\r\n    pickled_command, broadcast_vars, env, includes = _prepare_for_python_RDD(sc, command)\r\n                                                     ~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^\r\n  File \"/__w/spark/spark/python/pyspark/core/rdd.py\", line 5121, in _prepare_for_python_RDD\r\n    pickled_command = ser.dumps(command)\r\n  File \"/__w/spark/spark/python/pyspark/serializers.py\", line 470, in dumps\r\n    raise pickle.PicklingError(msg)\r\n_pickle.PicklingError: Could not serialize object: TypeError: cannot pickle '_abc._abc_data' object\r\n{code}\r\n\r\n{code}\r\n======================================================================\r\nERROR [0.014s]: test_in_memory_data_source (pyspark.sql.tests.connect.test_parity_python_datasource.PythonDataSourceParityTests.test_in_memory_data_source)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/__w/spark/spark/python/pyspark/serializers.py\", line 460, in dumps\r\n    return cloudpickle.dumps(obj, pickle_protocol)\r\n           ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/__w/spark/spark/python/pyspark/cloudpickle/cloudpickle.py\", line 1537, in dumps\r\n    cp.dump(obj)\r\n    ~~~~~~~^^^^^\r\n  File \"/__w/spark/spark/python/pyspark/cloudpickle/cloudpickle.py\", line 1303, in dump\r\n    return super().dump(obj)\r\n           ~~~~~~~~~~~~^^^^^\r\nTypeError: cannot pickle '_abc._abc_data' object\r\nwhen serializing dict item '_abc_impl'\r\nwhen serializing tuple item 0\r\nwhen serializing cell reconstructor arguments\r\nwhen serializing cell object\r\nwhen serializing tuple item 0\r\nwhen serializing dict item '__closure__'\r\nwhen serializing tuple item 1\r\nwhen serializing function state\r\nwhen serializing function object\r\nwhen serializing dict item '__annotate_func__'\r\nwhen serializing tuple item 0\r\nwhen serializing abc.ABCMeta state\r\nwhen serializing abc.ABCMeta object\r\nwhen serializing tuple item 0\r\nwhen serializing cell reconstructor arguments\r\nwhen serializing cell object\r\nwhen serializing tuple item 0\r\nwhen serializing dict item '__closure__'\r\nwhen serializing tuple item 1\r\nwhen serializing function state\r\nwhen serializing function object\r\nwhen serializing dict item 'reader'\r\nwhen serializing tuple item 0\r\nwhen serializing abc.ABCMeta state\r\nwhen serializing abc.ABCMeta object\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/__w/spark/spark/python/pyspark/sql/tests/test_python_datasource.py\", line 283, in test_in_memory_data_source\r\n    self.spark.dataSource.register(InMemoryDataSource)\r\n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^\r\n  File \"/__w/spark/spark/python/pyspark/sql/connect/datasource.py\", line 45, in register\r\n    self.sparkSession._client.register_data_source(dataSource)\r\n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^\r\n  File \"/__w/spark/spark/python/pyspark/sql/connect/client/core.py\", line 863, in register_data_source\r\n    ).to_data_source_proto(self)\r\n      ~~~~~~~~~~~~~~~~~~~~^^^^^^\r\n  File \"/__w/spark/spark/python/pyspark/sql/connect/plan.py\", line 2833, in to_data_source_proto\r\n    plan.python_data_source.CopyFrom(self._data_source.to_plan(session))\r\n                                     ~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^\r\n  File \"/__w/spark/spark/python/pyspark/sql/connect/plan.py\", line 2807, in to_plan\r\n    ds.command = CloudPickleSerializer().dumps(self._data_source)\r\n                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^\r\n  File \"/__w/spark/spark/python/pyspark/serializers.py\", line 470, in dumps\r\n    raise pickle.PicklingError(msg)\r\n_pickle.PicklingError: Could not serialize object: TypeError: cannot pickle '_abc._abc_data' object\r\n\r\n----------------------------------------------------------------------\r\n{code}", "comments": [], "derived_tasks": {"summarization": "Fix `test_in_memory_data_source` in Python 3.14", "classification": "feature", "qna": {"question": "What is the issue 'Fix `test_in_memory_data_source` in Python 3.14' about?", "answer": "{code}\r\n======================================================================\r\nERROR [0.007s]: test_in_memory_data_source (pyspark.sql.tests.test_python_datasource.PythonDataSourceTests.test_in_memory_data_source)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/__w/spark/spark/python/pyspark/serializers.py\", line 460, in dumps\r\n    return cloudpickle.dumps(obj, pickle_protocol)\r\n           ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/__w/spark/spark/python/pyspark/cloudpickle/cloudpickle.py\", line 1537, in dumps\r\n    cp.dump(obj)\r\n    ~~~~~~~^^^^^\r\n  File \"/__w/spark/spark/python/pyspark/cloudpickle/cloudpickle.py\", line 1303, in dump\r\n    return super().dump(obj)\r\n           ~~~~~~~~~~~~^^^^^\r\nTypeError: cannot pickle '_abc._abc_data' object\r\nwhen serializing dict item '_abc_impl'\r\nwhen serializing tuple item 0\r\nwhen serializing cell reconstructor arguments\r\nwhen serializing cell object\r\nwhen serializing tuple item 0\r\nwhen serializing dict item '__closure__'\r\nwhen serializing tuple item 1\r\nwhen serializing function state\r\nwhen serializing function object\r\nwhen serializing dict item '__annotate_func__'\r\nwhen serializing tuple item 0\r\nwhen serializing abc.ABCMeta state\r\nwhen serializing abc.ABCMeta object\r\nwhen serializing tuple item 0\r\nwhen serializing cell reconstructor arguments\r\nwhen serializing cell object\r\nwhen serializing tuple item 0\r\nwhen serializing dict item '__closure__'\r\nwhen serializing tuple item 1\r\nwhen serializing function state\r\nwhen serializing function object\r\nwhen serializing dict item 'reader'\r\nwhen serializing tuple item 0\r\nwhen serializing abc.ABCMeta state\r\nwhen serializing abc.ABCMeta object\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/__w/spark/spark/python/pyspark/sql/tests/test_python_datasource.py\", line 283, in test_in_memory_data_source\r\n    self.spark.dataSource.register(InMemoryDataSource)\r\n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^\r\n  File \"/__w/spark/spark/python/pyspark/sql/datasource.py\", line 1197, in register\r\n    wrapped = _wrap_function(sc, dataSource)\r\n  File \"/__w/spark/spark/python/pyspark/sql/udf.py\", line 59, in _wrap_function\r\n    pickled_command, broadcast_vars, env, includes = _prepare_for_python_RDD(sc, command)\r\n                                                     ~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^\r\n  File \"/__w/spark/spark/python/pyspark/core/rdd.py\", line 5121, in _prepare_for_python_RDD\r\n    pickled_command = ser.dumps(command)\r\n  File \"/__w/spark/spark/python/pyspark/serializers.py\", line 470, in dumps\r\n    raise pickle.PicklingError(msg)\r\n_pickle.PicklingError: Could not serialize object: TypeError: cannot pickle '_abc._abc_data' object\r\n{code}\r\n\r\n{code}\r\n======================================================================\r\nERROR [0.014s]: test_in_memory_data_source (pyspark.sql.tests.connect.test_parity_python_datasource.PythonDataSourceParityTests.test_in_memory_data_source)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/__w/spark/spark/python/pyspark/serializers.py\", line 460, in dumps\r\n    return cloudpickle.dumps(obj, pickle_protocol)\r\n           ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/__w/spark/spark/python/pyspark/cloudpickle/cloudpickle.py\", line 1537, in dumps\r\n    cp.dump(obj)\r\n    ~~~~~~~^^^^^\r\n  File \"/__w/spark/spark/python/pyspark/cloudpickle/cloudpickle.py\", line 1303, in dump\r\n    return super().dump(obj)\r\n           ~~~~~~~~~~~~^^^^^\r\nTypeError: cannot pickle '_abc._abc_data' object\r\nwhen serializing dict item '_abc_impl'\r\nwhen serializing tuple item 0\r\nwhen serializing cell reconstructor arguments\r\nwhen serializing cell object\r\nwhen serializing tuple item 0\r\nwhen serializing dict item '__closure__'\r\nwhen serializing tuple item 1\r\nwhen serializing function state\r\nwhen serializing function object\r\nwhen serializing dict item '__annotate_func__'\r\nwhen serializing tuple item 0\r\nwhen serializing abc.ABCMeta state\r\nwhen serializing abc.ABCMeta object\r\nwhen serializing tuple item 0\r\nwhen serializing cell reconstructor arguments\r\nwhen serializing cell object\r\nwhen serializing tuple item 0\r\nwhen serializing dict item '__closure__'\r\nwhen serializing tuple item 1\r\nwhen serializing function state\r\nwhen serializing function object\r\nwhen serializing dict item 'reader'\r\nwhen serializing tuple item 0\r\nwhen serializing abc.ABCMeta state\r\nwhen serializing abc.ABCMeta object\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/__w/spark/spark/python/pyspark/sql/tests/test_python_datasource.py\", line 283, in test_in_memory_data_source\r\n    self.spark.dataSource.register(InMemoryDataSource)\r\n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^\r\n  File \"/__w/spark/spark/python/pyspark/sql/connect/datasource.py\", line 45, in register\r\n    self.sparkSession._client.register_data_source(dataSource)\r\n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^\r\n  File \"/__w/spark/spark/python/pyspark/sql/connect/client/core.py\", line 863, in register_data_source\r\n    ).to_data_source_proto(self)\r\n      ~~~~~~~~~~~~~~~~~~~~^^^^^^\r\n  File \"/__w/spark/spark/python/pyspark/sql/connect/plan.py\", line 2833, in to_data_source_proto\r\n    plan.python_data_source.CopyFrom(self._data_source.to_plan(session))\r\n                                     ~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^\r\n  File \"/__w/spark/spark/python/pyspark/sql/connect/plan.py\", line 2807, in to_plan\r\n    ds.command = CloudPickleSerializer().dumps(self._data_source)\r\n                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^\r\n  File \"/__w/spark/spark/python/pyspark/serializers.py\", line 470, in dumps\r\n    raise pickle.PicklingError(msg)\r\n_pickle.PicklingError: Could not serialize object: TypeError: cannot pickle '_abc._abc_data' object\r\n\r\n----------------------------------------------------------------------\r\n{code}"}}}
{"project": "SPARK", "issue_key": "SPARK-54064", "title": "Simplify recursion detection of `cloudpickle`", "status": "Closed", "reporter": "Dongjoon Hyun", "assignee": null, "priority": "Major", "labels": ["pull-request-available"], "created": "2025-10-28T18:58:28.000+0000", "updated": "2025-10-29T03:29:24.000+0000", "description": null, "comments": [], "derived_tasks": {"summarization": "Simplify recursion detection of `cloudpickle`", "classification": "feature", "qna": {"question": "What is the issue 'Simplify recursion detection of `cloudpickle`' about?", "answer": null}}}
{"project": "SPARK", "issue_key": "SPARK-54063", "title": "Trigger snapshot generation for next batch when lag is detected", "status": "Open", "reporter": "Zifei Feng", "assignee": null, "priority": "Major", "labels": ["pull-request-available"], "created": "2025-10-28T18:32:08.000+0000", "updated": "2025-10-31T20:44:51.000+0000", "description": "We have done work to detect snapshot lag. Now we need to trigger snapshot generation for the next microbatch, when snapshot lag is detected.", "comments": [], "derived_tasks": {"summarization": "Trigger snapshot generation for next batch when lag is detected", "classification": "feature", "qna": {"question": "What is the issue 'Trigger snapshot generation for next batch when lag is detected' about?", "answer": "We have done work to detect snapshot lag. Now we need to trigger snapshot generation for the next microbatch, when snapshot lag is detected."}}}
{"project": "SPARK", "issue_key": "SPARK-54062", "title": "MergeScalarSubqueries code cleanup", "status": "Resolved", "reporter": "Peter Toth", "assignee": "Peter Toth", "priority": "Minor", "labels": ["pull-request-available"], "created": "2025-10-28T17:47:04.000+0000", "updated": "2025-10-29T17:56:04.000+0000", "description": null, "comments": [], "derived_tasks": {"summarization": "MergeScalarSubqueries code cleanup", "classification": "feature", "qna": {"question": "What is the issue 'MergeScalarSubqueries code cleanup' about?", "answer": null}}}
{"project": "SPARK", "issue_key": "SPARK-54061", "title": "Wrap IllegalArgumentException with proper error code for invalid datetime patterns", "status": "Resolved", "reporter": "Milan Dankovic", "assignee": "Milan Dankovic", "priority": "Major", "labels": ["pull-request-available"], "created": "2025-10-28T17:28:02.000+0000", "updated": "2025-10-30T05:14:34.000+0000", "description": "When `TO_TIMESTAMP` is called with an invalid datetime pattern (e.g., `'yyyyMMddHHMIss'` with invalid letter `'I'`), the `IllegalArgumentException` from Java's `DateTimeFormatterBuilder` escapes during constant folding optimization, resulting in a raw exception without proper error codes or `SQLSTATE`.", "comments": [], "derived_tasks": {"summarization": "Wrap IllegalArgumentException with proper error code for invalid datetime patterns", "classification": "feature", "qna": {"question": "What is the issue 'Wrap IllegalArgumentException with proper error code for invalid datetime patterns' about?", "answer": "When `TO_TIMESTAMP` is called with an invalid datetime pattern (e.g., `'yyyyMMddHHMIss'` with invalid letter `'I'`), the `IllegalArgumentException` from Java's `DateTimeFormatterBuilder` escapes during constant folding optimization, resulting in a raw exception without proper error codes or `SQLSTATE`."}}}
{"project": "SPARK", "issue_key": "SPARK-54060", "title": "Introduce Geometry and Geography in-memory wrapper formats", "status": "Resolved", "reporter": "Uro\u0161 Bojani\u0107", "assignee": "Uro\u0161 Bojani\u0107", "priority": "Major", "labels": ["pull-request-available"], "created": "2025-10-28T16:50:42.000+0000", "updated": "2025-10-29T16:32:58.000+0000", "description": null, "comments": [], "derived_tasks": {"summarization": "Introduce Geometry and Geography in-memory wrapper formats", "classification": "feature", "qna": {"question": "What is the issue 'Introduce Geometry and Geography in-memory wrapper formats' about?", "answer": null}}}
{"project": "SPARK", "issue_key": "SPARK-54059", "title": "Reduce default page size by LONG_ARRAY_OFFSET if ZGC or ShenandoahGC and ON_HEAP are used", "status": "Resolved", "reporter": "Dongjoon Hyun", "assignee": "Wan Kun", "priority": "Major", "labels": ["pull-request-available"], "created": "2025-10-28T15:37:39.000+0000", "updated": "2025-10-29T09:51:00.000+0000", "description": null, "comments": [], "derived_tasks": {"summarization": "Reduce default page size by LONG_ARRAY_OFFSET if ZGC or ShenandoahGC and ON_HEAP are used", "classification": "feature", "qna": {"question": "What is the issue 'Reduce default page size by LONG_ARRAY_OFFSET if ZGC or ShenandoahGC and ON_HEAP are used' about?", "answer": null}}}
{"project": "SPARK", "issue_key": "SPARK-54058", "title": "Fix Flaky Test: `KafkaMicroBatchV1SourceWithConsumerSuite.Query with Trigger.AvailableNow should throw error when topic partitions got unavailable during subsequent batches`", "status": "Resolved", "reporter": "Kousuke Saruta", "assignee": "Kousuke Saruta", "priority": "Major", "labels": ["pull-request-available"], "created": "2025-10-28T14:01:43.000+0000", "updated": "2025-10-28T19:07:45.000+0000", "description": "Recently, `KafkaMicroBatchV1SourceWithConsumerSuite.Query with Trigger.AvailableNow should throw error when topic partitions got unavailable during subsequent batches` frequently fails.\r\n\r\n[https://github.com/apache/spark/actions/runs/18872699886/job/53854858890]\r\n\r\n\u00a0\r\n{code:java}\r\n[info] - Query with Trigger.AvailableNow should throw error when topic partitions got unavailable during subsequent batches *** FAILED *** (1 minute)\r\n[info]   java.lang.AssertionError: assertion failed: Exception tree doesn't contain the expected exception with message: Some of partitions in Kafka topic(s) have been lost during running query with Trigger.AvailableNow.\r\n[info] org.scalatest.exceptions.TestFailedException: isPropagated was false Partition [topic-41, 1] metadata not propagated after timeout\r\n[info] \tat org.scalatest.Assertions.newAssertionFailedException(Assertions.scala:472)\r\n[info] \tat org.scalatest.Assertions.newAssertionFailedException$(Assertions.scala:471)\r\n[info] \tat org.scalatest.Assertions$.newAssertionFailedException(Assertions.scala:1231)\r\n[info] \tat org.scalatest.Assertions$AssertionsHelper.macroAssert(Assertions.scala:1295)\r\n[info] \tat org.apache.spark.sql.kafka010.KafkaTestUtils.$anonfun$waitUntilMetadataIsPropagated$1(KafkaTestUtils.scala:614)\r\n[info] \tat org.scalatest.enablers.Retrying$$anon$4.makeAValiantAttempt$1(Retrying.scala:184)\r\n[info] \tat org.scalatest.enablers.Retrying$$anon$4.tryTryAgain$2(Retrying.scala:196)\r\n[info] \tat org.scalatest.enablers.Retrying$$anon$4.retry(Retrying.scala:226)\r\n[info] \tat org.scalatest.concurrent.Eventually.eventually(Eventually.scala:348)\r\n[info] \tat org.scalatest.concurrent.Eventually.eventually$(Eventually.scala:347)\r\n[info] \tat org.scalatest.concurrent.Eventually$.eventually(Eventually.scala:457)\r\n[info] \tat org.apache.spark.sql.kafka010.KafkaTestUtils.waitUntilMetadataIsPropagated(KafkaTestUtils.scala:613)\r\n[info] \tat org.apache.spark.sql.kafka010.KafkaTestUtils.$anonfun$createTopic$1(KafkaTestUtils.scala:378)\r\n[info] \tat scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:256)\r\n[info] \tat org.apache.spark.sql.kafka010.KafkaTestUtils.createTopic(KafkaTestUtils.scala:377)\r\n[info] \tat org.apache.spark.sql.kafka010.KafkaMicroBatchSourceSuiteBase.$anonfun$new$11(KafkaMicroBatchSourceSuite.scala:352)\r\n[info] \tat org.apache.spark.sql.kafka010.KafkaMicroBatchSourceSuiteBase.$anonfun$new$11$adapted(KafkaMicroBatchSourceSuite.scala:349)\r\n[info] \tat org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.callBatchWriter(ForeachBatchSink.scala:56)\r\n[info] \tat org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:49)\r\n[info] \tat org.apache.spark.sql.execution.streaming.runtime.MicroBatchExecution.$anonfun$runBatch$19(MicroBatchExecution.scala:1063)\r\n[info] \tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:176)\r\n[info] \tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:284)\r\n[info] \tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:138)\r\n[info] \tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\r\n[info] \tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\r\n[info] \tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)\r\n[info] \tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\r\n[info] \tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:138)\r\n[info] \tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:307)\r\n[info] \tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:137)\r\n[info] \tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\r\n[info] \tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:91)\r\n[info] \tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:249)\r\n[info] \tat org.apache.spark.sql.execution.streaming.runtime.MicroBatchExecution.$anonfun$runBatch$18(MicroBatchExecution.scala:1054)\r\n[info] \tat org.apache.spark.sql.execution.streaming.runtime.ProgressContext.reportTimeTaken(ProgressReporter.scala:200)\r\n[info] \tat org.apache.spark.sql.execution.streaming.runtime.MicroBatchExecution.runBatch(MicroBatchExecution.scala:1054)\r\n[info] \tat org.apache.spark.sql.execution.streaming.runtime.MicroBatchExecution.$anonfun$executeOneBatch$2(MicroBatchExecution.scala:513)\r\n[info] \tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\r\n[info] \tat org.apache.spark.sql.execution.streaming.runtime.ProgressContext.reportTimeTaken(ProgressReporter.scala:200)\r\n[info] \tat org.apache.spark.sql.execution.streaming.runtime.MicroBatchExecution.executeOneBatch(MicroBatchExecution.scala:478)\r\n[info] \tat org.apache.spark.sql.execution.streaming.runtime.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:458)\r\n[info] \tat org.apache.spark.sql.execution.streaming.runtime.MicroBatchExecution.$anonfun$runActivatedStream$1$adapted(MicroBatchExecution.scala:458)\r\n[info] \tat org.apache.spark.sql.execution.streaming.runtime.TriggerExecutor.runOneBatch(TriggerExecutor.scala:40)\r\n[info] \tat org.apache.spark.sql.execution.streaming.runtime.TriggerExecutor.runOneBatch$(TriggerExecutor.scala:38)\r\n[info] \tat org.apache.spark.sql.execution.streaming.runtime.MultiBatchExecutor.runOneBatch(TriggerExecutor.scala:60)\r\n[info] \tat org.apache.spark.sql.execution.streaming.runtime.MultiBatchExecutor.execute(TriggerExecutor.scala:65)\r\n[info] \tat org.apache.spark.sql.execution.streaming.runtime.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:458)\r\n[info] \tat org.apache.spark.sql.execution.streaming.runtime.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:347)\r\n[info] \tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\r\n[info] \tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\r\n[info] \tat org.apache.spark.sql.execution.streaming.runtime.StreamExecution.org$apache$spark$sql$execution$streaming$runtime$StreamExecution$$runStream(StreamExecution.scala:307)\r\n[info] \tat org.apache.spark.sql.execution.streaming.runtime.StreamExecution$$anon$1.run(StreamExecution.scala:230)\r\n[info]   at scala.Predef$.assert(Predef.scala:279)\r\n[info]   at org.apache.spark.TestUtils$.assertExceptionMsg(TestUtils.scala:198)\r\n[info]   at org.apache.spark.sql.kafka010.KafkaMicroBatchSourceSuiteBase.$anonfun$new$9(KafkaMicroBatchSourceSuite.scala:374)\r\n[info]   at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\r\n[info]   at org.scalatest.enablers.Timed$$anon$1.timeoutAfter(Timed.scala:127)\r\n[info]   at org.scalatest.concurrent.TimeLimits$.failAfterImpl(TimeLimits.scala:282)\r\n[info]   at org.scalatest.concurrent.TimeLimits.failAfter(TimeLimits.scala:231)\r\n[info]   at org.scalatest.concurrent.TimeLimits.failAfter$(TimeLimits.scala:230)\r\n[info]   at org.apache.spark.SparkFunSuite.failAfter(SparkFunSuite.scala:68)\r\n[info]   at org.apache.spark.SparkFunSuite.$anonfun$test$2(SparkFunSuite.scala:154)\r\n[info]   at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)\r\n[info]   at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)\r\n[info]   at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)\r\n[info]   at org.scalatest.Transformer.apply(Transformer.scala:22)\r\n[info]   at org.scalatest.Transformer.apply(Transformer.scala:20)\r\n[info]   at org.scalatest.funsuite.AnyFunSuiteLike$$anon$1.apply(AnyFunSuiteLike.scala:226)\r\n[info]   at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:226)\r\n[info]   at org.scalatest.funsuite.AnyFunSuiteLike.invokeWithFixture$1(AnyFunSuiteLike.scala:224)\r\n[info]   at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTest$1(AnyFunSuiteLike.scala:236)\r\n[info]   at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)\r\n[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTest(AnyFunSuiteLike.scala:236)\r\n[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTest$(AnyFunSuiteLike.scala:218)\r\n[info]   at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterEach$$super$runTest(SparkFunSuite.scala:68)\r\n[info]   at org.scalatest.BeforeAndAfterEach.runTest(BeforeAndAfterEach.scala:234)\r\n[info]   at org.scalatest.BeforeAndAfterEach.runTest$(BeforeAndAfterEach.scala:227)\r\n[info]   at org.apache.spark.SparkFunSuite.runTest(SparkFunSuite.scala:68)\r\n[info]   at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTests$1(AnyFunSuiteLike.scala:269)\r\n[info]   at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:413)\r\n[info]   at scala.collection.immutable.List.foreach(List.scala:323)\r\n[info]   at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)\r\n[info]   at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:396)\r\n[info]   at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:475)\r\n[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTests(AnyFunSuiteLike.scala:269)\r\n[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTests$(AnyFunSuiteLike.scala:268)\r\n[info]   at org.scalatest.funsuite.AnyFunSuite.runTests(AnyFunSuite.scala:1564)\r\n[info]   at org.scalatest.Suite.run(Suite.scala:1114)\r\n[info]   at org.scalatest.Suite.run$(Suite.scala:1096)\r\n[info]   at org.scalatest.funsuite.AnyFunSuite.org$scalatest$funsuite$AnyFunSuiteLike$$super$run(AnyFunSuite.scala:1564)\r\n[info]   at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$run$1(AnyFunSuiteLike.scala:273)\r\n[info]   at org.scalatest.SuperEngine.runImpl(Engine.scala:535)\r\n[info]   at org.scalatest.funsuite.AnyFunSuiteLike.run(AnyFunSuiteLike.scala:273)\r\n[info]   at org.scalatest.funsuite.AnyFunSuiteLike.run$(AnyFunSuiteLike.scala:272)\r\n[info]   at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterAll$$super$run(SparkFunSuite.scala:68)\r\n[info]   at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:213)\r\n[info]   at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)\r\n[info]   at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208)\r\n[info]   at org.apache.spark.SparkFunSuite.run(SparkFunSuite.scala:68)\r\n[info]   at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:321)\r\n[info]   at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:517)\r\n[info]   at sbt.ForkMain$Run.lambda$runTest$1(ForkMain.java:414)\r\n[info]   at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\r\n[info]   at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n[info]   at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n[info]   at java.base/java.lang.Thread.run(Thread.java:840) {code}", "comments": [], "derived_tasks": {"summarization": "Fix Flaky Test: `KafkaMicroBatchV1SourceWithConsumerSuite.Query with Trigger.AvailableNow should throw error when topic partitions got unavailable during subsequent batches`", "classification": "feature", "qna": {"question": "What is the issue 'Fix Flaky Test: `KafkaMicroBatchV1SourceWithConsumerSuite.Query with Trigger.AvailableNow should throw error when topic partitions got unavailable during subsequent batches`' about?", "answer": "Recently, `KafkaMicroBatchV1SourceWithConsumerSuite.Query with Trigger.AvailableNow should throw error when topic partitions got unavailable during subsequent batches` frequently fails.\r\n\r\n[https://github.com/apache/spark/actions/runs/18872699886/job/53854858890]\r\n\r\n\u00a0\r\n{code:java}\r\n[info] - Query with Trigger.AvailableNow should throw error when topic partitions got unavailable during subsequent batches *** FAILED *** (1 minute)\r\n[info]   java.lang.AssertionError: assertion failed: Exception tree doesn't contain the expected exception with message: Some of partitions in Kafka topic(s) have been lost during running query with Trigger.AvailableNow.\r\n[info] org.scalatest.exceptions.TestFailedException: isPropagated was false Partition [topic-41, 1] metadata not propagated after timeout\r\n[info] \tat org.scalatest.Assertions.newAssertionFailedException(Assertions.scala:472)\r\n[info] \tat org.scalatest.Assertions.newAssertionFailedException$(Assertions.scala:471)\r\n[info] \tat org.scalatest.Assertions$.newAssertionFailedException(Assertions.scala:1231)\r\n[info] \tat org.scalatest.Assertions$AssertionsHelper.macroAssert(Assertions.scala:1295)\r\n[info] \tat org.apache.spark.sql.kafka010.KafkaTestUtils.$anonfun$waitUntilMetadataIsPropagated$1(KafkaTestUtils.scala:614)\r\n[info] \tat org.scalatest.enablers.Retrying$$anon$4.makeAValiantAttempt$1(Retrying.scala:184)\r\n[info] \tat org.scalatest.enablers.Retrying$$anon$4.tryTryAgain$2(Retrying.scala:196)\r\n[info] \tat org.scalatest.enablers.Retrying$$anon$4.retry(Retrying.scala:226)\r\n[info] \tat org.scalatest.concurrent.Eventually.eventually(Eventually.scala:348)\r\n[info] \tat org.scalatest.concurrent.Eventually.eventually$(Eventually.scala:347)\r\n[info] \tat org.scalatest.concurrent.Eventually$.eventually(Eventually.scala:457)\r\n[info] \tat org.apache.spark.sql.kafka010.KafkaTestUtils.waitUntilMetadataIsPropagated(KafkaTestUtils.scala:613)\r\n[info] \tat org.apache.spark.sql.kafka010.KafkaTestUtils.$anonfun$createTopic$1(KafkaTestUtils.scala:378)\r\n[info] \tat scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:256)\r\n[info] \tat org.apache.spark.sql.kafka010.KafkaTestUtils.createTopic(KafkaTestUtils.scala:377)\r\n[info] \tat org.apache.spark.sql.kafka010.KafkaMicroBatchSourceSuiteBase.$anonfun$new$11(KafkaMicroBatchSourceSuite.scala:352)\r\n[info] \tat org.apache.spark.sql.kafka010.KafkaMicroBatchSourceSuiteBase.$anonfun$new$11$adapted(KafkaMicroBatchSourceSuite.scala:349)\r\n[info] \tat org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.callBatchWriter(ForeachBatchSink.scala:56)\r\n[info] \tat org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:49)\r\n[info] \tat org.apache.spark.sql.execution.streaming.runtime.MicroBatchExecution.$anonfun$runBatch$19(MicroBatchExecution.scala:1063)\r\n[info] \tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:176)\r\n[info] \tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:284)\r\n[info] \tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:138)\r\n[info] \tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\r\n[info] \tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\r\n[info] \tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)\r\n[info] \tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\r\n[info] \tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:138)\r\n[info] \tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:307)\r\n[info] \tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:137)\r\n[info] \tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\r\n[info] \tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:91)\r\n[info] \tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:249)\r\n[info] \tat org.apache.spark.sql.execution.streaming.runtime.MicroBatchExecution.$anonfun$runBatch$18(MicroBatchExecution.scala:1054)\r\n[info] \tat org.apache.spark.sql.execution.streaming.runtime.ProgressContext.reportTimeTaken(ProgressReporter.scala:200)\r\n[info] \tat org.apache.spark.sql.execution.streaming.runtime.MicroBatchExecution.runBatch(MicroBatchExecution.scala:1054)\r\n[info] \tat org.apache.spark.sql.execution.streaming.runtime.MicroBatchExecution.$anonfun$executeOneBatch$2(MicroBatchExecution.scala:513)\r\n[info] \tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\r\n[info] \tat org.apache.spark.sql.execution.streaming.runtime.ProgressContext.reportTimeTaken(ProgressReporter.scala:200)\r\n[info] \tat org.apache.spark.sql.execution.streaming.runtime.MicroBatchExecution.executeOneBatch(MicroBatchExecution.scala:478)\r\n[info] \tat org.apache.spark.sql.execution.streaming.runtime.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:458)\r\n[info] \tat org.apache.spark.sql.execution.streaming.runtime.MicroBatchExecution.$anonfun$runActivatedStream$1$adapted(MicroBatchExecution.scala:458)\r\n[info] \tat org.apache.spark.sql.execution.streaming.runtime.TriggerExecutor.runOneBatch(TriggerExecutor.scala:40)\r\n[info] \tat org.apache.spark.sql.execution.streaming.runtime.TriggerExecutor.runOneBatch$(TriggerExecutor.scala:38)\r\n[info] \tat org.apache.spark.sql.execution.streaming.runtime.MultiBatchExecutor.runOneBatch(TriggerExecutor.scala:60)\r\n[info] \tat org.apache.spark.sql.execution.streaming.runtime.MultiBatchExecutor.execute(TriggerExecutor.scala:65)\r\n[info] \tat org.apache.spark.sql.execution.streaming.runtime.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:458)\r\n[info] \tat org.apache.spark.sql.execution.streaming.runtime.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:347)\r\n[info] \tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\r\n[info] \tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\r\n[info] \tat org.apache.spark.sql.execution.streaming.runtime.StreamExecution.org$apache$spark$sql$execution$streaming$runtime$StreamExecution$$runStream(StreamExecution.scala:307)\r\n[info] \tat org.apache.spark.sql.execution.streaming.runtime.StreamExecution$$anon$1.run(StreamExecution.scala:230)\r\n[info]   at scala.Predef$.assert(Predef.scala:279)\r\n[info]   at org.apache.spark.TestUtils$.assertExceptionMsg(TestUtils.scala:198)\r\n[info]   at org.apache.spark.sql.kafka010.KafkaMicroBatchSourceSuiteBase.$anonfun$new$9(KafkaMicroBatchSourceSuite.scala:374)\r\n[info]   at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\r\n[info]   at org.scalatest.enablers.Timed$$anon$1.timeoutAfter(Timed.scala:127)\r\n[info]   at org.scalatest.concurrent.TimeLimits$.failAfterImpl(TimeLimits.scala:282)\r\n[info]   at org.scalatest.concurrent.TimeLimits.failAfter(TimeLimits.scala:231)\r\n[info]   at org.scalatest.concurrent.TimeLimits.failAfter$(TimeLimits.scala:230)\r\n[info]   at org.apache.spark.SparkFunSuite.failAfter(SparkFunSuite.scala:68)\r\n[info]   at org.apache.spark.SparkFunSuite.$anonfun$test$2(SparkFunSuite.scala:154)\r\n[info]   at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)\r\n[info]   at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)\r\n[info]   at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)\r\n[info]   at org.scalatest.Transformer.apply(Transformer.scala:22)\r\n[info]   at org.scalatest.Transformer.apply(Transformer.scala:20)\r\n[info]   at org.scalatest.funsuite.AnyFunSuiteLike$$anon$1.apply(AnyFunSuiteLike.scala:226)\r\n[info]   at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:226)\r\n[info]   at org.scalatest.funsuite.AnyFunSuiteLike.invokeWithFixture$1(AnyFunSuiteLike.scala:224)\r\n[info]   at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTest$1(AnyFunSuiteLike.scala:236)\r\n[info]   at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)\r\n[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTest(AnyFunSuiteLike.scala:236)\r\n[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTest$(AnyFunSuiteLike.scala:218)\r\n[info]   at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterEach$$super$runTest(SparkFunSuite.scala:68)\r\n[info]   at org.scalatest.BeforeAndAfterEach.runTest(BeforeAndAfterEach.scala:234)\r\n[info]   at org.scalatest.BeforeAndAfterEach.runTest$(BeforeAndAfterEach.scala:227)\r\n[info]   at org.apache.spark.SparkFunSuite.runTest(SparkFunSuite.scala:68)\r\n[info]   at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTests$1(AnyFunSuiteLike.scala:269)\r\n[info]   at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:413)\r\n[info]   at scala.collection.immutable.List.foreach(List.scala:323)\r\n[info]   at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)\r\n[info]   at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:396)\r\n[info]   at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:475)\r\n[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTests(AnyFunSuiteLike.scala:269)\r\n[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTests$(AnyFunSuiteLike.scala:268)\r\n[info]   at org.scalatest.funsuite.AnyFunSuite.runTests(AnyFunSuite.scala:1564)\r\n[info]   at org.scalatest.Suite.run(Suite.scala:1114)\r\n[info]   at org.scalatest.Suite.run$(Suite.scala:1096)\r\n[info]   at org.scalatest.funsuite.AnyFunSuite.org$scalatest$funsuite$AnyFunSuiteLike$$super$run(AnyFunSuite.scala:1564)\r\n[info]   at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$run$1(AnyFunSuiteLike.scala:273)\r\n[info]   at org.scalatest.SuperEngine.runImpl(Engine.scala:535)\r\n[info]   at org.scalatest.funsuite.AnyFunSuiteLike.run(AnyFunSuiteLike.scala:273)\r\n[info]   at org.scalatest.funsuite.AnyFunSuiteLike.run$(AnyFunSuiteLike.scala:272)\r\n[info]   at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterAll$$super$run(SparkFunSuite.scala:68)\r\n[info]   at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:213)\r\n[info]   at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)\r\n[info]   at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208)\r\n[info]   at org.apache.spark.SparkFunSuite.run(SparkFunSuite.scala:68)\r\n[info]   at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:321)\r\n[info]   at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:517)\r\n[info]   at sbt.ForkMain$Run.lambda$runTest$1(ForkMain.java:414)\r\n[info]   at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\r\n[info]   at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n[info]   at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n[info]   at java.base/java.lang.Thread.run(Thread.java:840) {code}"}}}
{"project": "SPARK", "issue_key": "SPARK-54057", "title": "Retrieve dependency version from pom.xml for DependencyOverrides in SparkBuild.scala", "status": "Resolved", "reporter": "Kousuke Saruta", "assignee": "Kousuke Saruta", "priority": "Major", "labels": ["pull-request-available"], "created": "2025-10-28T12:10:10.000+0000", "updated": "2025-10-28T15:33:55.000+0000", "description": "Currently, version strings for some dependencies are hard coded in DependencyOverrides in SparkBuild, so developers need to keep consistent with version strings specified in pom.xml manually.", "comments": [], "derived_tasks": {"summarization": "Retrieve dependency version from pom.xml for DependencyOverrides in SparkBuild.scala", "classification": "feature", "qna": {"question": "What is the issue 'Retrieve dependency version from pom.xml for DependencyOverrides in SparkBuild.scala' about?", "answer": "Currently, version strings for some dependencies are hard coded in DependencyOverrides in SparkBuild, so developers need to keep consistent with version strings specified in pom.xml manually."}}}
{"project": "SPARK", "issue_key": "SPARK-54056", "title": "Enable substitution for SQLConf settings", "status": "Open", "reporter": "eugen yushin", "assignee": null, "priority": "Major", "labels": ["pull-request-available"], "created": "2025-10-28T11:13:47.000+0000", "updated": "2025-10-31T19:04:09.000+0000", "description": "Values set for custom catalogs are not being substituted:\r\n\r\n{code:java}\r\nspark.sql.catalog.mssql=org.apache.spark.sql.execution.datasources.v2.jdbc.JDBCTableCatalog\r\nspark.sql.catalog.mssql.user=${env:MSSQL__USER}\r\n{code}\r\n\r\n\r\nFails with:\r\n{code:java}\r\nspark.sql(\"show tables in mssql\").show()\r\n\r\ncom.microsoft.sqlserver.jdbc.SQLServerException: Login failed for user '${env:MSSQL__USER}'\r\n{code}\r\n\r\n", "comments": [], "derived_tasks": {"summarization": "Enable substitution for SQLConf settings", "classification": "feature", "qna": {"question": "What is the issue 'Enable substitution for SQLConf settings' about?", "answer": "Values set for custom catalogs are not being substituted:\r\n\r\n{code:java}\r\nspark.sql.catalog.mssql=org.apache.spark.sql.execution.datasources.v2.jdbc.JDBCTableCatalog\r\nspark.sql.catalog.mssql.user=${env:MSSQL__USER}\r\n{code}\r\n\r\n\r\nFails with:\r\n{code:java}\r\nspark.sql(\"show tables in mssql\").show()\r\n\r\ncom.microsoft.sqlserver.jdbc.SQLServerException: Login failed for user '${env:MSSQL__USER}'\r\n{code}\r\n\r\n"}}}
{"project": "SPARK", "issue_key": "SPARK-54055", "title": "Spark Connect sessions leak pyspark UDF daemon processes and threads", "status": "Open", "reporter": "Peter Andrew", "assignee": null, "priority": "Major", "labels": [], "created": "2025-10-28T09:52:58.000+0000", "updated": "2025-10-28T16:23:06.000+0000", "description": "Each Spark Connect session that uses Python UDFs seems to leak one PySpark `daemon` process. Over time, these can accumulate in the 100s until the corresponding node or container goes OOM (although it can take a while as each process doesn't use a lot of memory and some of those pages are shared).\r\n{code:java}\r\nspark \u00a0 \u00a0 \u00a0 \u00a0263 \u00a00.0 \u00a00.0 121424 59504 ? \u00a0 \u00a0 \u00a0 \u00a0S \u00a0 \u00a005:00 \u00a0 0:01 \u00a0\\_ /opt/spark/.venv/bin/python3 -m pyspark.daemon\r\nspark \u00a0 \u00a0 \u00a0 1515 \u00a00.0 \u00a00.0 121324 60148 ? \u00a0 \u00a0 \u00a0 \u00a0S \u00a0 \u00a005:04 \u00a0 0:01 \u00a0\\_ /opt/spark/.venv/bin/python3 -m pyspark.daemon\r\nspark \u00a0 \u00a0 \u00a0 1525 \u00a00.0 \u00a00.0 121324 60400 ? \u00a0 \u00a0 \u00a0 \u00a0S \u00a0 \u00a005:04 \u00a0 0:01 \u00a0\\_ /opt/spark/.venv/bin/python3 -m pyspark.daemon\r\nspark \u00a0 \u00a0 \u00a0 1568 \u00a00.0 \u00a00.0 121324 60280 ? \u00a0 \u00a0 \u00a0 \u00a0S \u00a0 \u00a005:04 \u00a0 0:01 \u00a0\\_ /opt/spark/.venv/bin/python3 -m pyspark.daemon{code}\r\nIn addition there are also threads leaking - e.g., here is a thread dump histogram from a sample executor with 200+ leaked daemon processes:\r\n{code:java}\r\n# These threads seem to be leaking\r\n226 threads   Idle Worker Monitor for /opt/spark/.venv/bin/python3\r\n226 threads \u00a0 process reaper\r\n226 threads \u00a0 stderr reader for /opt/spark/.venv/bin/python3\r\n226 threads   stdout reader for /opt/spark/.venv/bin/python3\r\n250 threads \u00a0 Worker Monitor for /opt/spark/.venv/bin/python3\r\n\r\n# These threads seem fine, Spark is configured with 24 cores/executor\r\n21 threads \u00a0  stdout writer for /opt/spark/.venv/bin/python3\r\n21 threads \u00a0  Writer Monitor for /opt/spark/.venv/bin/python3{code}\r\nWith Spark Connect, each session always has a `SPARK_JOB_ARTIFACT_UUID`, even if there are no artifacts, so the UDF environment built by `BasePythonRunner.compute` is always different, and each session ends up with its own `PythonWorkerFactory` and hence its own daemon process.\r\n\r\n`PythonWorkerFactory` has a `stop` method that stops the daemon, but there does not seem to be anyone that calls `PythonWorkerFactory.stop`, except at shutdown in `SparkEnv.stop`.\r\n\r\n\u00a0\r\n\r\nThis can be reproduced by running a bunch of Spark Connect sessions:\r\n{code:java}\r\nparallel -n0 .venv/bin/python3 dummy.py ::: {1..200} {code}\r\nwith `dummy.py`:\r\n{code:java}\r\nfrom collections.abc import Iterable\r\n\r\nimport pandas as pd\r\nfrom pyspark.sql import SparkSession\r\n\r\n\r\ndef _udf(iterator: Iterable[pd.DataFrame]) -> Iterable[pd.DataFrame]:\r\n    yield from iterator\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    spark = SparkSession.builder.remote(\"...\").getOrCreate()\r\n\r\n    df = spark.range(128)\r\n    df.mapInPandas(_udf, df.schema).count()\r\n {code}\r\n\u00a0", "comments": [], "derived_tasks": {"summarization": "Spark Connect sessions leak pyspark UDF daemon processes and threads", "classification": "feature", "qna": {"question": "What is the issue 'Spark Connect sessions leak pyspark UDF daemon processes and threads' about?", "answer": "Each Spark Connect session that uses Python UDFs seems to leak one PySpark `daemon` process. Over time, these can accumulate in the 100s until the corresponding node or container goes OOM (although it can take a while as each process doesn't use a lot of memory and some of those pages are shared).\r\n{code:java}\r\nspark \u00a0 \u00a0 \u00a0 \u00a0263 \u00a00.0 \u00a00.0 121424 59504 ? \u00a0 \u00a0 \u00a0 \u00a0S \u00a0 \u00a005:00 \u00a0 0:01 \u00a0\\_ /opt/spark/.venv/bin/python3 -m pyspark.daemon\r\nspark \u00a0 \u00a0 \u00a0 1515 \u00a00.0 \u00a00.0 121324 60148 ? \u00a0 \u00a0 \u00a0 \u00a0S \u00a0 \u00a005:04 \u00a0 0:01 \u00a0\\_ /opt/spark/.venv/bin/python3 -m pyspark.daemon\r\nspark \u00a0 \u00a0 \u00a0 1525 \u00a00.0 \u00a00.0 121324 60400 ? \u00a0 \u00a0 \u00a0 \u00a0S \u00a0 \u00a005:04 \u00a0 0:01 \u00a0\\_ /opt/spark/.venv/bin/python3 -m pyspark.daemon\r\nspark \u00a0 \u00a0 \u00a0 1568 \u00a00.0 \u00a00.0 121324 60280 ? \u00a0 \u00a0 \u00a0 \u00a0S \u00a0 \u00a005:04 \u00a0 0:01 \u00a0\\_ /opt/spark/.venv/bin/python3 -m pyspark.daemon{code}\r\nIn addition there are also threads leaking - e.g., here is a thread dump histogram from a sample executor with 200+ leaked daemon processes:\r\n{code:java}\r\n# These threads seem to be leaking\r\n226 threads   Idle Worker Monitor for /opt/spark/.venv/bin/python3\r\n226 threads \u00a0 process reaper\r\n226 threads \u00a0 stderr reader for /opt/spark/.venv/bin/python3\r\n226 threads   stdout reader for /opt/spark/.venv/bin/python3\r\n250 threads \u00a0 Worker Monitor for /opt/spark/.venv/bin/python3\r\n\r\n# These threads seem fine, Spark is configured with 24 cores/executor\r\n21 threads \u00a0  stdout writer for /opt/spark/.venv/bin/python3\r\n21 threads \u00a0  Writer Monitor for /opt/spark/.venv/bin/python3{code}\r\nWith Spark Connect, each session always has a `SPARK_JOB_ARTIFACT_UUID`, even if there are no artifacts, so the UDF environment built by `BasePythonRunner.compute` is always different, and each session ends up with its own `PythonWorkerFactory` and hence its own daemon process.\r\n\r\n`PythonWorkerFactory` has a `stop` method that stops the daemon, but there does not seem to be anyone that calls `PythonWorkerFactory.stop`, except at shutdown in `SparkEnv.stop`.\r\n\r\n\u00a0\r\n\r\nThis can be reproduced by running a bunch of Spark Connect sessions:\r\n{code:java}\r\nparallel -n0 .venv/bin/python3 dummy.py ::: {1..200} {code}\r\nwith `dummy.py`:\r\n{code:java}\r\nfrom collections.abc import Iterable\r\n\r\nimport pandas as pd\r\nfrom pyspark.sql import SparkSession\r\n\r\n\r\ndef _udf(iterator: Iterable[pd.DataFrame]) -> Iterable[pd.DataFrame]:\r\n    yield from iterator\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    spark = SparkSession.builder.remote(\"...\").getOrCreate()\r\n\r\n    df = spark.range(128)\r\n    df.mapInPandas(_udf, df.schema).count()\r\n {code}\r\n\u00a0"}}}
{"project": "SPARK", "issue_key": "SPARK-54054", "title": "Support row position for SparkConnectResultSet", "status": "Resolved", "reporter": "Cheng Pan", "assignee": "Cheng Pan", "priority": "Major", "labels": ["pull-request-available"], "created": "2025-10-28T05:28:22.000+0000", "updated": "2025-10-29T06:50:21.000+0000", "description": null, "comments": [], "derived_tasks": {"summarization": "Support row position for SparkConnectResultSet", "classification": "feature", "qna": {"question": "What is the issue 'Support row position for SparkConnectResultSet' about?", "answer": null}}}
{"project": "SPARK", "issue_key": "SPARK-54053", "title": "Use Swift 6.2 for all Linux CIs", "status": "Resolved", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "priority": "Major", "labels": ["pull-request-available"], "created": "2025-10-28T05:00:55.000+0000", "updated": "2025-10-28T14:47:51.000+0000", "description": null, "comments": [], "derived_tasks": {"summarization": "Use Swift 6.2 for all Linux CIs", "classification": "feature", "qna": {"question": "What is the issue 'Use Swift 6.2 for all Linux CIs' about?", "answer": null}}}
{"project": "SPARK", "issue_key": "SPARK-54052", "title": "Add SparkThrowable wrapper to workaround Py4J limitation", "status": "Open", "reporter": "Hyukjin Kwon", "assignee": null, "priority": "Major", "labels": ["pull-request-available"], "created": "2025-10-28T03:42:55.000+0000", "updated": "2025-10-30T20:51:33.000+0000", "description": "We could have a wrapper class to work around the problem in https://issues.apache.org/jira/browse/SPARK-52022", "comments": [], "derived_tasks": {"summarization": "Add SparkThrowable wrapper to workaround Py4J limitation", "classification": "feature", "qna": {"question": "What is the issue 'Add SparkThrowable wrapper to workaround Py4J limitation' about?", "answer": "We could have a wrapper class to work around the problem in https://issues.apache.org/jira/browse/SPARK-52022"}}}
{"project": "SPARK", "issue_key": "SPARK-54051", "title": "Keep coverage data when running pip tests", "status": "Resolved", "reporter": "Ruifeng Zheng", "assignee": null, "priority": "Major", "labels": ["pull-request-available"], "created": "2025-10-28T03:38:34.000+0000", "updated": "2025-10-29T04:55:07.000+0000", "description": null, "comments": [], "derived_tasks": {"summarization": "Keep coverage data when running pip tests", "classification": "feature", "qna": {"question": "What is the issue 'Keep coverage data when running pip tests' about?", "answer": null}}}
{"project": "SPARK", "issue_key": "SPARK-54050", "title": "Update the documents of arrow-batching related configures", "status": "Resolved", "reporter": "Ruifeng Zheng", "assignee": "Ruifeng Zheng", "priority": "Major", "labels": ["pull-request-available"], "created": "2025-10-28T03:02:22.000+0000", "updated": "2025-10-28T04:58:38.000+0000", "description": null, "comments": [], "derived_tasks": {"summarization": "Update the documents of arrow-batching related configures", "classification": "feature", "qna": {"question": "What is the issue 'Update the documents of arrow-batching related configures' about?", "answer": null}}}
{"project": "SPARK", "issue_key": "SPARK-54049", "title": "spark-network-common no longer shades all of Guava", "status": "Open", "reporter": "Yosef Fertel", "assignee": null, "priority": "Minor", "labels": ["pull-request-available"], "created": "2025-10-28T01:04:16.000+0000", "updated": "2025-10-30T05:05:26.000+0000", "description": "Shading of Guava in `spark-network-common` has changed from 3.x to 4.x such that conflicting classes prevent downstream code from using Guava. \r\n\r\nIn spark jars prior to 4.x, `com/google/thirdparty/publicsuffix/PublicSuffixPatterns.class` was not a class included in the jar. Post 4.x, Guava is still shaded to `sparkproject` but `com/google/thirdparty/publicsuffix/PublicSuffixPatterns.class`, used in Guava's `InternetDomainName` and elsewhere, is now present in the jar. \r\n{code:java}\r\n$ jar tf spark-network-common_2.13-4.0.0.jar | rg PublicSuffixPatterns \r\ncom/google/thirdparty/publicsuffix/PublicSuffixPatterns.class {code}\r\nIf a library depends on spark but also depends on Guava, calls to `InternetDomainName` that call `PublicSuffixPatterns` will fail with `Exception in thread \"main\" java.lang.NoSuchFieldError: EXACT`. \r\n\r\nInspecting the code locations in such a library via `classOf[InternetDomainName].getProtectionDomain.getCodeSource.getLocation` and `classOf[PublicSuffixPatterns].getProtectionDomain.getCodeSource.getLocation` reveals that `InternetDomainName` is sourced from Guava, `target/bg-jobs/sbt_2062a9c3/target/5fcb43b5/1685140132000/guava-32.0.0-jre.jar`, while `PublicSuffixPatterns` is sourced instead from spark jar, `target/bg-jobs/sbt_2062a9c3/target/db746978/1747651686000/spark-network-common_2.13-4.0.0.jar`.", "comments": [], "derived_tasks": {"summarization": "spark-network-common no longer shades all of Guava", "classification": "feature", "qna": {"question": "What is the issue 'spark-network-common no longer shades all of Guava' about?", "answer": "Shading of Guava in `spark-network-common` has changed from 3.x to 4.x such that conflicting classes prevent downstream code from using Guava. \r\n\r\nIn spark jars prior to 4.x, `com/google/thirdparty/publicsuffix/PublicSuffixPatterns.class` was not a class included in the jar. Post 4.x, Guava is still shaded to `sparkproject` but `com/google/thirdparty/publicsuffix/PublicSuffixPatterns.class`, used in Guava's `InternetDomainName` and elsewhere, is now present in the jar. \r\n{code:java}\r\n$ jar tf spark-network-common_2.13-4.0.0.jar | rg PublicSuffixPatterns \r\ncom/google/thirdparty/publicsuffix/PublicSuffixPatterns.class {code}\r\nIf a library depends on spark but also depends on Guava, calls to `InternetDomainName` that call `PublicSuffixPatterns` will fail with `Exception in thread \"main\" java.lang.NoSuchFieldError: EXACT`. \r\n\r\nInspecting the code locations in such a library via `classOf[InternetDomainName].getProtectionDomain.getCodeSource.getLocation` and `classOf[PublicSuffixPatterns].getProtectionDomain.getCodeSource.getLocation` reveals that `InternetDomainName` is sourced from Guava, `target/bg-jobs/sbt_2062a9c3/target/5fcb43b5/1685140132000/guava-32.0.0-jre.jar`, while `PublicSuffixPatterns` is sourced instead from spark jar, `target/bg-jobs/sbt_2062a9c3/target/db746978/1747651686000/spark-network-common_2.13-4.0.0.jar`."}}}
{"project": "SPARK", "issue_key": "SPARK-54048", "title": "Update `dev/requirements.txt` to install `torch/torchvision` in Python 3.14", "status": "Resolved", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "priority": "Major", "labels": ["pull-request-available"], "created": "2025-10-27T23:55:25.000+0000", "updated": "2025-10-28T01:47:01.000+0000", "description": null, "comments": [], "derived_tasks": {"summarization": "Update `dev/requirements.txt` to install `torch/torchvision` in Python 3.14", "classification": "feature", "qna": {"question": "What is the issue 'Update `dev/requirements.txt` to install `torch/torchvision` in Python 3.14' about?", "answer": null}}}
{"project": "SPARK", "issue_key": "SPARK-54047", "title": "Use difference error message when kill on idle timeout", "status": "Resolved", "reporter": "Takuya Ueshin", "assignee": "Takuya Ueshin", "priority": "Major", "labels": ["pull-request-available"], "created": "2025-10-27T23:19:35.000+0000", "updated": "2025-10-28T23:48:47.000+0000", "description": null, "comments": [], "derived_tasks": {"summarization": "Use difference error message when kill on idle timeout", "classification": "feature", "qna": {"question": "What is the issue 'Use difference error message when kill on idle timeout' about?", "answer": null}}}
{"project": "SPARK", "issue_key": "SPARK-54046", "title": "Upgrade PyArrow to 22.0.0", "status": "In Progress", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "priority": "Major", "labels": ["pull-request-available"], "created": "2025-10-27T22:45:17.000+0000", "updated": "2025-10-30T05:09:54.000+0000", "description": null, "comments": [], "derived_tasks": {"summarization": "Upgrade PyArrow to 22.0.0", "classification": "feature", "qna": {"question": "What is the issue 'Upgrade PyArrow to 22.0.0' about?", "answer": null}}}
{"project": "SPARK", "issue_key": "SPARK-54045", "title": "Upgrade `FlatBuffers` to v25.9.23", "status": "Resolved", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "priority": "Major", "labels": ["pull-request-available"], "created": "2025-10-27T22:24:51.000+0000", "updated": "2025-10-28T04:45:59.000+0000", "description": null, "comments": [], "derived_tasks": {"summarization": "Upgrade `FlatBuffers` to v25.9.23", "classification": "feature", "qna": {"question": "What is the issue 'Upgrade `FlatBuffers` to v25.9.23' about?", "answer": null}}}
{"project": "SPARK", "issue_key": "SPARK-54044", "title": "Upgrade `gRPC Swift NIO Transport` to 2.2.0", "status": "Resolved", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "priority": "Major", "labels": ["pull-request-available"], "created": "2025-10-27T21:57:06.000+0000", "updated": "2025-10-27T22:20:09.000+0000", "description": null, "comments": [], "derived_tasks": {"summarization": "Upgrade `gRPC Swift NIO Transport` to 2.2.0", "classification": "feature", "qna": {"question": "What is the issue 'Upgrade `gRPC Swift NIO Transport` to 2.2.0' about?", "answer": null}}}
{"project": "SPARK", "issue_key": "SPARK-54043", "title": "Update `Spark Connect`-generated `Swift` source code with `4.1.0-preview3` RC1", "status": "Resolved", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "priority": "Major", "labels": ["pull-request-available"], "created": "2025-10-27T20:05:42.000+0000", "updated": "2025-10-27T21:16:43.000+0000", "description": null, "comments": [], "derived_tasks": {"summarization": "Update `Spark Connect`-generated `Swift` source code with `4.1.0-preview3` RC1", "classification": "feature", "qna": {"question": "What is the issue 'Update `Spark Connect`-generated `Swift` source code with `4.1.0-preview3` RC1' about?", "answer": null}}}
{"project": "SPARK", "issue_key": "SPARK-54042", "title": "Use 4.1.0-preview3 in `integration-test-(token|mac-spark41)`", "status": "Resolved", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "priority": "Major", "labels": ["pull-request-available"], "created": "2025-10-27T16:11:43.000+0000", "updated": "2025-10-27T21:46:39.000+0000", "description": null, "comments": [], "derived_tasks": {"summarization": "Use 4.1.0-preview3 in `integration-test-(token|mac-spark41)`", "classification": "feature", "qna": {"question": "What is the issue 'Use 4.1.0-preview3 in `integration-test-(token|mac-spark41)`' about?", "answer": null}}}
{"project": "SPARK", "issue_key": "SPARK-54041", "title": "Refactor ParameterizedQuery arguments validation", "status": "Resolved", "reporter": "Mihailo Aleksic", "assignee": "Mihailo Aleksic", "priority": "Major", "labels": ["pull-request-available"], "created": "2025-10-27T16:09:06.000+0000", "updated": "2025-10-29T12:28:01.000+0000", "description": "* In this issue I propose to refactor ParameterizedQuery arguments validation to ParameterizedQueryArgumentsValidator so it can be reused between single-pass and fixed-point analyzer implementations. We also remove one redundant case from the ParameterizedQueryArgumentsValidator.isNotAllowed (Alias shouldn't call isNotAllowed again as it introduces unnecessary overhead to the method) to improve performance.", "comments": [], "derived_tasks": {"summarization": "Refactor ParameterizedQuery arguments validation", "classification": "feature", "qna": {"question": "What is the issue 'Refactor ParameterizedQuery arguments validation' about?", "answer": "* In this issue I propose to refactor ParameterizedQuery arguments validation to ParameterizedQueryArgumentsValidator so it can be reused between single-pass and fixed-point analyzer implementations. We also remove one redundant case from the ParameterizedQueryArgumentsValidator.isNotAllowed (Alias shouldn't call isNotAllowed again as it introduces unnecessary overhead to the method) to improve performance."}}}
{"project": "SPARK", "issue_key": "SPARK-54040", "title": "Remove unused commons-collections 3.x", "status": "Resolved", "reporter": "Cheng Pan", "assignee": "Cheng Pan", "priority": "Major", "labels": ["pull-request-available"], "created": "2025-10-27T14:47:43.000+0000", "updated": "2025-10-27T18:21:49.000+0000", "description": null, "comments": [], "derived_tasks": {"summarization": "Remove unused commons-collections 3.x", "classification": "feature", "qna": {"question": "What is the issue 'Remove unused commons-collections 3.x' about?", "answer": null}}}
{"project": "SPARK", "issue_key": "SPARK-54039", "title": "SS | `Add TaskContext information to KafkaDataConsumer release() logs for better debugging`", "status": "Resolved", "reporter": "Nishanth", "assignee": "Nishanth", "priority": "Major", "labels": ["pull-request-available"], "created": "2025-10-27T12:15:05.000+0000", "updated": "2025-10-31T08:44:20.000+0000", "description": "Currently, log messages in the {{release()}} method of {{KafkaDataConsumer}} do not include Spark task context information (e.g., TaskID).\r\n\r\nThis change adds task context details to the {{release()}} logs to improve traceability and debugging of Kafka consumer lifecycle events.\r\n\r\nWe previously added similar context in {{{}KafkaBatchReaderFactory.createReader{}}}, which has proven useful for debugging. Including this information in {{release()}} helps correlate consumer release metrics with the specific Spark tasks that borrowed the consumer.", "comments": [], "derived_tasks": {"summarization": "SS | `Add TaskContext information to KafkaDataConsumer release() logs for better debugging`", "classification": "feature", "qna": {"question": "What is the issue 'SS | `Add TaskContext information to KafkaDataConsumer release() logs for better debugging`' about?", "answer": "Currently, log messages in the {{release()}} method of {{KafkaDataConsumer}} do not include Spark task context information (e.g., TaskID).\r\n\r\nThis change adds task context details to the {{release()}} logs to improve traceability and debugging of Kafka consumer lifecycle events.\r\n\r\nWe previously added similar context in {{{}KafkaBatchReaderFactory.createReader{}}}, which has proven useful for debugging. Including this information in {{release()}} helps correlate consumer release metrics with the specific Spark tasks that borrowed the consumer."}}}
{"project": "SPARK", "issue_key": "SPARK-54038", "title": "Support getSQLKeywords for SparkConnectDatabaseMetaData", "status": "Resolved", "reporter": "Cheng Pan", "assignee": "Cheng Pan", "priority": "Major", "labels": ["pull-request-available"], "created": "2025-10-27T09:41:10.000+0000", "updated": "2025-10-30T07:26:51.000+0000", "description": null, "comments": [], "derived_tasks": {"summarization": "Support getSQLKeywords for SparkConnectDatabaseMetaData", "classification": "feature", "qna": {"question": "What is the issue 'Support getSQLKeywords for SparkConnectDatabaseMetaData' about?", "answer": null}}}
{"project": "SPARK", "issue_key": "SPARK-54037", "title": "Throughput deteriorated after migration from spark 3.5.5 to spark 4.0.0", "status": "Open", "reporter": "Adrian Pusty", "assignee": null, "priority": "Major", "labels": [], "created": "2025-10-27T08:21:21.000+0000", "updated": "2025-10-27T08:21:59.000+0000", "description": "My team recently updated spark dependency version from 3.5.5 to 4.0.0\r\nThis included use of spark-4.0.0-bin-hadoop3.tgz, update in pom.xml files and change of import statements (org.apache.spark.sql -> org.apache.spark.sql.classic).\r\n\r\nAfter this change our throughput (calculated as rows transferred per second) has significantly dropped for our both scenarios: 1. read from file, write to database and 2. read from database, write to database.\r\n\r\nI have performed comparison between application versions with spark 3.5.5 and 4.0.0 in cluster mode, local mode and one comparison (with use of synthetic file) using spark-shell only.\r\nIn case of spark-shell I had more or less the same throughput for 3.5.5 and 4.0.0 but in case of our app used in cluster / local mode - both of these scenarios had better throughput with 3.5.5.\r\n\r\nI have observed that with 4.0.0 there are longer delays (when compared with 3.5.5) between log lines\r\n\"Running task x in stage y\"\r\nand\r\n\"Finished task x in stage y\".\r\n\r\nIs this throughput degradation a known issue? Could it be related to this task - [SPARK-48456] [M1] Performance benchmark - ASF JIRA ?\r\n\r\n(I'll also mention that we are using checkpointing (in case it might be important here))", "comments": [], "derived_tasks": {"summarization": "Throughput deteriorated after migration from spark 3.5.5 to spark 4.0.0", "classification": "feature", "qna": {"question": "What is the issue 'Throughput deteriorated after migration from spark 3.5.5 to spark 4.0.0' about?", "answer": "My team recently updated spark dependency version from 3.5.5 to 4.0.0\r\nThis included use of spark-4.0.0-bin-hadoop3.tgz, update in pom.xml files and change of import statements (org.apache.spark.sql -> org.apache.spark.sql.classic).\r\n\r\nAfter this change our throughput (calculated as rows transferred per second) has significantly dropped for our both scenarios: 1. read from file, write to database and 2. read from database, write to database.\r\n\r\nI have performed comparison between application versions with spark 3.5.5 and 4.0.0 in cluster mode, local mode and one comparison (with use of synthetic file) using spark-shell only.\r\nIn case of spark-shell I had more or less the same throughput for 3.5.5 and 4.0.0 but in case of our app used in cluster / local mode - both of these scenarios had better throughput with 3.5.5.\r\n\r\nI have observed that with 4.0.0 there are longer delays (when compared with 3.5.5) between log lines\r\n\"Running task x in stage y\"\r\nand\r\n\"Finished task x in stage y\".\r\n\r\nIs this throughput degradation a known issue? Could it be related to this task - [SPARK-48456] [M1] Performance benchmark - ASF JIRA ?\r\n\r\n(I'll also mention that we are using checkpointing (in case it might be important here))"}}}
{"project": "SPARK", "issue_key": "SPARK-54036", "title": "Add `build_python_3.11_macos26.yml` GitHub Action job", "status": "Resolved", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "priority": "Major", "labels": ["pull-request-available"], "created": "2025-10-27T03:00:13.000+0000", "updated": "2025-10-27T03:19:34.000+0000", "description": null, "comments": [], "derived_tasks": {"summarization": "Add `build_python_3.11_macos26.yml` GitHub Action job", "classification": "feature", "qna": {"question": "What is the issue 'Add `build_python_3.11_macos26.yml` GitHub Action job' about?", "answer": null}}}
{"project": "SPARK", "issue_key": "SPARK-54035", "title": "Construct FileStatus from the executor side directly", "status": "Open", "reporter": "Cheng Pan", "assignee": null, "priority": "Major", "labels": [], "created": "2025-10-27T02:33:18.000+0000", "updated": "2025-10-27T02:33:18.000+0000", "description": "https://github.com/apache/spark/pull/50765#discussion_r2357607758", "comments": [], "derived_tasks": {"summarization": "Construct FileStatus from the executor side directly", "classification": "feature", "qna": {"question": "What is the issue 'Construct FileStatus from the executor side directly' about?", "answer": "https://github.com/apache/spark/pull/50765#discussion_r2357607758"}}}
{"project": "SPARK", "issue_key": "SPARK-54034", "title": "Fix `Utils.isBindCollision` to detect port conflict `NativeIoException` correctly", "status": "Resolved", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "priority": "Critical", "labels": ["pull-request-available"], "created": "2025-10-26T22:27:03.000+0000", "updated": "2025-10-27T15:29:41.000+0000", "description": null, "comments": [], "derived_tasks": {"summarization": "Fix `Utils.isBindCollision` to detect port conflict `NativeIoException` correctly", "classification": "feature", "qna": {"question": "What is the issue 'Fix `Utils.isBindCollision` to detect port conflict `NativeIoException` correctly' about?", "answer": null}}}
{"project": "SPARK", "issue_key": "SPARK-54033", "title": "Introduce Catalyst server-side geospatial execution classes", "status": "Resolved", "reporter": "Uro\u0161 Bojani\u0107", "assignee": "Uro\u0161 Bojani\u0107", "priority": "Major", "labels": ["pull-request-available"], "created": "2025-10-26T21:09:17.000+0000", "updated": "2025-10-31T07:53:32.000+0000", "description": null, "comments": [], "derived_tasks": {"summarization": "Introduce Catalyst server-side geospatial execution classes", "classification": "feature", "qna": {"question": "What is the issue 'Introduce Catalyst server-side geospatial execution classes' about?", "answer": null}}}
{"project": "SPARK", "issue_key": "SPARK-54032", "title": "Prefer to use native Netty transports by default", "status": "Resolved", "reporter": "Dongjoon Hyun", "assignee": "Dongjoon Hyun", "priority": "Major", "labels": ["pull-request-available"], "created": "2025-10-26T20:07:43.000+0000", "updated": "2025-10-27T02:42:36.000+0000", "description": null, "comments": [], "derived_tasks": {"summarization": "Prefer to use native Netty transports by default", "classification": "feature", "qna": {"question": "What is the issue 'Prefer to use native Netty transports by default' about?", "answer": null}}}
{"project": "SPARK", "issue_key": "SPARK-54031", "title": "Add golden files for Analyzer edge cases", "status": "Resolved", "reporter": "Vladimir Golubev", "assignee": "Vladimir Golubev", "priority": "Major", "labels": ["pull-request-available"], "created": "2025-10-25T21:20:48.000+0000", "updated": "2025-10-28T15:28:37.000+0000", "description": "New golden files for edge-cases discovered during the Analyzer support and development.", "comments": [], "derived_tasks": {"summarization": "Add golden files for Analyzer edge cases", "classification": "feature", "qna": {"question": "What is the issue 'Add golden files for Analyzer edge cases' about?", "answer": "New golden files for edge-cases discovered during the Analyzer support and development."}}}
{"project": "SPARK", "issue_key": "SPARK-54030", "title": "Add detailed error message for corrupted view metadata with mismatched column counts", "status": "Resolved", "reporter": "Ganesha S", "assignee": "Ganesha S", "priority": "Major", "labels": ["pull-request-available"], "created": "2025-10-25T16:11:19.000+0000", "updated": "2025-10-30T13:16:14.000+0000", "description": "Enhance error reporting for corrupted view metadata by adding a detailed, user-friendly assertion message when the number of view query column names and the number of columns in the view schema mismatch.", "comments": [], "derived_tasks": {"summarization": "Add detailed error message for corrupted view metadata with mismatched column counts", "classification": "feature", "qna": {"question": "What is the issue 'Add detailed error message for corrupted view metadata with mismatched column counts' about?", "answer": "Enhance error reporting for corrupted view metadata by adding a detailed, user-friendly assertion message when the number of view query column names and the number of columns in the view schema mismatch."}}}
